[
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 0,
    "text": "Intelligence of machines Artificial intelligence ( AI ) is the capability of computational systems to perform tasks typically associated with human intelligence , such as learning , reasoning , problem-solving , perception , and decision-making . It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search ); recommendation systems (used by YouTube , Amazon , and Netflix ); virtual assistants (e.g., Google Assistant , Siri , and Alexa ); autonomous vehicles (e.g., Waymo ); generative and creative tools (e.g., language models and AI art ); and superhuman play and analysis in strategy games (e.g., chess and Go ). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore .\" [ 2 ] [ 3 ] Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning , knowledge representation , planning , natural language processing , perception , and support for robotics . [ a ] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization , formal logic , artificial neural networks , and methods based on statistics , operations research , and economics . [ b ] AI also draws upon psychology , linguistics , philosophy , neuroscience , and other fields. [ 4 ] Some companies, such as OpenAI , Google DeepMind and Meta , [ 5 ] aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, [ 6 ] and the field went through multiple cycles of optimism throughout its history , [ 7 ] [ 8 ] followed by periods of disappointment and loss of funding, known as AI winters . [ 9 ] [ 10 ] Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. [ 11 ] This growth accelerated further after 2017 with the transformer architecture . In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom . Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks , prompting discussions about regulatory policies to ensure the safety and benefits of the technology. Goals The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. [ a ] Reasoning and problem-solving Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions . [ 13 ] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics . [ 14 ] Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. [ 15 ] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. [ 16 ] Accurate and efficient reasoning is an unsolved problem. Knowledge representation An ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts. Knowledge representation and knowledge engineering [ 17 ] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases ), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; [ 24 ] situations, events, states, and time; [ 25 ] causes and effects; [ 26 ] knowledge about knowledge (what we know about what other people know); [ 27 ] default reasoning (things that"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 1,
    "text": "humans assume are true until they are told differently and will remain true even when other facts are changing); [ 28 ] and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); [ 29 ] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). [ 16 ] There is also the difficulty of knowledge acquisition , the problem of obtaining knowledge for AI applications. [ c ] Planning and decision-making An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. [ d ] In automated planning , the agent has a specific goal. [ 33 ] In automated decision-making , the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \" utility \") that measures how much the agent prefers it. For each possible action, it can calculate the \" expected utility \": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility. [ 34 ] In classical planning , the agent knows exactly what the effect of any action will be. [ 35 ] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. [ 36 ] In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning ), or the agent can seek information to improve its preferences. [ 37 ] Information value theory can be used to weigh the value of exploratory or experimental actions. [ 38 ] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration ), be heuristic , or it can be learned. [ 39 ] Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. [ 40 ] Learning Machine learning is the study of programs that can improve their performance on a given task automatically. [ 41 ] It has been a part of AI from the beginning. [ e ] In supervised learning , the training data is labelled with the expected answers, while in unsupervised learning , the model identifies patterns or structures in unlabelled data. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. [ 44 ] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). [ 45 ] In reinforcement learning , the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". [ 46 ] Transfer learning is when the knowledge gained from one problem is applied to a new problem. [ 47 ] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. [ 48 ] Computational learning theory can assess learners by computational complexity , by sample complexity (how much data is required), or by other notions of optimization . [ 49 ] Natural language processing Natural language processing (NLP) allows programs to read, write and communicate in human languages. [ 50 ] Specific problems include speech recognition , speech synthesis , machine translation , information extraction , information retrieval and question answering . [ 51 ] Early work, based on Noam"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 2,
    "text": "Chomsky 's generative grammar and semantic networks , had difficulty with word-sense disambiguation [ f ] unless restricted to small domains called \" micro-worlds \" (due to the common sense knowledge problem [ 29 ] ). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. [ 54 ] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam , SAT test, GRE test, and many other real-world applications. Perception Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar , sonar, radar, and tactile sensors ) to deduce aspects of the world. Computer vision is the ability to analyze visual input. [ 58 ] The field includes speech recognition , image classification , facial recognition , object recognition , object tracking , [ 62 ] and robotic perception . Social intelligence Kismet , a robot head which was made in the 1990s; it is a machine that can recognize and simulate emotions. Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood . [ 65 ] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction . However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis , wherein AI classifies the effects displayed by a videotaped subject. General intelligence A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence . [ 68 ] Techniques AI research uses a wide variety of techniques to accomplish the goals above. [ b ] Search and optimization AI can solve many problems by intelligently searching through many possible solutions. [ 69 ] There are two very different kinds of search used in AI: state space search and local search . State space search State space search searches through a tree of possible states to try to find a goal state. [ 70 ] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis . Simple exhaustive searches [ 72 ] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers . The result is a search that is too slow or never completes. [ 15 ] \" Heuristics \" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal. [ 73 ] Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position. [ 74 ] Local search Illustration of gradient descent for 3 different starting points; two parameters (represented by the plan coordinates) are adjusted in order to minimize the loss function (the height) Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally. [ 75 ] Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function . Variants of gradient descent are commonly used to train neural networks , [ 76 ] through the backpropagation algorithm. Another type of local search is evolutionary computation , which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation. [ 77 ] Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking ) and ant colony optimization (inspired by ant trails ). Logic Formal logic is used for reasoning and knowledge representation . [ 79 ] Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") [ 80 ] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \" Every X is a Y \" and \"There are some X"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 3,
    "text": "s that are Y s\"). [ 81 ] Deductive reasoning in logic is the process of proving a new statement ( conclusion ) from other statements that are given and assumed to be true (the premises ). [ 82 ] Proofs can be structured as proof trees , in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules . Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms . In the case of Horn clauses , problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. [ 83 ] In the more general case of the clausal form of first-order logic , resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved. [ 84 ] Inference in both Horn clause logic and first-order logic is undecidable , and therefore intractable . However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog , is Turing complete . Moreover, its efficiency is competitive with computation in other symbolic programming languages. [ 85 ] Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true. [ 86 ] Non-monotonic logics , including logic programming with negation as failure , are designed to handle default reasoning . [ 28 ] Other specialized versions of logic have been developed to describe many complex domains. Probabilistic methods for uncertain reasoning A simple Bayesian network , with the associated conditional probability tables Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. [ 87 ] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory , decision analysis , [ 88 ] and information value theory . [ 89 ] These tools include models such as Markov decision processes , [ 90 ] dynamic decision networks , [ 91 ] game theory and mechanism design . [ 92 ] Bayesian networks [ 93 ] are a tool that can be used for reasoning (using the Bayesian inference algorithm), [ g ] [ 95 ] learning (using the expectation–maximization algorithm ), [ h ] [ 97 ] planning (using decision networks ) [ 98 ] and perception (using dynamic Bayesian networks ). [ 91 ] Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters ). [ 91 ] Expectation–maximization clustering of Old Faithful eruption data starts from a random guess but then successfully converges on an accurate clustering of the two physically distinct modes of eruption. Classifiers and statistical learning methods The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers [ 99 ] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning . Each pattern (also called an \" observation \") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set . When a new observation is received, that observation is classified based on previous experience. [ 45 ] There are many kinds of classifiers in use. [ 100 ] The decision tree is the simplest and most widely used symbolic machine learning algorithm. [ 101 ] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s. [ 102 ] The naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability. [ 104 ] Neural networks are also used as classifiers. [ 105 ] Artificial neural networks A neural network is an interconnected group of nodes, akin to the vast network of neurons in the human brain . An artificial neural network is based on a collection of nodes also known as artificial neurons , which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 4,
    "text": "an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers. [ 105 ] Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. [ 106 ] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function. [ 107 ] In feedforward neural networks the signal passes in only one direction. [ 108 ] The term perceptron typically refers to a single-layer neural network. [ 109 ] In contrast, deep learning uses many layers. [ 110 ] Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem . [ 111 ] Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing , where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects. [ 112 ] Deep learning Deep learning is a subset of machine learning , which is itself a subset of artificial intelligence. [ 113 ] Deep learning uses several layers of neurons between the network's inputs and outputs. [ 110 ] The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing , lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces. Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision , speech recognition , natural language processing , image classification , and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) [ i ] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs ) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet . [ j ] GPT Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \" hallucinations \". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. [ 124 ] Such systems are used in chatbots , which allow people to ask a question or request a task in simple text. [ 126 ] Current models and services include ChatGPT , Claude , Gemini , Copilot , and Meta AI . [ 127 ] Multimodal GPT models can process different types of data ( modalities ) such as images, videos, sound, and text. Hardware and software In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, [ 130 ] but general-purpose programming languages like Python have become predominant. [ 131 ] The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law , named after the Intel co-founder Gordon Moore , who first identified it. Improvements in GPUs have been even faster, [ 132 ] a trend sometimes called Huang's law , [ 133 ] named after Nvidia co-founder and CEO Jensen"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 5,
    "text": "Huang . Applications AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search ), targeting online advertisements , recommendation systems (offered by Netflix , YouTube or Amazon ), driving internet traffic , targeted advertising ( AdSense , Facebook ), virtual assistants (such as Siri or Alexa ), autonomous vehicles (including drones , ADAS and self-driving cars ), automatic language translation ( Microsoft Translator , Google Translate ), facial recognition ( Apple 's FaceID or Microsoft 's DeepFace and Google 's FaceNet ) and image labeling (used by Facebook , Apple's Photos and TikTok ). The deployment of AI may be overseen by a chief automation officer (CAO). Health and medicine The application of AI in medicine and medical research has the potential to increase patient care and quality of life. [ 134 ] Through the lens of the Hippocratic Oath , medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients. [ 135 ] [ 136 ] For medical research, AI is an important tool for processing and integrating big data . This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. [ 137 ] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. [ 137 ] [ 138 ] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein . [ 139 ] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. [ 140 ] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold. [ 141 ] [ 142 ] Games Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. [ 143 ] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov , on 11 May 1997. [ 144 ] In 2011, in a Jeopardy! quiz show exhibition match, IBM 's question answering system , Watson , defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings , by a significant margin. [ 145 ] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol , becoming the first computer Go -playing system to beat a professional Go player without handicaps . Then, in 2017, it defeated Ke Jie , who was the best Go player in the world. [ 146 ] Other programs handle imperfect-information games, such as the poker -playing program Pluribus . [ 147 ] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero , which could be trained to play chess, Go, or Atari games. [ 148 ] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II , a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. [ 149 ] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. [ 150 ] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions. [ 151 ] Mathematics Large language models, such as GPT-4 , Gemini , Claude , Llama or Mistral , are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations . They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning [ 152 ] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. [ 153 ] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. [ 154 ] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. [ 155 ] The Alibaba Group developed a version of its Qwen models called Qwen2-Math , that achieved state-of-the-art performance"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 6,
    "text": "on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. [ 156 ] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems. [ 157 ] Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor , AlphaGeometry , AlphaProof and AlphaEvolve [ 158 ] all from Google DeepMind , [ 159 ] Llemma from EleutherAI [ 160 ] or Julius . [ 161 ] When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025. [ 162 ] Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics. [ 163 ] Topological deep learning integrates various topological approaches. Finance Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years. [ 164 ] According to Nicolas Firzli, director of the World Pensions & Investments Forum , it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\" [ 165 ] Military Various countries are deploying AI military applications. [ 166 ] The main applications enhance command and control , communications, sensors, integration and interoperability. [ 167 ] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles . [ 166 ] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition , coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous . [ 167 ] AI has been used in military operations in Iraq, Syria, Israel and Ukraine. [ 166 ] [ 168 ] [ 169 ] [ 170 ] Generative AI Vincent van Gogh in watercolour created by generative AI software Generative artificial intelligence (Generative AI, GenAI, [ 171 ] or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. [ 172 ] [ 173 ] [ 174 ] These models learn the underlying patterns and structures of their training data and use them to produce new data [ 175 ] [ 176 ] based on the input, which often comes in the form of natural language prompts . [ 177 ] [ 178 ] Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer -based deep neural networks , particularly large language models (LLMs). Major tools include chatbots such as ChatGPT , Copilot , Gemini , Claude , Grok , and DeepSeek ; text-to-image models such as Stable Diffusion , Midjourney , and DALL-E ; and text-to-video models such as Veo , LTXV and Sora . [ 179 ] [ 180 ] [ 181 ] [ 182 ] [ 183 ] Technology companies developing generative AI include OpenAI , xAI , Anthropic , Meta AI , Microsoft , Google , DeepSeek , and Baidu . [ 177 ] [ 184 ] [ 185 ] Generative AI has raised many ethical questions and governance challenges as it can be used for cybercrime , or to deceive or manipulate people through fake news or deepfakes . [ 186 ] [ 187 ] Even if used ethically, it may lead to mass replacement of human jobs . [ 188 ] The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works. [ 189 ] Agents AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants , chatbots , autonomous vehicles , game-playing systems , and industrial robotics . AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 7,
    "text": "scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks. [ 190 ] [ 191 ] [ 192 ] Sexuality Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, [ 193 ] AI-integrated sex toys (e.g., teledildonics ), [ 194 ] AI-generated sexual education content, [ 195 ] and AI agents that simulate sexual and romantic partners (e.g., Replika ). [ 196 ] AI is also used for the production of non-consensual deepfake pornography , raising significant ethical and legal concerns. [ 197 ] AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors. [ 198 ] [ 199 ] Other industry-specific tasks There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. [ 200 ] A few examples are energy storage , medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy , or supply chain management. AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions. [ 201 ] [ 202 ] [ 203 ] In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics , classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water. Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation. During the 2024 Indian elections , US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages. [ 204 ] Ethics Street art in Tel Aviv [ 205 ] [ 206 ] AI has potential benefits and potential risks. [ 207 ] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. [ 210 ] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning. Risks and harm Privacy and copyright Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy , surveillance and copyright . AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency. Sensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy . AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation , de-identification and differential privacy . Since 2016, some privacy experts, such as Cynthia Dwork , have begun to view privacy in terms of fairness . Brian Christian wrote that experts"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 8,
    "text": "have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\" Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \" fair use \". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". [ 218 ] Website owners who do not wish to have their content scraped can indicate it in a \" robots.txt \" file. [ 219 ] In 2023, leading authors (including John Grisham and Jonathan Franzen ) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors. [ 222 ] Dominance by tech giants The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc. , Amazon , Apple Inc. , Meta Platforms , and Microsoft . [ 223 ] [ 224 ] [ 225 ] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers , allowing them to entrench further in the marketplace. [ 226 ] [ 227 ] Power needs and environmental impacts In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026 , forecasting electric power use. [ 228 ] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation. [ 229 ] Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms. [ 230 ] A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge , found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. [ 231 ] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all. [ 232 ] In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. [ 233 ] Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers. [ 234 ] In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission . If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act . [ 235 ] The US government and the state of Michigan are investing almost US$2 billion"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 9,
    "text": "to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation. [ 236 ] After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. [ 237 ] Taiwan aims to phase out nuclear power by 2025. [ 237 ] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. [ 237 ] Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident , according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI. [ 238 ] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI. [ 238 ] On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. [ 239 ] According to the Commission Chairman Willie L. Phillips , it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors. [ 239 ] In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it. [ 240 ] Misinformation YouTube , Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation , conspiracy theories , and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. [ 242 ] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem. [ 243 ] In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, [ 244 ] while realistic AI-generated videos became feasible in the mid-2020s. [ 245 ] [ 246 ] [ 247 ] It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda . [ 249 ] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks. AI researchers at Microsoft , OpenAI , universities and other organisations have suggested using \" personhood credentials \" as a way to overcome online deception enabled by AI models. [ 251 ] Algorithmic bias and fairness Machine learning applications will be biased [ k ] if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine , finance , recruitment , housing or policing ) then the algorithm may cause discrimination . [ 256 ] The field of fairness studies how to prevent harms from algorithmic biases. On June 28, 2015, Google Photos 's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 10,
    "text": "system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon. COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist . In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers [ l ] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data. [ 262 ] A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". [ 263 ] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\" [ 264 ] Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations , some of these \"recommendations\" will likely be racist. [ 265 ] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive. [ m ] Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women. There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness , which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws . [ 252 ] At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery , in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed. [ dubious – discuss ] Lack of transparency Many AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks , in which there are many non- linear relationships between inputs and outputs. But some popular explainability techniques exist. [ 269 ] It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 11,
    "text": "misleading. People who have been harmed by an algorithm's decision have a right to an explanation. [ 272 ] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. [ n ] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used. DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems. Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution , DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers , Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts. [ 279 ] Bad actors and weaponized AI Artificial intelligence provides a number of tools that are useful to bad actors , such as authoritarian governments , terrorists , criminals or rogue states . A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. [ o ] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction . Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person . In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations ' Convention on Certain Conventional Weapons , however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots. [ 283 ] AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance . Machine learning , operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets . It lowers the cost and difficulty of digital warfare and advanced spyware . All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China. [ 285 ] [ 286 ] There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours. Technological unemployment Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment. [ 288 ] In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. [ 289 ] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment , but they generally agree that it could be a net benefit if productivity gains are redistributed . Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". [ p ] [ 292 ] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. [ 288 ] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence. [ 293 ] [ 294 ] Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 12,
    "text": "care-related professions ranging from personal healthcare to the clergy. [ 296 ] From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum , about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement. [ 297 ] Existential risk It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \" spell the end of the human race \". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. [ q ] These sci-fi scenarios are misleading in several ways. First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer ). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\". [ 302 ] Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies , law , government , money and the economy are built on language ; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking , Bill Gates , and Elon Musk , [ 305 ] as well as AI pioneers such as Yoshua Bengio , Stuart Russell , Demis Hassabis , and Sam Altman , have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". [ 306 ] He notably mentioned risks of an AI takeover , [ 307 ] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. [ 308 ] In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" [ 310 ] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" [ 311 ] [ 312 ] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" [ 313 ] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" [ 314 ] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. [ 315 ] However, after 2016, the study of current and future risks and possible solutions became a serious area of research. Ethical machines and alignment Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky , who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk. Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas. [ 318 ] The field of machine ethics is also called computational morality, [ 318 ] and was founded at"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 13,
    "text": "an AAAI symposium in 2005. Other approaches include Wendell Wallach 's \"artificial moral agents\" and Stuart J. Russell 's three principles for developing provably beneficial machines. Open source Active organizations in the AI open-source community include Hugging Face , [ 322 ] Google , [ 323 ] EleutherAI and Meta . [ 324 ] Various AI models, such as Llama 2 , Mistral or Stable Diffusion , have been made open-weight, [ 325 ] [ 326 ] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned , which allows companies to specialize them with their own data and for their own use-case. [ 327 ] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism ) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. [ 328 ] Frameworks Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows: [ 329 ] [ 330 ] Respect the dignity of individual people Connect with other people sincerely, openly, and inclusively Care for the wellbeing of everyone Protect social values, justice, and the public interest Other developments in ethical frameworks include those decided upon during the Asilomar Conference , the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; [ 331 ] however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks. [ 332 ] Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers. [ 333 ] The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities. [ 334 ] Regulation The first global AI Safety Summit was held in the United Kingdom in November 2023 with a declaration calling for international cooperation. The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. [ 335 ] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford , the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger , Eric Schmidt , and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. [ 342 ] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \" Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law \". It was adopted by the European Union, the United States, the United Kingdom, and other signatories. [ 343 ] In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters /Ipsos poll"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 14,
    "text": "found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\". In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. [ 347 ] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. [ 348 ] [ 349 ] In May 2024 at the AI Seoul Summit , 16 global AI tech companies agreed to safety commitments on the development of AI. [ 350 ] [ 351 ] History In 2024, AI patents in China and the US numbered more than three-fourths of AI patents worldwide. [ 352 ] Though China had more AI patents, the US had 35% more patents per AI patent-applicant company than China. [ 352 ] The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing 's theory of computation , which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. [ 354 ] This, along with concurrent discoveries in cybernetics , information theory and neurobiology , led researchers to consider the possibility of building an \"electronic brain\". [ r ] They developed several areas of research that would become part of AI, [ 356 ] such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper ' Computing Machinery and Intelligence ', which introduced the Turing test and showed that \"machine intelligence\" was plausible. [ 357 ] [ 354 ] The field of AI research was founded at a workshop at Dartmouth College in 1956. [ s ] [ 6 ] The attendees became the leaders of AI research in the 1960s. [ t ] They and their students produced programs that the press described as \"astonishing\": [ u ] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. [ v ] [ 7 ] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s. [ 354 ] Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". [ 362 ] In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". [ 363 ] They had, however, underestimated the difficulty of the problem. [ w ] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects . Minsky and Papert 's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \" AI winter \", a period when obtaining funding for AI projects was difficult, followed. [ 9 ] In the early 1980s, AI research was revived by the commercial success of expert systems , [ 368 ] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research . [ 8 ] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began. [ 10 ] Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception , robotics , learning and pattern recognition , and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. [ x ] Judea Pearl , Lotfi Zadeh , and others developed methods that handled"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 15,
    "text": "incomplete and uncertain information by making reasonable guesses rather than precise logic. [ 87 ] But the most important development was the revival of \" connectionism \", including neural network research, by Geoffrey Hinton and others. [ 376 ] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks. AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \" narrow \" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics , economics and mathematics ). [ 378 ] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect ). [ 379 ] However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s. [ 68 ] Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field. [ 11 ] For many specific tasks, other methods were abandoned. [ y ] Deep learning's success was based on both hardware improvements ( faster computers , [ 381 ] graphics processing units , cloud computing ) and access to large amounts of data [ 383 ] (including curated datasets, such as ImageNet ). Deep learning's success led to an enormous increase in interest and funding in AI. [ z ] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. The number of Google searches for the term \"AI\" accelerated in 2022. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo , developed by DeepMind , beat the world champion Go player . The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. [ 384 ] ChatGPT , launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. [ 385 ] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. [ 386 ] These programs, and others, inspired an aggressive AI boom , where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. [ 389 ] Philosophy Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. [ 390 ] Another major focus has been whether machines can be conscious, and the associated ethical implications. [ 391 ] Many other topics in philosophy are relevant to AI, such as epistemology and free will . [ 392 ] Rapid advancements have intensified public discussions on the philosophy and ethics of AI . [ 391 ] Defining artificial intelligence Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test , which measures the ability of a machine to simulate human conversation. [ 357 ] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\" The Turing test can provide some evidence of intelligence, but it penalizes non-human intelligent behavior. [ 395 ] Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \" Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 16,
    "text": "that fly so exactly like pigeons that they can fool other pigeons. ' \" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky , similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible. Another definition has been adopted by Google, [ 400 ] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, [ 401 ] with many companies during the early 2020s AI boom using the term as a marketing buzzword , often even if they did \"not actually use AI in a material way\". [ 402 ] There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text . [ 403 ] Evaluating approaches to AI No established unifying theory or paradigm has guided AI research for most of its history. [ aa ] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic , soft and narrow . Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AI and its limits Symbolic AI (or \" GOFAI \") [ 405 ] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis : \"A physical symbol system has the necessary and sufficient means of general intelligent action.\" [ 406 ] However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning . Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. [ 407 ] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. [ 408 ] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him. [ ab ] [ 16 ] The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias . Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI : it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches. Neat vs. scruffy \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic , optimization , or neural networks ). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, [ 412 ] but eventually was seen as irrelevant. Modern AI has elements of both. Soft vs. hard computing Finding a provably correct or optimal solution is intractable for many important problems. [ 15 ] Soft computing is a set of techniques, including genetic algorithms , fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 17,
    "text": "many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. Machine consciousness, sentience, and mind There is no settled consensus in philosophy of mind on whether a machine can have a mind , consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction . Consciousness David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like . Computationalism and functionalism Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem . This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam . Philosopher John Searle characterized this position as \" strong AI \": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" [ ac ] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind. [ 422 ] AI welfare and rights It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. [ 423 ] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. [ 424 ] [ 425 ] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness ) may provide another moral basis for AI rights. [ 424 ] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society. [ 426 ] In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. [ 427 ] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights , and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own. [ 428 ] [ 429 ] Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming , which could lead to large-scale suffering if sentient AI is created and carelessly exploited. [ 425 ] [ 424 ] Future Superintelligence and the singularity A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself . The improved software would be even better at improving itself, leading to what I."
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 18,
    "text": "J. Good called an \" intelligence explosion \" and Vernor Vinge called a \" singularity \". [ 430 ] However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve , slowing when they reach the physical limits of what the technology can do. Transhumanism Robot designer Hans Moravec , cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger . [ 432 ] Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler 's \" Darwin among the Machines \" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence . [ 433 ] In fiction The word \"robot\" itself was coined by Karel Čapek in his 1921 play R.U.R. , the title standing for \"Rossum's Universal Robots\". Thought-capable artificial beings have appeared as storytelling devices since antiquity, [ 434 ] and have been a persistent theme in science fiction . A common trope in these works began with Mary Shelley 's Frankenstein , where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000 , the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture. Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \" Multivac \" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; [ 437 ] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity. Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel , and thus to suffer. This appears in Karel Čapek 's R.U.R. , the films A.I. Artificial Intelligence and Ex Machina , as well as the novel Do Androids Dream of Electric Sheep? , by Philip K. Dick . Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. See also Explanatory notes ^ a b This list of intelligent traits is based on the topics covered by the major AI textbooks, including: Russell & Norvig (2021) , Luger & Stubblefield (2004) , Poole, Mackworth & Goebel (1998) and Nilsson (1998) ^ a b This list of tools is based on the topics covered by the major AI textbooks, including: Russell & Norvig (2021) , Luger & Stubblefield (2004) , Poole, Mackworth & Goebel (1998) and Nilsson (1998) ^ It is among the reasons that expert systems proved to be inefficient for capturing knowledge. ^ \"Rational agent\" is general term used in economics , philosophy and theoretical artificial intelligence. It can refer to anything that directs its behavior to accomplish goals, such as a person, an animal, a corporation, a nation, or in the case of AI, a computer program. ^ Alan Turing discussed the centrality of learning as early as 1950, in his classic paper \" Computing Machinery and Intelligence \". In 1956, at the original Dartmouth AI summer conference, Ray Solomonoff wrote a report on unsupervised probabilistic machine learning: \"An Inductive Inference Machine\". ^ See AI winter § Machine translation and the ALPAC report of 1966 ^ Compared with symbolic logic, formal Bayesian inference is computationally expensive. For inference to be tractable, most observations must be conditionally independent of one another. AdSense uses a Bayesian network with over 300 million edges to learn which ads to serve. ^ Expectation–maximization, one of the most popular algorithms in machine learning, allows clustering in the presence of unknown latent variables . ^ Some form of deep neural networks (without a specific learning algorithm) were described by: Warren S. McCulloch and Walter Pitts (1943) Alan Turing (1948); Karl Steinbuch and Roger David Joseph (1961). Deep or recurrent networks that learned (or used gradient descent) were developed by: Frank Rosenblatt (1957); Oliver Selfridge (1959); Alexey Ivakhnenko and Valentin Lapa (1965); Kaoru Nakano (1971); Shun-Ichi Amari (1972); John Joseph Hopfield (1982). Precursors to backpropagation were developed by: Henry J. Kelley (1960); Arthur E. Bryson (1962); Stuart Dreyfus (1962); Arthur E. Bryson and Yu-Chi Ho (1969); Backpropagation was independently developed by: Seppo Linnainmaa (1970); Paul"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 19,
    "text": "Werbos (1974). ^ Geoffrey Hinton said, of his work on neural networks in the 1990s, \"our labeled datasets were thousands of times too small. [And] our computers were millions of times too slow.\" [ 123 ] ^ In statistics, a bias is a systematic error or deviation from the correct value. But in the context of fairness , it refers to a tendency in favor or against a certain group or individual characteristic, usually in a way that is considered unfair or harmful. A statistically unbiased AI system that produces disparate outcomes for different demographic groups may thus be viewed as biased in the ethical sense. [ 252 ] ^ Including Jon Kleinberg ( Cornell University ), Sendhil Mullainathan ( University of Chicago ), Cynthia Chouldechova ( Carnegie Mellon ) and Sam Corbett-Davis ( Stanford ) ^ Moritz Hardt (a director at the Max Planck Institute for Intelligent Systems ) argues that machine learning \"is fundamentally the wrong tool for a lot of domains, where you're trying to design interventions and mechanisms that change the world.\" [ 266 ] ^ When the law was passed in 2018, it still contained a form of this provision. ^ This is the United Nations ' definition, and includes things like land mines as well. ^ See table 4; 9% is both the OECD average and the U.S. average. ^ Sometimes called a \" robopocalypse \" ^ \"Electronic brain\" was the term used by the press around this time. [ 355 ] ^ Daniel Crevier wrote, \"the conference is generally recognized as the official birthdate of the new science.\" Russell and Norvig called the conference \"the inception of artificial intelligence.\" ^ Russell and Norvig wrote \"for the next 20 years the field would be dominated by these people and their students.\" ^ Russell and Norvig wrote, \"it was astonishing whenever a computer did anything kind of smartish\". ^ The programs described are Arthur Samuel 's checkers program for the IBM 701 , Daniel Bobrow 's STUDENT , Newell and Simon 's Logic Theorist and Terry Winograd 's SHRDLU . ^ Russell and Norvig write: \"in almost all cases, these early systems failed on more difficult problems\" ^ Embodied approaches to AI were championed by Hans Moravec and Rodney Brooks and went by many names: Nouvelle AI . Developmental robotics . [ 374 ] ^ Matteo Wong wrote in The Atlantic : \"Whereas for decades, computer-science fields such as natural-language processing, computer vision, and robotics used extremely different methods, now they all use a programming method called \"deep learning\". As a result, their code and approaches have become more similar, and their models are easier to integrate into one another.\" ^ Jack Clark wrote in Bloomberg : \"After a half-decade of quiet breakthroughs in artificial intelligence, 2015 has been a landmark year. Computers are smarter and learning faster than ever\", and noted that the number of software projects that use machine learning at Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects in 2015. ^ Nils Nilsson wrote in 1983: \"Simply put, there is wide disagreement in the field about what AI is all about.\" ^ Daniel Crevier wrote that \"time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.\" ^ Searle presented this definition of \"Strong AI\" in 1999. Searle's original formulation was \"The appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states.\" Strong AI is defined similarly by Russell and Norvig : \"Stong AI – the assertion that machines that do so are actually thinking (as opposed to simulating thinking).\" References ^ AI set to exceed human brain power Archived 2008-02-19 at the Wayback Machine CNN.com (July 26, 2006) ^ Kaplan, Andreas; Haenlein, Michael (2019). \"Siri, Siri, in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence\". Business Horizons . 62 : 15– 25. doi : 10.1016/j.bushor.2018.08.004 . ISSN 0007-6813 . S2CID 158433736 . ^ Russell & Norvig (2021 , §1.2). ^ \"Tech companies want to build artificial general intelligence. But who decides when AGI is attained?\" . AP News . 4 April 2024 . Retrieved 20 May 2025 . ^ a b Dartmouth workshop : Russell & Norvig (2021 , p. 18), McCorduck (2004 , pp. 111–136), NRC (1999 , pp. 200–201) The proposal: McCarthy et al. (1955) ^ a b Successful programs of the 1960s: McCorduck (2004 , pp. 243–252), Crevier (1993 , pp. 52–107), Moravec (1988 , p. 9), Russell & Norvig (2021 , pp. 19–21) ^ a b Funding initiatives in the early 1980s: Fifth Generation Project (Japan), Alvey (UK), Microelectronics"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 20,
    "text": "and Computer Technology Corporation (US), Strategic Computing Initiative (US): McCorduck (2004 , pp. 426–441), Crevier (1993 , pp. 161–162, 197–203, 211, 240), Russell & Norvig (2021 , p. 23), NRC (1999 , pp. 210–211), Newquist (1994 , pp. 235–248) ^ a b First AI Winter , Lighthill report , Mansfield Amendment : Crevier (1993 , pp. 115–117), Russell & Norvig (2021 , pp. 21–22), NRC (1999 , pp. 212–213), Howe (1994) , Newquist (1994 , pp. 189–201) ^ a b Second AI Winter : Russell & Norvig (2021 , p. 24), McCorduck (2004 , pp. 430–435), Crevier (1993 , pp. 209–210), NRC (1999 , pp. 214–216), Newquist (1994 , pp. 301–318) ^ a b Deep learning revolution, AlexNet : Goldman (2022) , Russell & Norvig (2021 , p. 26), McKinsey (2018) ^ Problem-solving, puzzle solving, game playing, and deduction: Russell & Norvig (2021 , chpt. 3–5), Russell & Norvig (2021 , chpt. 6) ( constraint satisfaction ), Poole, Mackworth & Goebel (1998 , chpt. 2, 3, 7, 9), Luger & Stubblefield (2004 , chpt. 3, 4, 6, 8), Nilsson (1998 , chpt. 7–12) ^ Uncertain reasoning: Russell & Norvig (2021 , chpt. 12–18), Poole, Mackworth & Goebel (1998 , pp. 345–395), Luger & Stubblefield (2004 , pp. 333–381), Nilsson (1998 , chpt. 7–12) ^ a b c Intractability and efficiency and the combinatorial explosion : Russell & Norvig (2021 , p. 21) ^ a b c Psychological evidence of the prevalence of sub-symbolic reasoning and knowledge: Kahneman (2011) , Dreyfus & Dreyfus (1986) , Wason & Shapiro (1966) , Kahneman, Slovic & Tversky (1982) ^ Knowledge representation and knowledge engineering : Russell & Norvig (2021 , chpt. 10), Poole, Mackworth & Goebel (1998 , pp. 23–46, 69–81, 169–233, 235–277, 281–298, 319–345), Luger & Stubblefield (2004 , pp. 227–243), Nilsson (1998 , chpt. 17.1–17.4, 18) ^ Representing categories and relations: Semantic networks , description logics , inheritance (including frames , and scripts ): Russell & Norvig (2021 , §10.2 & 10.5), Poole, Mackworth & Goebel (1998 , pp. 174–177), Luger & Stubblefield (2004 , pp. 248–258), Nilsson (1998 , chpt. 18.3) ^ Representing events and time: Situation calculus , event calculus , fluent calculus (including solving the frame problem ): Russell & Norvig (2021 , §10.3), Poole, Mackworth & Goebel (1998 , pp. 281–298), Nilsson (1998 , chpt. 18.2) ^ Causal calculus : Poole, Mackworth & Goebel (1998 , pp. 335–337) ^ Representing knowledge about knowledge: Belief calculus, modal logics : Russell & Norvig (2021 , §10.4), Poole, Mackworth & Goebel (1998 , pp. 275–277) ^ a b Default reasoning , Frame problem , default logic , non-monotonic logics , circumscription , closed world assumption , abduction : Russell & Norvig (2021 , §10.6), Poole, Mackworth & Goebel (1998 , pp. 248–256, 323–335), Luger & Stubblefield (2004 , pp. 335–363), Nilsson (1998 , ~18.3.3) (Poole et al. places abduction under \"default reasoning\". Luger et al. places this under \"uncertain reasoning\"). ^ a b Breadth of commonsense knowledge: Lenat & Guha (1989 , Introduction), Crevier (1993 , pp. 113–114), Moravec (1988 , p. 13), Russell & Norvig (2021 , pp. 241, 385, 982) ( qualification problem ) ^ Automated planning : Russell & Norvig (2021 , chpt. 11). ^ Automated decision making , Decision theory : Russell & Norvig (2021 , chpt. 16–18). ^ Classical planning : Russell & Norvig (2021 , Section 11.2). ^ Sensorless or \"conformant\" planning, contingent planning, replanning (a.k.a. online planning): Russell & Norvig (2021 , Section 11.5). ^ Uncertain preferences: Russell & Norvig (2021 , Section 16.7) Inverse reinforcement learning : Russell & Norvig (2021 , Section 22.6) ^ Information value theory : Russell & Norvig (2021 , Section 16.6). ^ Markov decision process : Russell & Norvig (2021 , chpt. 17). ^ Game theory and multi-agent decision theory: Russell & Norvig (2021 , chpt. 18). ^ Learning : Russell & Norvig (2021 , chpt. 19–22), Poole, Mackworth & Goebel (1998 , pp. 397–438), Luger & Stubblefield (2004 , pp. 385–542), Nilsson (1998 , chpt. 3.3, 10.3, 17.5, 20) ^ Unsupervised learning : Russell & Norvig (2021 , pp. 653) (definition), Russell & Norvig (2021 , pp. 738–740) ( cluster analysis ), Russell & Norvig (2021 , pp. 846–860) ( word embedding ) ^ a b Supervised learning : Russell & Norvig (2021 , §19.2) (Definition), Russell & Norvig (2021 , Chpt. 19–20) (Techniques) ^ Reinforcement learning : Russell & Norvig (2021 , chpt. 22), Luger & Stubblefield (2004 , pp. 442–449) ^ Transfer learning : Russell & Norvig (2021 , pp. 281), The Economist (2016) ^ \"Artificial Intelligence (AI): What Is AI and How Does It Work? | Built In\" . builtin.com . Retrieved 30 October 2023 . ^ Computational learning theory : Russell & Norvig (2021 , pp. 672–674), Jordan & Mitchell (2015) ^ Natural"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 21,
    "text": "language processing (NLP): Russell & Norvig (2021 , chpt. 23–24), Poole, Mackworth & Goebel (1998 , pp. 91–104), Luger & Stubblefield (2004 , pp. 591–632) ^ Subproblems of NLP : Russell & Norvig (2021 , pp. 849–850) ^ Modern statistical and deep learning approaches to NLP : Russell & Norvig (2021 , chpt. 24), Cambria & White (2014) ^ Computer vision : Russell & Norvig (2021 , chpt. 25), Nilsson (1998 , chpt. 6) ^ Challa et al. (2011) . ^ Affective computing : Thro (1993) , Edelson (1991) , Tao & Tan (2005) , Scassellati (2002) ^ a b Artificial general intelligence : Russell & Norvig (2021 , pp. 32–33, 1020–1021) Proposal for the modern version: Pennachin & Goertzel (2007) Warnings of overspecialization in AI from leading researchers: Nilsson (1995) , McCarthy (2007) , Beal & Winston (2009) ^ Search algorithms : Russell & Norvig (2021 , chpts. 3–5), Poole, Mackworth & Goebel (1998 , pp. 113–163), Luger & Stubblefield (2004 , pp. 79–164, 193–219), Nilsson (1998 , chpts. 7–12) ^ State space search : Russell & Norvig (2021 , chpt. 3) ^ Uninformed searches ( breadth first search , depth-first search and general state space search ): Russell & Norvig (2021 , sect. 3.4), Poole, Mackworth & Goebel (1998 , pp. 113–132), Luger & Stubblefield (2004 , pp. 79–121), Nilsson (1998 , chpt. 8) ^ Heuristic or informed searches (e.g., greedy best first and A* ): Russell & Norvig (2021 , sect. 3.5), Poole, Mackworth & Goebel (1998 , pp. 132–147), Poole & Mackworth (2017 , sect. 3.6), Luger & Stubblefield (2004 , pp. 133–150) ^ Adversarial search : Russell & Norvig (2021 , chpt. 5) ^ Local or \" optimization \" search: Russell & Norvig (2021 , chpt. 4) ^ Singh Chauhan, Nagesh (18 December 2020). \"Optimization Algorithms in Neural Networks\" . KDnuggets . Retrieved 13 January 2024 . ^ Evolutionary computation : Russell & Norvig (2021 , sect. 4.1.2) ^ Logic : Russell & Norvig (2021 , chpts. 6–9), Luger & Stubblefield (2004 , pp. 35–77), Nilsson (1998 , chpt. 13–16) ^ Propositional logic : Russell & Norvig (2021 , chpt. 6), Luger & Stubblefield (2004 , pp. 45–50), Nilsson (1998 , chpt. 13) ^ First-order logic and features such as equality : Russell & Norvig (2021 , chpt. 7), Poole, Mackworth & Goebel (1998 , pp. 268–275), Luger & Stubblefield (2004 , pp. 50–62), Nilsson (1998 , chpt. 15) ^ Logical inference : Russell & Norvig (2021 , chpt. 10) ^ logical deduction as search: Russell & Norvig (2021 , sects. 9.3, 9.4), Poole, Mackworth & Goebel (1998 , pp. ~46–52), Luger & Stubblefield (2004 , pp. 62–73), Nilsson (1998 , chpt. 4.2, 7.2) ^ Resolution and unification : Russell & Norvig (2021 , sections 7.5.2, 9.2, 9.5) ^ Warren, D.H.; Pereira, L.M.; Pereira, F. (1977). \"Prolog-the language and its implementation compared with Lisp\". ACM SIGPLAN Notices . 12 (8): 109– 115. doi : 10.1145/872734.806939 . ^ Fuzzy logic: Russell & Norvig (2021 , pp. 214, 255, 459), Scientific American (1999) ^ a b Stochastic methods for uncertain reasoning: Russell & Norvig (2021 , chpt. 12–18, 20), Poole, Mackworth & Goebel (1998 , pp. 345–395), Luger & Stubblefield (2004 , pp. 165–191, 333–381), Nilsson (1998 , chpt. 19) ^ decision theory and decision analysis : Russell & Norvig (2021 , chpt. 16–18), Poole, Mackworth & Goebel (1998 , pp. 381–394) ^ Information value theory : Russell & Norvig (2021 , sect. 16.6) ^ Markov decision processes and dynamic decision networks : Russell & Norvig (2021 , chpt. 17) ^ a b c Stochastic temporal models: Russell & Norvig (2021 , chpt. 14) Hidden Markov model : Russell & Norvig (2021 , sect. 14.3) Kalman filters : Russell & Norvig (2021 , sect. 14.4) Dynamic Bayesian networks : Russell & Norvig (2021 , sect. 14.5) ^ Game theory and mechanism design : Russell & Norvig (2021 , chpt. 18) ^ Bayesian networks : Russell & Norvig (2021 , sects. 12.5–12.6, 13.4–13.5, 14.3–14.5, 16.5, 20.2–20.3), Poole, Mackworth & Goebel (1998 , pp. 361–381), Luger & Stubblefield (2004 , pp. ~182–190, ≈363–379), Nilsson (1998 , chpt. 19.3–19.4) ^ Bayesian inference algorithm: Russell & Norvig (2021 , sect. 13.3–13.5), Poole, Mackworth & Goebel (1998 , pp. 361–381), Luger & Stubblefield (2004 , pp. ~363–379), Nilsson (1998 , chpt. 19.4 & 7) ^ Bayesian learning and the expectation–maximization algorithm : Russell & Norvig (2021 , chpt. 20), Poole, Mackworth & Goebel (1998 , pp. 424–433), Nilsson (1998 , chpt. 20), Domingos (2015 , p. 210) ^ Bayesian decision theory and Bayesian decision networks : Russell & Norvig (2021 , sect. 16.5) ^ Statistical learning methods and classifiers : Russell & Norvig (2021 , chpt. 20), ^ Ciaramella, Alberto ; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 22,
    "text": "generative AI . Intellisemantic Editions. ISBN 978-8-8947-8760-3 . ^ Decision trees : Russell & Norvig (2021 , sect. 19.3), Domingos (2015 , p. 88) ^ Non-parameteric learning models such as K-nearest neighbor and support vector machines : Russell & Norvig (2021 , sect. 19.7), Domingos (2015 , p. 187) (k-nearest neighbor) ^ Naive Bayes classifier : Russell & Norvig (2021 , sect. 12.6), Domingos (2015 , p. 152) ^ a b Neural networks: Russell & Norvig (2021 , chpt. 21), Domingos (2015 , Chapter 4) ^ Gradient calculation in computational graphs, backpropagation , automatic differentiation : Russell & Norvig (2021 , sect. 21.2), Luger & Stubblefield (2004 , pp. 467–474), Nilsson (1998 , chpt. 3.3) ^ Universal approximation theorem : Russell & Norvig (2021 , p. 752) The theorem: Cybenko (1988) , Hornik, Stinchcombe & White (1989) ^ Feedforward neural networks : Russell & Norvig (2021 , sect. 21.1) ^ Perceptrons : Russell & Norvig (2021 , pp. 21, 22, 683, 22) ^ a b Deep learning : Russell & Norvig (2021 , chpt. 21), Goodfellow, Bengio & Courville (2016) , Hinton et al. (2016) , Schmidhuber (2015) ^ Recurrent neural networks : Russell & Norvig (2021 , sect. 21.6) ^ Convolutional neural networks : Russell & Norvig (2021 , sect. 21.3) ^ Sindhu V, Nivedha S, Prakash M (February 2020). \"An Empirical Science Research on Bioinformatics in Machine Learning\" . Journal of Mechanics of Continua and Mathematical Sciences (7). doi : 10.26782/jmcms.spl.7/2020.02.00006 . ^ Quoted in Christian (2020 , p. 22) ^ Metz, Cade; Weise, Karen (5 May 2025). \"A.I. Hallucinations Are Getting Worse, Even as New Systems Become More Powerful\" . The New York Times . ISSN 0362-4331 . Retrieved 6 May 2025 . ^ \"Explained: Generative AI\" . 9 November 2023. ^ \"AI Writing and Content Creation Tools\" . MIT Sloan Teaching & Learning Technologies. Archived from the original on 25 December 2023 . Retrieved 25 December 2023 . ^ Thomason, James (21 May 2024). \"Mojo Rising: The resurgence of AI-first programming languages\" . VentureBeat . Archived from the original on 27 June 2024 . Retrieved 26 May 2024 . ^ Wodecki, Ben (5 May 2023). \"7 AI Programming Languages You Need to Know\" . AI Business . Archived from the original on 25 July 2024 . Retrieved 5 October 2024 . ^ Plumb, Taryn (18 September 2024). \"Why Jensen Huang and Marc Benioff see 'gigantic' opportunity for agentic AI\" . VentureBeat . Archived from the original on 5 October 2024 . Retrieved 4 October 2024 . ^ Mims, Christopher (19 September 2020). \"Huang's Law Is the New Moore's Law, and Explains Why Nvidia Wants Arm\" . Wall Street Journal . ISSN 0099-9660 . Archived from the original on 2 October 2023 . Retrieved 19 January 2025 . ^ Davenport, T; Kalakota, R (June 2019). \"The potential for artificial intelligence in healthcare\" . Future Healthc J . 6 (2): 94– 98. doi : 10.7861/futurehosp.6-2-94 . PMC 6616181 . PMID 31363513 . ^ Lyakhova, U.A.; Lyakhov, P.A. (2024). \"Systematic review of approaches to detection and classification of skin cancer using artificial intelligence: Development and prospects\" . Computers in Biology and Medicine . 178 : 108742. doi : 10.1016/j.compbiomed.2024.108742 . PMID 38875908 . Archived from the original on 3 December 2024 . Retrieved 10 October 2024 . ^ Alqudaihi, Kawther S.; Aslam, Nida; Khan, Irfan Ullah; Almuhaideb, Abdullah M.; Alsunaidi, Shikah J.; Ibrahim, Nehad M. Abdel Rahman; Alhaidari, Fahd A.; Shaikh, Fatema S.; Alsenbel, Yasmine M.; Alalharith, Dima M.; Alharthi, Hajar M.; Alghamdi, Wejdan M.; Alshahrani, Mohammed S. (2021). \"Cough Sound Detection and Diagnosis Using Artificial Intelligence Techniques: Challenges and Opportunities\" . IEEE Access . 9 : 102327– 102344. Bibcode : 2021IEEEA...9j2327A . doi : 10.1109/ACCESS.2021.3097559 . ISSN 2169-3536 . PMC 8545201 . PMID 34786317 . ^ a b Bax, Monique; Thorpe, Jordan; Romanov, Valentin (December 2023). \"The future of personalized cardiovascular medicine demands 3D and 4D printing, stem cells, and artificial intelligence\" . Frontiers in Sensors . 4 . doi : 10.3389/fsens.2023.1294721 . ISSN 2673-5067 . ^ Dankwa-Mullan, Irene (2024). \"Health Equity and Ethical Considerations in Using Artificial Intelligence in Public Health and Medicine\" . Preventing Chronic Disease . 21 : E64. doi : 10.5888/pcd21.240245 . ISSN 1545-1151 . PMC 11364282 . PMID 39173183 . ^ Jumper, J; Evans, R; Pritzel, A (2021). \"Highly accurate protein structure prediction with AlphaFold\" . Nature . 596 (7873): 583– 589. Bibcode : 2021Natur.596..583J . doi : 10.1038/s41586-021-03819-2 . PMC 8371605 . PMID 34265844 . ^ \"AI discovers new class of antibiotics to kill drug-resistant bacteria\" . 20 December 2023. Archived from the original on 16 September 2024 . Retrieved 5 October 2024 . ^ \"AI speeds up drug design for Parkinson's ten-fold\" . Cambridge University. 17 April 2024. Archived from the original on 5 October 2024 . Retrieved 5 October 2024"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 23,
    "text": ". ^ Horne, Robert I.; Andrzejewska, Ewa A.; Alam, Parvez; Brotzakis, Z. Faidon; Srivastava, Ankit; Aubert, Alice; Nowinska, Magdalena; Gregory, Rebecca C.; Staats, Roxine; Possenti, Andrea; Chia, Sean; Sormanni, Pietro; Ghetti, Bernardino; Caughey, Byron; Knowles, Tuomas P. J.; Vendruscolo, Michele (17 April 2024). \"Discovery of potent inhibitors of α-synuclein aggregation using structure-based iterative learning\" . Nature Chemical Biology . 20 (5). Nature: 634– 645. doi : 10.1038/s41589-024-01580-x . PMC 11062903 . PMID 38632492 . ^ Grant, Eugene F.; Lardner, Rex (25 July 1952). \"The Talk of the Town – It\" . The New Yorker . ISSN 0028-792X . Archived from the original on 16 February 2020 . Retrieved 28 January 2024 . ^ Anderson, Mark Robert (11 May 2017). \"Twenty years on from Deep Blue vs Kasparov: how a chess match started the big data revolution\" . The Conversation . Archived from the original on 17 September 2024 . Retrieved 28 January 2024 . ^ Markoff, John (16 February 2011). \"Computer Wins on 'Jeopardy!': Trivial, It's Not\" . The New York Times . ISSN 0362-4331 . Archived from the original on 22 October 2014 . Retrieved 28 January 2024 . ^ Byford, Sam (27 May 2017). \"AlphaGo retires from competitive Go after defeating world number one 3–0\" . The Verge . Archived from the original on 7 June 2017 . Retrieved 28 January 2024 . ^ Brown, Noam; Sandholm, Tuomas (30 August 2019). \"Superhuman AI for multiplayer poker\" . Science . 365 (6456): 885– 890. Bibcode : 2019Sci...365..885B . doi : 10.1126/science.aay2400 . ISSN 0036-8075 . PMID 31296650 . ^ \"MuZero: Mastering Go, chess, shogi and Atari without rules\" . Google DeepMind . 23 December 2020 . Retrieved 28 January 2024 . ^ Sample, Ian (30 October 2019). \"AI becomes grandmaster in 'fiendishly complex' StarCraft II\" . The Guardian . ISSN 0261-3077 . Archived from the original on 29 December 2020 . Retrieved 28 January 2024 . ^ Wurman, P. R.; Barrett, S.; Kawamoto, K. (2022). \"Outracing champion Gran Turismo drivers with deep reinforcement learning\" (PDF) . Nature . 602 (7896): 223– 228. Bibcode : 2022Natur.602..223W . doi : 10.1038/s41586-021-04357-7 . PMID 35140384 . ^ Wilkins, Alex (13 March 2024). \"Google AI learns to play open-world video games by watching them\" . New Scientist . Archived from the original on 26 July 2024 . Retrieved 21 July 2024 . ^ Wu, Zhengxuan; Arora, Aryaman; Wang, Zheng; Geiger, Atticus; Jurafsky, Dan; Manning, Christopher D.; Potts, Christopher (2024). \"ReFT: Representation Finetuning for Language Models\". NeurIPS . arXiv : 2404.03592 . ^ \"Improving mathematical reasoning with process supervision\" . OpenAI . 31 May 2023 . Retrieved 26 January 2025 . ^ Srivastava, Saurabh (29 February 2024). \"Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap\". arXiv : 2402.19450 [ cs.AI ]. ^ Lightman, Hunter; Kosaraju, Vineet; Burda, Yura; Edwards, Harri; Baker, Bowen; Lee, Teddy; Leike, Jan; Schulman, John; Sutskever, Ilya; Cobbe, Karl (2023). \"Let's Verify Step by Step\". arXiv : 2305.20050v1 [ cs.LG ]. ^ Franzen, Carl (8 August 2024). \"Alibaba claims no. 1 spot in AI math models with Qwen2-Math\" . VentureBeat . Retrieved 16 February 2025 . ^ Franzen, Carl (9 January 2025). \"Microsoft's new rStar-Math technique upgrades small models to outperform OpenAI's o1-preview at math problems\" . VentureBeat . Retrieved 26 January 2025 . ^ Gina Genkina: New AI Model Advances the “Kissing Problem” and More. AlphaEvolve made several mathematical discoveries and practical optimizations IEEE Spectrum 2025-05-14. Retrieved 2025-06-07 ^ Roberts, Siobhan (25 July 2024). \"AI achieves silver-medal standard solving International Mathematical Olympiad problems\" . The New York Times . Archived from the original on 26 September 2024 . Retrieved 7 August 2024 . ^ Azerbayev, Zhangir; Schoelkopf, Hailey; Paster, Keiran; Santos, Marco Dos; McAleer', Stephen; Jiang, Albert Q.; Deng, Jia; Biderman, Stella; Welleck, Sean (16 October 2023). \"Llemma: An Open Language Model For Mathematics\" . EleutherAI Blog . Retrieved 26 January 2025 . ^ \"Julius AI\" . julius.ai . ^ Metz, Cade (21 July 2025). \"Google A.I. System Wins Gold Medal in International Math Olympiad\" . The New York Times . ISSN 0362-4331 . Retrieved 24 July 2025 . ^ McFarland, Alex (12 July 2024). \"8 Best AI for Math Tools (January 2025)\" . Unite.AI . Retrieved 26 January 2025 . ^ Matthew Finio & Amanda Downie: IBM Think 2024 Primer, \"What is Artificial Intelligence (AI) in Finance?\" 8 Dec. 2023 ^ M. Nicolas, J. Firzli: Pensions Age / European Pensions magazine, \"Artificial Intelligence: Ask the Industry\", May–June 2024. https://videovoice.org/ai-in-finance-innovation-entrepreneurship-vs-over-regulation-with-the-eus-artificial-intelligence-act-wont-work-as-intended/ Archived 11 September 2024 at the Wayback Machine . ^ a b c Congressional Research Service (2019). Artificial Intelligence and National Security (PDF) . Washington, DC: Congressional Research Service. Archived (PDF) from the original on 8 May 2020 . Retrieved 25 February 2024 . PD-notice ^ a b Slyusar, Vadym (2019). Artificial intelligence as the basis"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 24,
    "text": "of future control networks (Preprint). doi : 10.13140/RG.2.2.30247.50087 . ^ Iraqi, Amjad (3 April 2024). \" 'Lavender': The AI machine directing Israel's bombing spree in Gaza\" . +972 Magazine . Archived from the original on 10 October 2024 . Retrieved 6 April 2024 . ^ Davies, Harry; McKernan, Bethan; Sabbagh, Dan (1 December 2023). \" 'The Gospel': how Israel uses AI to select bombing targets in Gaza\" . The Guardian . Archived from the original on 6 December 2023 . Retrieved 4 December 2023 . ^ Marti, J Werner (10 August 2024). \"Drohnen haben den Krieg in der Ukraine revolutioniert, doch sie sind empfindlich auf Störsender – deshalb sollen sie jetzt autonom operieren\" . Neue Zürcher Zeitung (in German). Archived from the original on 10 August 2024 . Retrieved 10 August 2024 . ^ Newsom, Gavin; Weber, Shirley N. (5 September 2023). \"Executive Order N-12-23\" (PDF) . Executive Department, State of California. Archived (PDF) from the original on 21 February 2024 . Retrieved 7 September 2023 . ^ ^ \"What is ChatGPT, DALL-E, and generative AI?\" . McKinsey . Archived from the original on 23 April 2023 . Retrieved 14 December 2024 . ^ \"What is generative AI?\" . IBM . 22 March 2024. Archived from the original on 13 December 2024 . Retrieved 13 December 2024 . ^ Pasick, Adam (27 March 2023). \"Artificial Intelligence Glossary: Neural Networks and Other Terms Explained\" . The New York Times . ISSN 0362-4331 . Archived from the original on 1 September 2023 . Retrieved 22 April 2023 . ^ Karpathy, Andrej; Abbeel, Pieter; Brockman, Greg; Chen, Peter; Cheung, Vicki; Duan, Yan; Goodfellow, Ian; Kingma, Durk; Ho, Jonathan; Rein Houthooft; Tim Salimans; John Schulman; Ilya Sutskever; Wojciech Zaremba (16 June 2016). \"Generative models\" . OpenAI. Archived from the original on 17 November 2023 . Retrieved 15 March 2023 . ^ a b Griffith, Erin; Metz, Cade (27 January 2023). \"Anthropic Said to Be Closing In on $300 Million in New A.I. Funding\" . The New York Times . Archived from the original on 9 December 2023 . Retrieved 14 March 2023 . ^ Lanxon, Nate; Bass, Dina; Davalos, Jackie (10 March 2023). \"A Cheat Sheet to AI Buzzwords and Their Meanings\" . Bloomberg News . Archived from the original on 17 November 2023 . Retrieved 14 March 2023 . ^ Metz, Cade (14 March 2023). \"OpenAI Plans to Up the Ante in Tech's A.I. Race\" . The New York Times . ISSN 0362-4331 . Archived from the original on 31 March 2023 . Retrieved 31 March 2023 . ^ Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer, Noam; Kulshreshtha, Apoorv (20 January 2022). \"LaMDA: Language Models for Dialog Applications\". arXiv : 2201.08239 [ cs.CL ]. ^ Roose, Kevin (21 October 2022). \"A Coming-Out Party for Generative A.I., Silicon Valley's New Craze\" . The New York Times . Archived from the original on 15 February 2023 . Retrieved 14 March 2023 . ^ Metz, Cade (15 February 2024). \"OpenAI Unveils A.I. That Instantly Generates Eye-Popping Videos\" . The New York Times . ISSN 0362-4331 . Archived from the original on 15 February 2024 . Retrieved 16 February 2024 . ^ Fink, Charlie. \"LTX Video Breaks The 60-Second Barrier, Redefining AI Video As A Longform Medium\" . Forbes . Retrieved 24 July 2025 . ^ \"The race of the AI labs heats up\" . The Economist . 30 January 2023. Archived from the original on 17 November 2023 . Retrieved 14 March 2023 . ^ Yang, June; Gokturk, Burak (14 March 2023). \"Google Cloud brings generative AI to developers, businesses, and governments\" . Archived from the original on 17 November 2023 . Retrieved 15 March 2023 . ^ Taeihagh, Araz (4 April 2025). \"Governance of Generative AI\" . Policy and Society . 44 (1): 1– 22. doi : 10.1093/polsoc/puaf001 . ISSN 1449-4035 . ^ Simon, Felix M.; Altay, Sacha; Mercier, Hugo (18 October 2023). \"Misinformation reloaded? Fears about the impact of generative AI on misinformation are overblown\" (PDF) . Harvard Kennedy School Misinformation Review . doi : 10.37016/mr-2020-127 . S2CID 264113883 . Retrieved 16 November 2023 . ^ Hendrix, Justin (16 May 2023). \"Transcript: Senate Judiciary Subcommittee Hearing on Oversight of AI\" . techpolicy.press . Archived from the original on 17 November 2023 . Retrieved 19 May 2023 . ^ \"New AI systems collide with copyright law\" . BBC News . 1 August 2023 . Retrieved 28 September 2024 . ^ Poole, David; Mackworth, Alan (2023). Artificial Intelligence, Foundations of Computational Agents (3rd ed.). Cambridge University Press. doi : 10.1017/9781009258227 . ISBN 978-1-0092-5819-7 . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ Russell, Stuart; Norvig, Peter (2020). Artificial Intelligence: A Modern Approach (4th ed.). Pearson. ISBN 978-0-1346-1099-3 . ^ \"Why agents are the next frontier of generative AI\""
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 25,
    "text": ". McKinsey Digital . 24 July 2024. Archived from the original on 3 October 2024 . Retrieved 10 August 2024 . ^ Figueiredo, Mayara Costa; Ankrah, Elizabeth; Powell, Jacquelyn E.; Epstein, Daniel A.; Chen, Yunan (12 January 2024). \"Powered by AI: Examining How AI Descriptions Influence Perceptions of Fertility Tracking Applications\" . Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies . 7 (4): 1– 24. doi : 10.1145/3631414 . ^ Power, Jennifer; Pym, Tinonee; James, Alexandra; Waling, Andrea (5 July 2024). \"Smart Sex Toys: A Narrative Review of Recent Research on Cultural, Health and Safety Considerations\" . Current Sexual Health Reports . 16 (3): 199– 215. doi : 10.1007/s11930-024-00392-3 . ISSN 1548-3592 . ^ Marcantonio, Tiffany L.; Avery, Gracie; Thrash, Anna; Leone, Ruschelle M. (10 September 2024). \"Large Language Models in an App: Conducting a Qualitative Synthetic Data Analysis of How Snapchat's \"My AI\" Responds to Questions About Sexual Consent, Sexual Refusals, Sexual Assault, and Sexting\" . The Journal of Sex Research : 1– 15. doi : 10.1080/00224499.2024.2396457 . ISSN 0022-4499 . PMC 11891083. PMID 39254628 . Archived from the original on 9 December 2024 . Retrieved 9 December 2024 . ^ Hanson, Kenneth R.; Bolthouse, Hannah (2024). \" \"Replika Removing Erotic Role-Play Is Like Grand Theft Auto Removing Guns or Cars\": Reddit Discourse on Artificial Intelligence Chatbots and Sexual Technologies\" . Socius: Sociological Research for a Dynamic World . 10 . doi : 10.1177/23780231241259627 . ISSN 2378-0231 . ^ Mania, Karolina (1 January 2024). \"Legal Protection of Revenge and Deepfake Porn Victims in the European Union: Findings From a Comparative Legal Study\" . Trauma, Violence, & Abuse . 25 (1): 117– 129. doi : 10.1177/15248380221143772 . ISSN 1524-8380 . PMID 36565267 . ^ Singh, Suyesha; Nambiar, Vaishnavi (2024). \"Role of Artificial Intelligence in the Prevention of Online Child Sexual Abuse: A Systematic Review of Literature\" . Journal of Applied Security Research . 19 (4): 586– 627. doi : 10.1080/19361610.2024.2331885 . ISSN 1936-1610 . Archived from the original on 9 December 2024 . Retrieved 9 December 2024 . ^ Razi, Afsaneh; Kim, Seunghyun; Alsoubai, Ashwaq; Stringhini, Gianluca; Solorio, Thamar; De Choudhury, Munmun ; Wisniewski, Pamela J. (13 October 2021). \"A Human-Centered Systematic Literature Review of the Computational Approaches for Online Sexual Risk Detection\" . Proceedings of the ACM on Human-Computer Interaction . 5 (CSCW2): 1– 38. doi : 10.1145/3479609 . ISSN 2573-0142 . Archived from the original on 9 December 2024 . Retrieved 9 December 2024 . ^ Ransbotham, Sam; Kiron, David; Gerbert, Philipp; Reeves, Martin (6 September 2017). \"Reshaping Business With Artificial Intelligence\" . MIT Sloan Management Review . Archived from the original on 13 February 2024. ^ Sun, Yuran; Zhao, Xilei; Lovreglio, Ruggiero; Kuligowski, Erica (1 January 2024), Naser, M. Z. (ed.), \"8 – AI for large-scale evacuation modeling: promises and challenges\" , Interpretable Machine Learning for the Analysis, Design, Assessment, and Informed Decision Making for Civil Infrastructure , Woodhead Publishing Series in Civil and Structural Engineering, Woodhead Publishing, pp. 185– 204, ISBN 978-0-1282-4073-1 , archived from the original on 19 May 2024 , retrieved 28 June 2024 . ^ Gomaa, Islam; Adelzadeh, Masoud; Gwynne, Steven; Spencer, Bruce; Ko, Yoon; Bénichou, Noureddine; Ma, Chunyun; Elsagan, Nour; Duong, Dana; Zalok, Ehab; Kinateder, Max (1 November 2021). \"A Framework for Intelligent Fire Detection and Evacuation System\" . Fire Technology . 57 (6): 3179– 3185. doi : 10.1007/s10694-021-01157-3 . ISSN 1572-8099 . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ Zhao, Xilei; Lovreglio, Ruggiero; Nilsson, Daniel (1 May 2020). \"Modelling and interpreting pre-evacuation decision-making using machine learning\" . Automation in Construction . 113 : 103140. doi : 10.1016/j.autcon.2020.103140 . hdl : 10179/17315 . ISSN 0926-5805 . Archived from the original on 19 May 2024 . Retrieved 5 October 2024 . ^ \"India's latest election embraced AI technology. Here are some ways it was used constructively\" . PBS News . 12 June 2024. Archived from the original on 17 September 2024 . Retrieved 28 October 2024 . ^ \"Экономист Дарон Асемоглу написал книгу об угрозах искусственного интеллекта — и о том, как правильное управление может обратить его на пользу человечеству Спецкор \"Медузы\" Маргарита Лютова узнала у ученого, как скоро мир сможет приблизиться к этой утопии\" . Meduza (in Russian). Archived from the original on 20 June 2023 . Retrieved 21 June 2023 . ^ \"Learning, thinking, artistic collaboration and other such human endeavours in the age of AI\" . The Hindu . 2 June 2023. Archived from the original on 21 June 2023 . Retrieved 21 June 2023 . ^ Müller, Vincent C. (30 April 2020). \"Ethics of Artificial Intelligence and Robotics\" . Stanford Encyclopedia of Philosophy Archive . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ \"Assessing potential future artificial intelligence risks, benefits and"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 26,
    "text": "policy imperatives\" . OECD . 14 November 2024 . Retrieved 1 August 2025 . ^ Kopel, Matthew. \"Copyright Services: Fair Use\" . Cornell University Library . Archived from the original on 26 September 2024 . Retrieved 26 April 2024 . ^ Burgess, Matt. \"How to Stop Your Data From Being Used to Train AI\" . Wired . ISSN 1059-1028 . Archived from the original on 3 October 2024 . Retrieved 26 April 2024 . ^ \"Getting the Innovation Ecosystem Ready for AI. An IP policy toolkit\" (PDF) . WIPO . ^ Hammond, George (27 December 2023). \"Big Tech is spending more than VC firms on AI startups\" . Ars Technica . Archived from the original on 10 January 2024. ^ Wong, Matteo (24 October 2023). \"The Future of AI Is GOMA\" . The Atlantic . Archived from the original on 5 January 2024. ^ \"Big tech and the pursuit of AI dominance\" . The Economist . 26 March 2023. Archived from the original on 29 December 2023. ^ Fung, Brian (19 December 2023). \"Where the battle to dominate AI may be won\" . CNN Business . Archived from the original on 13 January 2024. ^ Metz, Cade (5 July 2023). \"In the Age of A.I., Tech's Little Guys Need Big Friends\" . The New York Times . Archived from the original on 8 July 2024 . Retrieved 5 October 2024 . ^ \"Electricity 2024 – Analysis\" . IEA . 24 January 2024 . Retrieved 13 July 2024 . ^ Calvert, Brian (28 March 2024). \"AI already uses as much energy as a small country. It's only the beginning\" . Vox . New York, New York. Archived from the original on 3 July 2024 . Retrieved 5 October 2024 . ^ Halper, Evan; O'Donovan, Caroline (21 June 2024). \"AI is exhausting the power grid. Tech firms are seeking a miracle solution\" . Washington Post . ^ Davenport, Carly. \"AI Data Centers and the Coming YS Power Demand Surge\" (PDF) . Goldman Sachs . Archived from the original (PDF) on 26 July 2024 . Retrieved 5 October 2024 . ^ Ryan, Carol (12 April 2024). \"Energy-Guzzling AI Is Also the Future of Energy Savings\" . Wall Street Journal . Dow Jones. ^ Hiller, Jennifer (1 July 2024). \"Tech Industry Wants to Lock Up Nuclear Power for AI\" . Wall Street Journal . Dow Jones. Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ Kendall, Tyler (28 September 2024). \"Nvidia's Huang Says Nuclear Power an Option to Feed Data Centers\" . Bloomberg . ^ Halper, Evan (20 September 2024). \"Microsoft deal would reopen Three Mile Island nuclear plant to power AI\" . Washington Post . ^ Hiller, Jennifer (20 September 2024). \"Three Mile Island's Nuclear Plant to Reopen, Help Power Microsoft's AI Centers\" . Wall Street Journal . Dow Jones. Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ a b c Niva Yadav (19 August 2024). \"Taiwan to stop large data centers in the North, cites insufficient power\" . DatacenterDynamics. Archived from the original on 8 November 2024 . Retrieved 7 November 2024 . ^ a b Mochizuki, Takashi; Oda, Shoko (18 October 2024). \"エヌビディア出資の日本企業、原発近くでＡＩデータセンター新設検討\" . Bloomberg (in Japanese). Archived from the original on 8 November 2024 . Retrieved 7 November 2024 . ^ a b Naureen S Malik and Will Wade (5 November 2024). \"Nuclear-Hungry AI Campuses Need New Plan to Find Power Fast\" . Bloomberg. ^ \"Energy and AI Executive summary\" . International Energy Agency . Retrieved 10 April 2025 . ^ Rainie, Lee; Keeter, Scott; Perrin, Andrew (22 July 2019). \"Trust and Distrust in America\" . Pew Research Center . Archived from the original on 22 February 2024. ^ Kosoff, Maya (8 February 2018). \"YouTube Struggles to Contain Its Conspiracy Problem\" . Vanity Fair . Retrieved 10 April 2025 . ^ Berry, David M. (19 March 2025). \"Synthetic media and computational capitalism: towards a critical theory of artificial intelligence\" . AI & Society . doi : 10.1007/s00146-025-02265-2 . ISSN 1435-5655 . ^ \"Unreal: A quantum leap in AI video\" . The Week . 17 June 2025 . Retrieved 20 June 2025 . ^ Snow, Jackie. \"AI video is getting real. Beware what comes next\" . Quartz . Retrieved 20 June 2025 . ^ Chow, Andrew R.; Perrigo, Billy (3 June 2025). \"Google's New AI Tool Generates Convincing Deepfakes of Riots, Conflict, and Election Fraud\" . Time . Retrieved 20 June 2025 . ^ Olanipekun, Samson Olufemi (2025). \"Computational propaganda and misinformation: AI technologies as tools of media manipulation\" . World Journal of Advanced Research and Reviews . 25 (1): 911– 923. doi : 10.30574/wjarr.2025.25.1.0131 . ISSN 2581-9615 . ^ \"To fight AI, we need 'personhood credentials,' say AI firms\" . Archived from the original on 24 April"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 27,
    "text": "2025 . Retrieved 9 May 2025 . ^ a b Samuel, Sigal (19 April 2022). \"Why it's so damn hard to make AI fair and unbiased\" . Vox . Archived from the original on 5 October 2024 . Retrieved 24 July 2024 . ^ Berdahl et al. (2023) ; Goffrey (2008 , p. 17); Rose (2023) ; Russell & Norvig (2021 , p. 995) ^ Christian (2020 , pp. 67–70); Russell & Norvig (2021 , pp. 993–994) ^ Russell & Norvig (2021 , p. 995); Lipartito (2011 , p. 36); Goodman & Flaxman (2017 , p. 6); Christian (2020 , pp. 39–40, 65) ^ Quoted in Christian (2020 , p. 65). ^ Russell & Norvig (2021 , p. 994); Christian (2020 , pp. 40, 80–81) ^ Quoted in Christian (2020 , p. 80) ^ \"Black Box AI\" . 16 June 2023. Archived from the original on 15 June 2024 . Retrieved 5 October 2024 . ^ Christian (2020 , p. 83); Russell & Norvig (2021 , p. 997) ^ Ropek, Lucas (21 May 2024). \"New Anthropic Research Sheds Light on AI's 'Black Box' \" . Gizmodo . Archived from the original on 5 October 2024 . Retrieved 23 May 2024 . ^ Robitzski (2018) ; Sainato (2015) ^ Buckley, Chris; Mozur, Paul (22 May 2019). \"How China Uses High-Tech Surveillance to Subdue Minorities\" . The New York Times . Archived from the original on 25 November 2019 . Retrieved 2 July 2019 . ^ \"Security lapse exposed a Chinese smart city surveillance system\" . 3 May 2019. Archived from the original on 7 March 2021 . Retrieved 14 September 2020 . ^ a b E. McGaughey, 'Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy' (2022), 51(3) Industrial Law Journal 511–559 . Archived 27 May 2023 at the Wayback Machine . ^ Ford & Colvin (2015) ; McGaughey (2022) ^ Lohr (2017) ; Frey & Osborne (2017) ; Arntz, Gregory & Zierahn (2016 , p. 33) ^ Zhou, Viola (11 April 2023). \"AI is already taking video game illustrators' jobs in China\" . Rest of World . Archived from the original on 21 February 2024 . Retrieved 17 August 2023 . ^ Carter, Justin (11 April 2023). \"China's game art industry reportedly decimated by growing AI use\" . Game Developer . Archived from the original on 17 August 2023 . Retrieved 17 August 2023 . ^ Mahdawi (2017) ; Thompson (2014) ^ Tarnoff, Ben (4 August 2023). \"Lessons from Eliza\". The Guardian Weekly . pp. 34– 39. ^ Bostrom (2014) ; Müller & Bostrom (2014) ; Bostrom (2015) . ^ Leaders' concerns about the existential risks of AI around 2015: Rawlinson (2015) , Holley (2015) , Gibbs (2014) , Sainato (2015) ^ \" \"Godfather of artificial intelligence\" talks impact and potential of new AI\" . CBS News . 25 March 2023. Archived from the original on 28 March 2023 . Retrieved 28 March 2023 . ^ Pittis, Don (4 May 2023). \"Canadian artificial intelligence leader Geoffrey Hinton piles on fears of computer takeover\" . CBC . Archived from the original on 7 July 2024 . Retrieved 5 October 2024 . ^ \" '50–50 chance' that AI outsmarts humanity, Geoffrey Hinton says\" . Bloomberg BNN . 14 June 2024. Archived from the original on 14 June 2024 . Retrieved 6 July 2024 . ^ Taylor, Josh (7 May 2023). \"Rise of artificial intelligence is inevitable but should not be feared, 'father of AI' says\" . The Guardian . Archived from the original on 23 October 2023 . Retrieved 26 May 2023 . ^ Colton, Emma (7 May 2023). \" 'Father of AI' says tech fears misplaced: 'You cannot stop it' \" . Fox News . Archived from the original on 26 May 2023 . Retrieved 26 May 2023 . ^ Jones, Hessie (23 May 2023). \"Juergen Schmidhuber, Renowned 'Father Of Modern AI,' Says His Life's Work Won't Lead To Dystopia\" . Forbes . Archived from the original on 26 May 2023 . Retrieved 26 May 2023 . ^ McMorrow, Ryan (19 December 2023). \"Andrew Ng: 'Do we think the world is better off with more or less intelligence?' \" . Financial Times . Archived from the original on 25 January 2024 . Retrieved 30 December 2023 . ^ Levy, Steven (22 December 2023). \"How Not to Be Stupid About AI, With Yann LeCun\" . Wired . Archived from the original on 28 December 2023 . Retrieved 30 December 2023 . ^ Arguments that AI is not an imminent risk: Brooks (2014) , Geist (2015) , Madrigal (2015) , Lee (2014) ^ a b Anderson & Anderson (2011) . ^ Stewart, Ashley; Melton, Monica. \"Hugging Face CEO says he's focused on building a 'sustainable model' for the $4.5 billion open-source-AI startup\" . Business Insider . Archived from the"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 28,
    "text": "original on 25 September 2024 . Retrieved 14 April 2024 . ^ Wiggers, Kyle (9 April 2024). \"Google open sources tools to support AI model development\" . TechCrunch . Archived from the original on 10 September 2024 . Retrieved 14 April 2024 . ^ Heaven, Will Douglas (12 May 2023). \"The open-source AI boom is built on Big Tech's handouts. How long will it last?\" . MIT Technology Review . Retrieved 14 April 2024 . ^ Brodsky, Sascha (19 December 2023). \"Mistral AI's New Language Model Aims for Open Source Supremacy\" . AI Business . Archived from the original on 5 September 2024 . Retrieved 5 October 2024 . ^ Edwards, Benj (22 February 2024). \"Stability announces Stable Diffusion 3, a next-gen AI image generator\" . Ars Technica . Archived from the original on 5 October 2024 . Retrieved 14 April 2024 . ^ Marshall, Matt (29 January 2024). \"How enterprises are using open source LLMs: 16 examples\" . VentureBeat . Archived from the original on 26 September 2024 . Retrieved 5 October 2024 . ^ Piper, Kelsey (2 February 2024). \"Should we make our most powerful AI models open source to all?\" . Vox . Archived from the original on 5 October 2024 . Retrieved 14 April 2024 . ^ Alan Turing Institute (2019). \"Understanding artificial intelligence ethics and safety\" (PDF) . Archived (PDF) from the original on 11 September 2024 . Retrieved 5 October 2024 . ^ Alan Turing Institute (2023). \"AI Ethics and Governance in Practice\" (PDF) . Archived (PDF) from the original on 11 September 2024 . Retrieved 5 October 2024 . ^ Floridi, Luciano; Cowls, Josh (23 June 2019). \"A Unified Framework of Five Principles for AI in Society\" . Harvard Data Science Review . 1 (1). doi : 10.1162/99608f92.8cd550d1 . S2CID 198775713 . Archived from the original on 7 August 2019 . Retrieved 5 December 2023 . ^ Buruk, Banu; Ekmekci, Perihan Elif; Arda, Berna (1 September 2020). \"A critical perspective on guidelines for responsible and trustworthy artificial intelligence\" . Medicine, Health Care and Philosophy . 23 (3): 387– 399. doi : 10.1007/s11019-020-09948-1 . ISSN 1572-8633 . PMID 32236794 . S2CID 214766800 . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ Kamila, Manoj Kumar; Jasrotia, Sahil Singh (1 January 2023). \"Ethical issues in the development of artificial intelligence: recognizing the risks\" . International Journal of Ethics and Systems . 41 (ahead-of-print): 45– 63. doi : 10.1108/IJOES-05-2023-0107 . ISSN 2514-9369 . S2CID 259614124 . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ \"AI Safety Institute releases new AI safety evaluations platform\" . UK Government. 10 May 2024. Archived from the original on 5 October 2024 . Retrieved 14 May 2024 . ^ Regulation of AI to mitigate risks: Berryhill et al. (2019) , Barfield & Pagallo (2018) , Iphofen & Kritikos (2019) , Wirtz, Weyerer & Geyer (2018) , Buiten (2019) ^ VOA News (25 October 2023). \"UN Announces Advisory Body on Artificial Intelligence\" . Archived from the original on 18 September 2024 . Retrieved 5 October 2024 . ^ \"Council of Europe opens first ever global treaty on AI for signature\" . Council of Europe . 5 September 2024. Archived from the original on 17 September 2024 . Retrieved 17 September 2024 . ^ Milmo, Dan (3 November 2023). \"Hope or Horror? The great AI debate dividing its pioneers\". The Guardian Weekly . pp. 10– 12. ^ \"The Bletchley Declaration by Countries Attending the AI Safety Summit, 1–2 November 2023\" . GOV.UK . 1 November 2023. Archived from the original on 1 November 2023 . Retrieved 2 November 2023 . ^ \"Countries agree to safe and responsible development of frontier AI in landmark Bletchley Declaration\" . GOV.UK (Press release). Archived from the original on 1 November 2023 . Retrieved 1 November 2023 . ^ \"Second global AI summit secures safety commitments from companies\" . Reuters. 21 May 2024 . Retrieved 23 May 2024 . ^ \"Frontier AI Safety Commitments, AI Seoul Summit 2024\" . gov.uk. 21 May 2024. Archived from the original on 23 May 2024 . Retrieved 23 May 2024 . ^ a b Buntz, Brian (3 November 2024). \"Quality vs. quantity: US and China chart different paths in global AI patent race in 2024 / Geographical breakdown of AI patents in 2024\" . R&D World. Archived from the original on 9 December 2024. ^ a b c Copeland, J., ed. (2004). The Essential Turing: the ideas that gave birth to the computer age . Oxford, England: Clarendon Press. ISBN 0-1982-5079-7 . ^ \"Google books ngram\" . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ AI's immediate precursors: McCorduck (2004 , pp. 51–107), Crevier (1993 , pp. 27–32), Russell & Norvig (2021"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 29,
    "text": ", pp. 8–17), Moravec (1988 , p. 3) ^ a b Turing's original publication of the Turing test in \" Computing machinery and intelligence \": Turing (1950) Historical influence and philosophical implications: Haugeland (1985 , pp. 6–9), Crevier (1993 , p. 24), McCorduck (2004 , pp. 70–71), Russell & Norvig (2021 , pp. 2, 984) ^ Simon (1965 , p. 96) quoted in Crevier (1993 , p. 109) ^ Minsky (1967 , p. 2) quoted in Crevier (1993 , p. 109) ^ Expert systems : Russell & Norvig (2021 , pp. 23, 292), Luger & Stubblefield (2004 , pp. 227–331), Nilsson (1998 , chpt. 17.4), McCorduck (2004 , pp. 327–335, 434–435), Crevier (1993 , pp. 145–162, 197–203), Newquist (1994 , pp. 155–183) ^ Developmental robotics : Weng et al. (2001) , Lungarella et al. (2003) , Asada et al. (2009) , Oudeyer (2010) ^ Crevier (1993 , pp. 214–215), Russell & Norvig (2021 , pp. 24, 26) ^ Formal and narrow methods adopted in the 1990s: Russell & Norvig (2021 , pp. 24–26), McCorduck (2004 , pp. 486–487) ^ AI widely used in the late 1990s: Kurzweil (2005 , p. 265), NRC (1999 , pp. 216–222), Newquist (1994 , pp. 189–201) ^ Moore's Law and AI: Russell & Norvig (2021 , pp. 14, 27) ^ Big data : Russell & Norvig (2021 , p. 26) ^ Sagar, Ram (3 June 2020). \"OpenAI Releases GPT-3, The Largest Model So Far\" . Analytics India Magazine . Archived from the original on 4 August 2020 . Retrieved 15 March 2023 . ^ Milmo, Dan (2 February 2023). \"ChatGPT reaches 100 million users two months after launch\" . The Guardian . ISSN 0261-3077 . Archived from the original on 3 February 2023 . Retrieved 31 December 2024 . ^ Gorichanaz, Tim (29 November 2023). \"ChatGPT turns 1: AI chatbot's success says as much about humans as technology\" . The Conversation . Archived from the original on 31 December 2024 . Retrieved 31 December 2024 . ^ \"Nearly 1 in 4 new startups is an AI company\" . PitchBook . 24 December 2024 . Retrieved 3 January 2025 . ^ Grayling, Anthony; Ball, Brian (1 August 2024). \"Philosophy is crucial in the age of AI\" . The Conversation . Archived from the original on 5 October 2024 . Retrieved 4 October 2024 . ^ a b Jarow, Oshan (15 June 2024). \"Will AI ever become conscious? It depends on how you think about biology\" . Vox . Archived from the original on 21 September 2024 . Retrieved 4 October 2024 . ^ McCarthy, John. \"The Philosophy of AI and the AI of Philosophy\" . jmc.stanford.edu . Archived from the original on 23 October 2018 . Retrieved 3 October 2024 . ^ Kirk-Giannini, Cameron Domenico; Goldstein, Simon (16 October 2023). \"AI is closer than ever to passing the Turing test for 'intelligence'. What happens when it does?\" . The Conversation . Archived from the original on 25 September 2024 . Retrieved 17 August 2024 . ^ \"What Is Artificial Intelligence (AI)?\" . Google Cloud Platform . Archived from the original on 31 July 2023 . Retrieved 16 October 2023 . ^ \"One of the Biggest Problems in Regulating AI Is Agreeing on a Definition\" . Carnegie Endowment for International Peace . Retrieved 31 July 2024 . ^ \"AI or BS? How to tell if a marketing tool really uses artificial intelligence\" . The Drum . Retrieved 31 July 2024 . ^ Musser, George (1 September 2023). \"How AI Knows Things No One Told It\" . Scientific American . Retrieved 17 July 2025 . ^ Haugeland (1985) , pp. 112–117. ^ Physical symbol system hypothesis: Newell & Simon (1976 , p. 116) Historical significance: McCorduck (2004 , p. 153), Russell & Norvig (2021 , p. 19) ^ Moravec's paradox : Moravec (1988 , pp. 15–16), Minsky (1986 , p. 29), Pinker (2007 , pp. 190–191) ^ Dreyfus' critique of AI : Dreyfus (1972) , Dreyfus & Dreyfus (1986) Historical significance and philosophical implications: Crevier (1993 , pp. 120–132), McCorduck (2004 , pp. 211–239), Russell & Norvig (2021 , pp. 981–982), Fearn (2007 , chpt. 3) ^ Neats vs. scruffies , the historic debate: McCorduck (2004 , pp. 421–424, 486–489), Crevier (1993 , p. 168), Nilsson (1983 , pp. 10–11), Russell & Norvig (2021 , p. 24) A classic example of the \"scruffy\" approach to intelligence: Minsky (1986) A modern example of neat AI and its aspirations in the 21st century: Domingos (2015) ^ Searle's Chinese room argument: Searle (1980) . Searle's original presentation of the thought experiment., Searle (1999) . Discussion: Russell & Norvig (2021 , pp. 985), McCorduck (2004 , pp. 443–445), Crevier (1993 , pp. 269–271) ^ Leith, Sam (7 July 2022). \"Nick Bostrom: How can we be certain a machine isn't conscious?\" . The"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 30,
    "text": "Spectator . Archived from the original on 26 September 2024 . Retrieved 23 February 2024 . ^ a b c Thomson, Jonny (31 October 2022). \"Why don't robots have rights?\" . Big Think . Archived from the original on 13 September 2024 . Retrieved 23 February 2024 . ^ a b Kateman, Brian (24 July 2023). \"AI Should Be Terrified of Humans\" . Time . Archived from the original on 25 September 2024 . Retrieved 23 February 2024 . ^ Wong, Jeff (10 July 2023). \"What leaders need to know about robot rights\" . Fast Company . ^ Hern, Alex (12 January 2017). \"Give robots 'personhood' status, EU committee argues\" . The Guardian . ISSN 0261-3077 . Archived from the original on 5 October 2024 . Retrieved 23 February 2024 . ^ Dovey, Dana (14 April 2018). \"Experts Don't Think Robots Should Have Rights\" . Newsweek . Archived from the original on 5 October 2024 . Retrieved 23 February 2024 . ^ Cuddy, Alice (13 April 2018). \"Robot rights violate human rights, experts warn EU\" . euronews . Archived from the original on 19 September 2024 . Retrieved 23 February 2024 . ^ The Intelligence explosion and technological singularity : Russell & Norvig (2021 , pp. 1004–1005), Omohundro (2008) , Kurzweil (2005) I. J. Good 's \"intelligence explosion\": Good (1965) Vernor Vinge 's \"singularity\": Vinge (1993) ^ Transhumanism : Moravec (1988) , Kurzweil (2005) , Russell & Norvig (2021 , p. 1005) ^ AI as evolution: Edward Fredkin is quoted in McCorduck (2004 , p. 401), Butler (1863) , Dyson (1998) ^ AI in myth: McCorduck (2004 , pp. 4–5) ^ Anderson (2008) . AI textbooks The two most widely used textbooks in 2023 (see the Open Syllabus ): The four most widely used AI textbooks in 2008: Luger, George ; Stubblefield, William (2004). Artificial Intelligence: Structures and Strategies for Complex Problem Solving (5th ed.). Benjamin/Cummings. ISBN 978-0-8053-4780-7 . Archived from the original on 26 July 2020 . Retrieved 17 December 2019 . Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis . Morgan Kaufmann. ISBN 978-1-5586-0467-4 . Archived from the original on 26 July 2020 . Retrieved 18 November 2019 . Russell, Stuart J. ; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2 . Poole, David ; Mackworth, Alan ; Goebel, Randy (1998). Computational Intelligence: A Logical Approach . New York: Oxford University Press. ISBN 978-0-1951-0270-3 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Later edition: Poole, David; Mackworth, Alan (2017). Artificial Intelligence: Foundations of Computational Agents (2nd ed.). Cambridge University Press. ISBN 978-1-1071-9539-4 . Archived from the original on 7 December 2017 . Retrieved 6 December 2017 . Other textbooks: History of AI Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence . New York, NY: BasicBooks. ISBN 0-465-02997-3 . McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, Massachusetts: A. K. Peters, ISBN 1-5688-1205-1 Newquist, H. P. (1994). The Brain Makers: Genius, Ego, And Greed In The Quest For Machines That Think . New York: Macmillan/SAMS. ISBN 978-0-6723-0412-5 . Harmon, Paul; Sawyer, Brian (1990). Creating Expert Systems for Business and Industry . New York: John Wiley & Sons. ISBN 0471614963 . Other sources AI & ML in Fusion AI & ML in Fusion, video lecture Archived 2 July 2023 at the Wayback Machine Alter, Alexandra; Harris, Elizabeth A. (20 September 2023), \"Franzen, Grisham and Other Prominent Authors Sue OpenAI\" , The New York Times , archived from the original on 14 September 2024 , retrieved 5 October 2024 Altman, Sam ; Brockman, Greg ; Sutskever, Ilya (22 May 2023). \"Governance of Superintelligence\" . openai.com . Archived from the original on 27 May 2023 . Retrieved 27 May 2023 . Anderson, Susan Leigh (2008). \"Asimov's \"three laws of robotics\" and machine metaethics\". AI & Society . 22 (4): 477– 493. doi : 10.1007/s00146-007-0094-5 . S2CID 1809459 . Anderson, Michael; Anderson, Susan Leigh (2011). Machine Ethics . Cambridge University Press. Arntz, Melanie; Gregory, Terry; Zierahn, Ulrich (2016), \"The risk of automation for jobs in OECD countries: A comparative analysis\", OECD Social, Employment, and Migration Working Papers 189 Asada, M.; Hosoda, K.; Kuniyoshi, Y.; Ishiguro, H.; Inui, T.; Yoshikawa, Y.; Ogino, M.; Yoshida, C. (2009). \"Cognitive developmental robotics: a survey\". IEEE Transactions on Autonomous Mental Development . 1 (1): 12– 34. doi : 10.1109/tamd.2009.2021702 . S2CID 10168773 . \"Ask the AI experts: What's driving today's progress in AI?\" . McKinsey & Company . Archived from the original on 13 April 2018 . Retrieved 13 April 2018 . Barfield, Woodrow; Pagallo, Ugo (2018). Research handbook on the law of artificial intelligence . Cheltenham, UK: Edward Elgar Publishing. ISBN 978-1-7864-3904-8 . OCLC 1039480085 . Beal, J.; Winston, Patrick (2009), \"The New Frontier of Human-Level"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 31,
    "text": "Artificial Intelligence\", IEEE Intelligent Systems , vol. 24, pp. 21– 24, doi : 10.1109/MIS.2009.75 , hdl : 1721.1/52357 , S2CID 32437713 Berdahl, Carl Thomas; Baker, Lawrence; Mann, Sean; Osoba, Osonde; Girosi, Federico (7 February 2023). \"Strategies to Improve the Impact of Artificial Intelligence on Health Equity: Scoping Review\" . JMIR AI . 2 : e42936. doi : 10.2196/42936 . ISSN 2817-1705 . PMC 11041459 . PMID 38875587 . S2CID 256681439 . Berryhill, Jamie; Heang, Kévin Kok; Clogher, Rob; McBride, Keegan (2019). Hello, World: Artificial Intelligence and its Use in the Public Sector (PDF) . Paris: OECD Observatory of Public Sector Innovation. Archived (PDF) from the original on 20 December 2019 . Retrieved 9 August 2020 . Bertini, M; Del Bimbo, A; Torniai, C (2006). \"Automatic annotation and semantic retrieval of video sequences using multimedia ontologies\". MM '06 Proceedings of the 14th ACM international conference on Multimedia . 14th ACM international conference on Multimedia. Santa Barbara: ACM. pp. 679– 682. Bostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies . Oxford University Press. Bostrom, Nick (2015). \"What happens when our computers get smarter than we are?\" . TED (conference) . Archived from the original on 25 July 2020 . Retrieved 30 January 2020 . Brooks, Rodney (10 November 2014). \"artificial intelligence is a tool, not a threat\" . Archived from the original on 12 November 2014. Brooks, Rodney (1990). \"Elephants Don't Play Chess\" (PDF) . Robotics and Autonomous Systems . 6 ( 1– 2): 3– 15. CiteSeerX 10.1.1.588.7539 . doi : 10.1016/S0921-8890(05)80025-9 . Archived (PDF) from the original on 9 August 2007. Buiten, Miriam C (2019). \"Towards Intelligent Regulation of Artificial Intelligence\" . European Journal of Risk Regulation . 10 (1): 41– 59. doi : 10.1017/err.2019.8 . ISSN 1867-299X . Bushwick, Sophie (16 March 2023), \"What the New GPT-4 AI Can Do\" , Scientific American , archived from the original on 22 August 2023 , retrieved 5 October 2024 Butler, Samuel (13 June 1863). \"Darwin among the Machines\" . Letters to the Editor. The Press . Christchurch, New Zealand. Archived from the original on 19 September 2008 . Retrieved 16 October 2014 – via Victoria University of Wellington. Buttazzo, G. (July 2001). \"Artificial consciousness: Utopia or real possibility?\". Computer . 34 (7): 24– 30. doi : 10.1109/2.933500 . Cambria, Erik; White, Bebo (May 2014). \"Jumping NLP Curves: A Review of Natural Language Processing Research [Review Article]\". IEEE Computational Intelligence Magazine . 9 (2): 48– 57. doi : 10.1109/MCI.2014.2307227 . S2CID 206451986 . Cellan-Jones, Rory (2 December 2014). \"Stephen Hawking warns artificial intelligence could end mankind\" . BBC News . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Chalmers, David (1995). \"Facing up to the problem of consciousness\" . Journal of Consciousness Studies . 2 (3): 200– 219. CiteSeerX 10.1.1.103.8362 . Archived from the original on 8 March 2005 . Retrieved 11 October 2018 . Challa, Subhash; Moreland, Mark R.; Mušicki, Darko; Evans, Robin J. (2011). Fundamentals of Object Tracking . Cambridge University Press. doi : 10.1017/CBO9780511975837 . ISBN 978-0-5218-7628-5 . Christian, Brian (2020). The Alignment Problem : Machine learning and human values . W. W. Norton & Company. ISBN 978-0-3938-6833-3 . OCLC 1233266753 . Ciresan, D.; Meier, U.; Schmidhuber, J. (2012). \"Multi-column deep neural networks for image classification\". 2012 IEEE Conference on Computer Vision and Pattern Recognition . pp. 3642– 3649. arXiv : 1202.2745 . doi : 10.1109/cvpr.2012.6248110 . ISBN 978-1-4673-1228-8 . S2CID 2161592 . Clark, Jack (2015b). \"Why 2015 Was a Breakthrough Year in Artificial Intelligence\" . Bloomberg.com . Archived from the original on 23 November 2016 . Retrieved 23 November 2016 . CNA (12 January 2019). \"Commentary: Bad news. Artificial intelligence is biased\" . CNA . Archived from the original on 12 January 2019 . Retrieved 19 June 2020 . Cybenko, G. (1988). Continuous valued neural networks with two hidden layers are sufficient (Report). Department of Computer Science, Tufts University. Deng, L.; Yu, D. (2014). \"Deep Learning: Methods and Applications\" (PDF) . Foundations and Trends in Signal Processing . 7 ( 3– 4): 197– 387. doi : 10.1561/2000000039 . Archived (PDF) from the original on 14 March 2016 . Retrieved 18 October 2014 . Dennett, Daniel (1991). Consciousness Explained . The Penguin Press. ISBN 978-0-7139-9037-9 . DiFeliciantonio, Chase (3 April 2023). \"AI has already changed the world. This report shows how\" . San Francisco Chronicle . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Dickson, Ben (2 May 2022). \"Machine learning: What is the transformer architecture?\" . TechTalks . Archived from the original on 22 November 2023 . Retrieved 22 November 2023 . Dockrill, Peter (27 June 2022), \"Robots With Flawed AI Make Sexist And Racist Decisions, Experiment Shows\" , Science Alert , archived from the original on 27 June 2022 Domingos, Pedro (2015). The Master Algorithm: How the"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 32,
    "text": "Quest for the Ultimate Learning Machine Will Remake Our World . Basic Books . ISBN 978-0-4650-6570-7 . Dreyfus, Hubert (1972). What Computers Can't Do . New York: MIT Press. ISBN 978-0-0601-1082-6 . Dreyfus, Hubert ; Dreyfus, Stuart (1986). Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer . Oxford: Blackwell. ISBN 978-0-0290-8060-3 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Dyson, George (1998). Darwin among the Machines . Allan Lane Science. ISBN 978-0-7382-0030-9 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Edelson, Edward (1991). The Nervous System . New York: Chelsea House. ISBN 978-0-7910-0464-7 . Archived from the original on 26 July 2020 . Retrieved 18 November 2019 . Edwards, Benj (17 May 2023). \"Poll: AI poses risk to humanity, according to majority of Americans\" . Ars Technica . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Fearn, Nicholas (2007). The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World's Greatest Thinkers . New York: Grove Press. ISBN 978-0-8021-1839-4 . Ford, Martin; Colvin, Geoff (6 September 2015). \"Will robots create more jobs than they destroy?\" . The Guardian . Archived from the original on 16 June 2018 . Retrieved 13 January 2018 . Fox News (2023). \"Fox News Poll\" (PDF) . Fox News. Archived (PDF) from the original on 12 May 2023 . Retrieved 19 June 2023 . Frey, Carl Benedikt; Osborne, Michael A (1 January 2017). \"The future of employment: How susceptible are jobs to computerisation?\". Technological Forecasting and Social Change . 114 : 254– 280. CiteSeerX 10.1.1.395.416 . doi : 10.1016/j.techfore.2016.08.019 . ISSN 0040-1625 . \"From not working to neural networking\" . The Economist . 2016. Archived from the original on 31 December 2016 . Retrieved 26 April 2018 . Galvan, Jill (1 January 1997). \"Entering the Posthuman Collective in Philip K. Dick's \"Do Androids Dream of Electric Sheep?\" \". Science Fiction Studies . 24 (3): 413– 429. doi : 10.1525/sfs.24.3.0413 . JSTOR 4240644 . Geist, Edward Moore (9 August 2015). \"Is artificial intelligence really an existential threat to humanity?\" . Bulletin of the Atomic Scientists . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Gibbs, Samuel (27 October 2014). \"Elon Musk: artificial intelligence is our biggest existential threat\" . The Guardian . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Goffrey, Andrew (2008). \"Algorithm\". In Fuller, Matthew (ed.). Software studies: a lexicon . Cambridge, Mass.: MIT Press. pp. 15 –20. ISBN 978-1-4356-4787-9 . Goldman, Sharon (14 September 2022). \"10 years later, deep learning 'revolution' rages on, say AI pioneers Hinton, LeCun and Li\" . VentureBeat . Archived from the original on 5 October 2024 . Retrieved 8 December 2023 . Good, I. J. (1965), Speculations Concerning the First Ultraintelligent Machine , archived from the original on 10 July 2023 , retrieved 5 October 2024 Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016), Deep Learning , MIT Press., archived from the original on 16 April 2016 , retrieved 12 November 2017 Goodman, Bryce; Flaxman, Seth (2017). \"EU regulations on algorithmic decision-making and a 'right to explanation' \". AI Magazine . 38 (3): 50. arXiv : 1606.08813 . doi : 10.1609/aimag.v38i3.2741 . S2CID 7373959 . Government Accountability Office (13 September 2022). Consumer Data: Increasing Use Poses Risks to Privacy . gao.gov (Report). Archived from the original on 13 September 2024 . Retrieved 5 October 2024 . Grant, Nico; Hill, Kashmir (22 May 2023). \"Google's Photo App Still Can't Find Gorillas. And Neither Can Apple's\" . The New York Times . Archived from the original on 14 September 2024 . Retrieved 5 October 2024 . Goswami, Rohan (5 April 2023). \"Here's where the A.I. jobs are\" . CNBC . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Harari, Yuval Noah (October 2018). \"Why Technology Favors Tyranny\" . The Atlantic . Archived from the original on 25 September 2021 . Retrieved 23 September 2021 . Harari, Yuval Noah (2023). \"AI and the future of humanity\" . YouTube . Archived from the original on 30 September 2024 . Retrieved 5 October 2024 . Haugeland, John (1985). Artificial Intelligence: The Very Idea . Cambridge, Mass.: MIT Press. ISBN 978-0-2620-8153-5 . Hinton, G.; Deng, L.; Yu, D.; Dahl, G.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T. ; Kingsbury, B. (2012). \"Deep Neural Networks for Acoustic Modeling in Speech Recognition – The shared views of four research groups\". IEEE Signal Processing Magazine . 29 (6): 82– 97. Bibcode : 2012ISPM...29...82H . doi : 10.1109/msp.2012.2205597 . S2CID 206485943 . Holley, Peter (28 January 2015). \"Bill Gates on dangers of artificial intelligence: 'I don't understand"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 33,
    "text": "why some people are not concerned' \" . The Washington Post . ISSN 0190-8286 . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Hornik, Kurt; Stinchcombe, Maxwell; White, Halbert (1989). Multilayer Feedforward Networks are Universal Approximators (PDF) . Neural Networks . Vol. 2. Pergamon Press. pp. 359– 366. Archived (PDF) from the original on 21 April 2023 . Retrieved 5 October 2024 . Horst, Steven (2005). \"The Computational Theory of Mind\" . The Stanford Encyclopedia of Philosophy . Archived from the original on 6 March 2016 . Retrieved 7 March 2016 . Howe, J. (November 1994). \"Artificial Intelligence at Edinburgh University: a Perspective\" . Archived from the original on 15 May 2007 . Retrieved 30 August 2007 . IGM Chicago (30 June 2017). \"Robots and Artificial Intelligence\" . igmchicago.org . Archived from the original on 1 May 2019 . Retrieved 3 July 2019 . Iphofen, Ron; Kritikos, Mihalis (3 January 2019). \"Regulating artificial intelligence and robotics: ethics by design in a digital society\". Contemporary Social Science . 16 (2): 170– 184. doi : 10.1080/21582041.2018.1563803 . ISSN 2158-2041 . S2CID 59298502 . Jordan, M. I.; Mitchell, T. M. (16 July 2015). \"Machine learning: Trends, perspectives, and prospects\". Science . 349 (6245): 255– 260. Bibcode : 2015Sci...349..255J . doi : 10.1126/science.aaa8415 . PMID 26185243 . S2CID 677218 . Kahneman, Daniel (2011). Thinking, Fast and Slow . Macmillan. ISBN 978-1-4299-6935-2 . Archived from the original on 15 March 2023 . Retrieved 8 April 2012 . Kahneman, Daniel ; Slovic, D.; Tversky, Amos (1982). \"Judgment under uncertainty: Heuristics and biases\". Science . 185 (4157). New York: Cambridge University Press: 1124– 1131. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . ISBN 978-0-5212-8414-1 . PMID 17835457 . S2CID 143452957 . Kasperowicz, Peter (1 May 2023). \"Regulate AI? GOP much more skeptical than Dems that government can do it right: poll\" . Fox News . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Katz, Yarden (1 November 2012). \"Noam Chomsky on Where Artificial Intelligence Went Wrong\" . The Atlantic . Archived from the original on 28 February 2019 . Retrieved 26 October 2014 . \"Kismet\" . MIT Artificial Intelligence Laboratory, Humanoid Robotics Group. Archived from the original on 17 October 2014 . Retrieved 25 October 2014 . Kissinger, Henry (1 November 2021). \"The Challenge of Being Human in the Age of AI\" . The Wall Street Journal . Archived from the original on 4 November 2021 . Retrieved 4 November 2021 . Kobielus, James (27 November 2019). \"GPUs Continue to Dominate the AI Accelerator Market for Now\" . InformationWeek . Archived from the original on 19 October 2021 . Retrieved 11 June 2020 . Kuperman, G. J.; Reichley, R. M.; Bailey, T. C. (1 July 2006). \"Using Commercial Knowledge Bases for Clinical Decision Support: Opportunities, Hurdles, and Recommendations\" . Journal of the American Medical Informatics Association . 13 (4): 369– 371. doi : 10.1197/jamia.M2055 . PMC 1513681 . PMID 16622160 . Kurzweil, Ray (2005). The Singularity is Near . Penguin Books. ISBN 978-0-6700-3384-3 . Langley, Pat (2011). \"The changing science of machine learning\" . Machine Learning . 82 (3): 275– 279. doi : 10.1007/s10994-011-5242-y . Larson, Jeff; Angwin, Julia (23 May 2016). \"How We Analyzed the COMPAS Recidivism Algorithm\" . ProPublica . Archived from the original on 29 April 2019 . Retrieved 19 June 2020 . Laskowski, Nicole (November 2023). \"What is Artificial Intelligence and How Does AI Work? TechTarget\" . Enterprise AI . Archived from the original on 5 October 2024 . Retrieved 30 October 2023 . Law Library of Congress (U.S.). Global Legal Research Directorate, issuing body. (2019). Regulation of artificial intelligence in selected jurisdictions . LCCN 2019668143 . OCLC 1110727808 . Lee, Timothy B. (22 August 2014). \"Will artificial intelligence destroy humanity? Here are 5 reasons not to worry\" . Vox . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Lenat, Douglas ; Guha, R. V. (1989). Building Large Knowledge-Based Systems . Addison-Wesley. ISBN 978-0-2015-1752-1 . Lighthill, James (1973). \"Artificial Intelligence: A General Survey\". Artificial Intelligence: a paper symposium . Science Research Council. Lipartito, Kenneth (6 January 2011), The Narrative and the Algorithm: Genres of Credit Reporting from the Nineteenth Century to Today (PDF) (Unpublished manuscript), doi : 10.2139/ssrn.1736283 , S2CID 166742927 , archived (PDF) from the original on 9 October 2022 Lohr, Steve (2017). \"Robots Will Take Jobs, but Not as Fast as Some Fear, New Report Says\" . The New York Times . Archived from the original on 14 January 2018 . Retrieved 13 January 2018 . Lungarella, M.; Metta, G.; Pfeifer, R.; Sandini, G. (2003). \"Developmental robotics: a survey\". Connection Science . 15 (4): 151– 190. Bibcode : 2003ConSc..15..151L . CiteSeerX 10.1.1.83.7615 . doi : 10.1080/09540090310001655110 . S2CID 1452734 . \"Machine Ethics\" ."
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 34,
    "text": "aaai.org . Archived from the original on 29 November 2014. Madrigal, Alexis C. (27 February 2015). \"The case against killer robots, from a guy actually working on artificial intelligence\" . Fusion.net . Archived from the original on 4 February 2016 . Retrieved 31 January 2016 . Mahdawi, Arwa (26 June 2017). \"What jobs will still be around in 20 years? Read this to prepare your future\" . The Guardian . Archived from the original on 14 January 2018 . Retrieved 13 January 2018 . Maker, Meg Houston (2006), AI@50: AI Past, Present, Future , Dartmouth College, archived from the original on 8 October 2008 , retrieved 16 October 2008 Marmouyet, Françoise (15 December 2023). \"Google's Gemini: is the new AI model really better than ChatGPT?\" . The Conversation . Archived from the original on 4 March 2024 . Retrieved 25 December 2023 . Minsky, Marvin (1986), The Society of Mind , Simon and Schuster McCarthy, John ; Minsky, Marvin ; Rochester, Nathan ; Shannon, Claude (1955). \"A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence\" . Archived from the original on 26 August 2007 . Retrieved 30 August 2007 . McCarthy, John (2007), \"From Here to Human-Level AI\", Artificial Intelligence , p. 171 McCarthy, John (1999), What is AI? , archived from the original on 4 December 2022 , retrieved 4 December 2022 McCauley, Lee (2007). \"AI armageddon and the three laws of robotics\". Ethics and Information Technology . 9 (2): 153– 164. CiteSeerX 10.1.1.85.8904 . doi : 10.1007/s10676-007-9138-2 . S2CID 37272949 . McGarry, Ken (1 December 2005). \"A survey of interestingness measures for knowledge discovery\". The Knowledge Engineering Review . 20 (1): 39– 61. doi : 10.1017/S0269888905000408 . S2CID 14987656 . McGaughey, E (2022), Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy , p. 51(3) Industrial Law Journal 511–559, doi : 10.2139/ssrn.3044448 , S2CID 219336439 , SSRN 3044448 , archived from the original on 31 January 2021 , retrieved 27 May 2023 Merkle, Daniel; Middendorf, Martin (2013). \"Swarm Intelligence\". In Burke, Edmund K.; Kendall, Graham (eds.). Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques . Springer Science & Business Media. ISBN 978-1-4614-6940-7 . Minsky, Marvin (1967), Computation: Finite and Infinite Machines , Englewood Cliffs, N.J.: Prentice-Hall Moravec, Hans (1988). Mind Children . Harvard University Press. ISBN 978-0-6745-7616-2 . Archived from the original on 26 July 2020 . Retrieved 18 November 2019 . Morgenstern, Michael (9 May 2015). \"Automation and anxiety\" . The Economist . Archived from the original on 12 January 2018 . Retrieved 13 January 2018 . Müller, Vincent C.; Bostrom, Nick (2014). \"Future Progress in Artificial Intelligence: A Poll Among Experts\" (PDF) . AI Matters . 1 (1): 9– 11. doi : 10.1145/2639475.2639478 . S2CID 8510016 . Archived (PDF) from the original on 15 January 2016. Neumann, Bernd; Möller, Ralf (January 2008). \"On scene interpretation with description logics\". Image and Vision Computing . 26 (1): 82– 101. doi : 10.1016/j.imavis.2007.08.013 . S2CID 10767011 . Nilsson, Nils (1995), \"Eyes on the Prize\", AI Magazine , vol. 16, pp. 9– 17 Newell, Allen ; Simon, H. A. (1976). \"Computer Science as Empirical Inquiry: Symbols and Search\" . Communications of the ACM . 19 (3): 113– 126. doi : 10.1145/360018.360022 . Nicas, Jack (7 February 2018). \"How YouTube Drives People to the Internet's Darkest Corners\" . The Wall Street Journal . ISSN 0099-9660 . Archived from the original on 5 October 2024 . Retrieved 16 June 2018 . Nilsson, Nils (1983). \"Artificial Intelligence Prepares for 2001\" (PDF) . AI Magazine . 1 (1). Archived (PDF) from the original on 17 August 2020 . Retrieved 22 August 2020 . Presidential Address to the Association for the Advancement of Artificial Intelligence . NRC (United States National Research Council) (1999). \"Developments in Artificial Intelligence\". Funding a Revolution: Government Support for Computing Research . National Academy Press. Omohundro, Steve (2008). The Nature of Self-Improving Artificial Intelligence . presented and distributed at the 2007 Singularity Summit, San Francisco, CA. Oudeyer, P-Y. (2010). \"On the impact of robotics in behavioral and cognitive sciences: from insect navigation to human cognitive development\" (PDF) . IEEE Transactions on Autonomous Mental Development . 2 (1): 2– 16. doi : 10.1109/tamd.2009.2039057 . S2CID 6362217 . Archived (PDF) from the original on 3 October 2018 . Retrieved 4 June 2013 . Pennachin, C.; Goertzel, B. (2007). \"Contemporary Approaches to Artificial General Intelligence\". Artificial General Intelligence . Cognitive Technologies. Berlin, Heidelberg: Springer. pp. 1– 30. doi : 10.1007/978-3-540-68677-4_1 . ISBN 978-3-5402-3733-4 . Pinker, Steven (2007) [1994], The Language Instinct , Perennial Modern Classics, Harper, ISBN 978-0-0613-3646-1 Poria, Soujanya; Cambria, Erik; Bajpai, Rajiv; Hussain, Amir (September 2017). \"A review of affective computing: From unimodal analysis to multimodal fusion\" . Information Fusion . 37 : 98– 125. doi : 10.1016/j.inffus.2017.02.003 . hdl : 1893/25490 . S2CID"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 35,
    "text": "205433041 . Archived from the original on 23 March 2023 . Retrieved 27 April 2021 . Rawlinson, Kevin (29 January 2015). \"Microsoft's Bill Gates insists AI is a threat\" . BBC News . Archived from the original on 29 January 2015 . Retrieved 30 January 2015 . Reisner, Alex (19 August 2023), \"Revealed: The Authors Whose Pirated Books are Powering Generative AI\" , The Atlantic , archived from the original on 3 October 2024 , retrieved 5 October 2024 Roberts, Jacob (2016). \"Thinking Machines: The Search for Artificial Intelligence\" . Distillations . Vol. 2, no. 2. pp. 14– 23. Archived from the original on 19 August 2018 . Retrieved 20 March 2018 . Robitzski, Dan (5 September 2018). \"Five experts share what scares them the most about AI\" . Archived from the original on 8 December 2019 . Retrieved 8 December 2019 . Rose, Steve (11 July 2023). \"AI Utopia or dystopia?\". The Guardian Weekly . pp. 42– 43. Russell, Stuart (2019). Human Compatible: Artificial Intelligence and the Problem of Control . United States: Viking. ISBN 978-0-5255-5861-3 . OCLC 1083694322 . Sainato, Michael (19 August 2015). \"Stephen Hawking, Elon Musk, and Bill Gates Warn About Artificial Intelligence\" . Observer . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Sample, Ian (5 November 2017). \"Computer says no: why making AIs fair, accountable and transparent is crucial\" . The Guardian . Archived from the original on 10 October 2022 . Retrieved 30 January 2018 . Rothman, Denis (7 October 2020). \"Exploring LIME Explanations and the Mathematics Behind It\" . Codemotion . Archived from the original on 25 November 2023 . Retrieved 25 November 2023 . Scassellati, Brian (2002). \"Theory of mind for a humanoid robot\". Autonomous Robots . 12 (1): 13– 24. doi : 10.1023/A:1013298507114 . S2CID 1979315 . Schmidhuber, J. (2015). \"Deep Learning in Neural Networks: An Overview\". Neural Networks . 61 : 85– 117. arXiv : 1404.7828 . doi : 10.1016/j.neunet.2014.09.003 . PMID 25462637 . S2CID 11715509 . Schmidhuber, Jürgen (2022). \"Annotated History of Modern AI and Deep Learning\" . Archived from the original on 7 August 2023 . Retrieved 5 October 2024 . Searle, John (1980). \"Minds, Brains and Programs\" (PDF) . Behavioral and Brain Sciences . 3 (3): 417– 457. doi : 10.1017/S0140525X00005756 . S2CID 55303721 . Archived (PDF) from the original on 17 March 2019 . Retrieved 22 August 2020 . Searle, John (1999). Mind, language and society . New York: Basic Books. ISBN 978-0-4650-4521-1 . OCLC 231867665 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Simon, H. A. (1965), The Shape of Automation for Men and Management , New York: Harper & Row Simonite, Tom (31 March 2016). \"How Google Plans to Solve Artificial Intelligence\" . MIT Technology Review . Archived from the original on 16 September 2024 . Retrieved 5 October 2024 . Smith, Craig S. (15 March 2023). \"ChatGPT-4 Creator Ilya Sutskever on AI Hallucinations and AI Democracy\" . Forbes . Archived from the original on 18 September 2024 . Retrieved 25 December 2023 . Smoliar, Stephen W.; Zhang, HongJiang (1994). \"Content based video indexing and retrieval\". IEEE MultiMedia . 1 (2): 62– 72. doi : 10.1109/93.311653 . S2CID 32710913 . Solomonoff, Ray (1956). An Inductive Inference Machine (PDF) . Dartmouth Summer Research Conference on Artificial Intelligence. Archived (PDF) from the original on 26 April 2011 . Retrieved 22 March 2011 – via std.com, pdf scanned copy of the original. Later published as Solomonoff, Ray (1957). \"An Inductive Inference Machine\". IRE Convention Record . Vol. Section on Information Theory, part 2. pp. 56– 62. Stanford University (2023). \"Artificial Intelligence Index Report 2023/Chapter 6: Policy and Governance\" (PDF) . AI Index. Archived (PDF) from the original on 19 June 2023 . Retrieved 19 June 2023 . Tao, Jianhua; Tan, Tieniu (2005). Affective Computing and Intelligent Interaction . Affective Computing: A Review. Lecture Notes in Computer Science. Vol. 3784. Springer. pp. 981– 995. doi : 10.1007/11573548 . ISBN 978-3-5402-9621-8 . Taylor, Josh; Hern, Alex (2 May 2023). \" 'Godfather of AI' Geoffrey Hinton quits Google and warns over dangers of misinformation\" . The Guardian . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . Thompson, Derek (23 January 2014). \"What Jobs Will the Robots Take?\" . The Atlantic . Archived from the original on 24 April 2018 . Retrieved 24 April 2018 . Thro, Ellen (1993). Robotics: The Marriage of Computers and Machines . New York: Facts on File. ISBN 978-0-8160-2628-9 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Toews, Rob (3 September 2023). \"Transformers Revolutionized AI. What Will Replace Them?\" . Forbes . Archived from the original on 8 December 2023 . Retrieved 8 December 2023 . Turing, Alan (October"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 36,
    "text": "1950). \"Computing Machinery and Intelligence\" . Mind . 59 (236): 433– 460. doi : 10.1093/mind/LIX.236.433 . ISSN 1460-2113 . JSTOR 2251299 . S2CID 14636783 . UNESCO Science Report: the Race Against Time for Smarter Development . Paris: UNESCO. 2021. ISBN 978-9-2310-0450-6 . Archived from the original on 18 June 2022 . Retrieved 18 September 2021 . Urbina, Fabio; Lentzos, Filippa; Invernizzi, Cédric; Ekins, Sean (7 March 2022). \"Dual use of artificial-intelligence-powered drug discovery\" . Nature Machine Intelligence . 4 (3): 189– 191. doi : 10.1038/s42256-022-00465-9 . PMC 9544280 . PMID 36211133 . S2CID 247302391 . Valance, Christ (30 May 2023). \"Artificial intelligence could lead to extinction, experts warn\" . BBC News . Archived from the original on 17 June 2023 . Retrieved 18 June 2023 . Valinsky, Jordan (11 April 2019), \"Amazon reportedly employs thousands of people to listen to your Alexa conversations\" , CNN.com , archived from the original on 26 January 2024 , retrieved 5 October 2024 Verma, Yugesh (25 December 2021). \"A Complete Guide to SHAP – SHAPley Additive exPlanations for Practitioners\" . Analytics India Magazine . Archived from the original on 25 November 2023 . Retrieved 25 November 2023 . Vincent, James (7 November 2019). \"OpenAI has published the text-generating AI it said was too dangerous to share\" . The Verge . Archived from the original on 11 June 2020 . Retrieved 11 June 2020 . Vincent, James (15 November 2022). \"The scary truth about AI copyright is nobody knows what will happen next\" . The Verge . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Vincent, James (3 April 2023). \"AI is entering an era of corporate control\" . The Verge . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Vinge, Vernor (1993). \"The Coming Technological Singularity: How to Survive in the Post-Human Era\" . Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace : 11. Bibcode : 1993vise.nasa...11V . Archived from the original on 1 January 2007 . Retrieved 14 November 2011 . Waddell, Kaveh (2018). \"Chatbots Have Entered the Uncanny Valley\" . The Atlantic . Archived from the original on 24 April 2018 . Retrieved 24 April 2018 . Wallach, Wendell (2010). Moral Machines . Oxford University Press. Wason, P. C. ; Shapiro, D. (1966). \"Reasoning\" . In Foss, B. M. (ed.). New horizons in psychology . Harmondsworth: Penguin. Archived from the original on 26 July 2020 . Retrieved 18 November 2019 . Weng, J.; McClelland; Pentland, A.; Sporns, O.; Stockman, I.; Sur, M.; Thelen, E. (2001). \"Autonomous mental development by robots and animals\" (PDF) . Science . 291 (5504): 599– 600. doi : 10.1126/science.291.5504.599 . PMID 11229402 . S2CID 54131797 . Archived (PDF) from the original on 4 September 2013 . Retrieved 4 June 2013 – via msu.edu. \"What is 'fuzzy logic'? Are there computers that are inherently fuzzy and do not apply the usual binary logic?\" . Scientific American . 21 October 1999. Archived from the original on 6 May 2018 . Retrieved 5 May 2018 . Williams, Rhiannon (28 June 2023), \"Humans may be more likely to believe disinformation generated by AI\" , MIT Technology Review , archived from the original on 16 September 2024 , retrieved 5 October 2024 Wirtz, Bernd W.; Weyerer, Jan C.; Geyer, Carolin (24 July 2018). \"Artificial Intelligence and the Public Sector – Applications and Challenges\" . International Journal of Public Administration . 42 (7): 596– 615. doi : 10.1080/01900692.2018.1498103 . ISSN 0190-0692 . S2CID 158829602 . Archived from the original on 18 August 2020 . Retrieved 22 August 2020 . Wong, Matteo (19 May 2023), \"ChatGPT Is Already Obsolete\" , The Atlantic , archived from the original on 18 September 2024 , retrieved 5 October 2024 Yudkowsky, E (2008), \"Artificial Intelligence as a Positive and Negative Factor in Global Risk\" (PDF) , Global Catastrophic Risks , Oxford University Press, 2008, Bibcode : 2008gcr..book..303Y , archived (PDF) from the original on 19 October 2013 , retrieved 24 September 2021 Further reading Autor, David H. , \"Why Are There Still So Many Jobs? The History and Future of Workplace Automation\" (2015) 29(3) Journal of Economic Perspectives 3. Boyle, James, The Line: AI and the Future of Personhood , MIT Press , 2024. Cukier, Kenneth , \"Ready for Robots? How to Think about the Future of AI\", Foreign Affairs , vol. 98, no. 4 (July/August 2019), pp. 192–198. George Dyson , historian of computing, writes (in what might be called \"Dyson's Law\") that \"Any system simple enough to be understandable will not be complicated enough to behave intelligently, while any system complicated enough to behave intelligently will be too complicated to understand.\" (p. 197.) Computer scientist Alex Pentland writes: \"Current AI machine-learning algorithms are, at their core, dead simple stupid. They work, but"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 37,
    "text": "they work by brute force.\" (p. 198.) Evans, Woody (2015). \"Posthuman Rights: Dimensions of Transhuman Worlds\" . Teknokultura . 12 (2). doi : 10.5209/rev_TK.2015.v12.n2.49072 . S2CID 147612763 . Frank, Michael (22 September 2023). \"US Leadership in Artificial Intelligence Can Shape the 21st Century Global Order\" . The Diplomat . Archived from the original on 16 September 2024 . Retrieved 8 December 2023 . Instead, the United States has developed a new area of dominance that the rest of the world views with a mixture of awe, envy, and resentment: artificial intelligence... From AI models and research to cloud computing and venture capital, U.S. companies, universities, and research labs – and their affiliates in allied countries – appear to have an enormous lead in both developing cutting-edge AI and commercializing it. The value of U.S. venture capital investments in AI start-ups exceeds that of the rest of the world combined. Gertner, Jon. (2023) \"Wikipedia's Moment of Truth: Can the online encyclopedia help teach A.I. chatbots to get their facts right — without destroying itself in the process?\" New York Times Magazine (July 18, 2023) online Archived 20 July 2023 at the Wayback Machine Gleick, James , \"The Fate of Free Will\" (review of Kevin J. Mitchell, Free Agents: How Evolution Gave Us Free Will , Princeton University Press, 2023, 333 pp.), The New York Review of Books , vol. LXXI, no. 1 (18 January 2024), pp. 27–28, 30. \" Agency is what distinguishes us from machines. For biological creatures, reason and purpose come from acting in the world and experiencing the consequences. Artificial intelligences – disembodied, strangers to blood, sweat, and tears – have no occasion for that.\" (p. 30.) Gleick, James , \" The Parrot in the Machine \" (review of Emily M. Bender and Alex Hanna, The AI Con: How to Fight Big Tech's Hype and Create the Future We Want , Harper, 274 pp.; and James Boyle , The Line: AI and the Future of Personhood , MIT Press, 326 pp.), The New York Review of Books , vol. LXXII, no. 12 (24 July 2025), pp. 43–46. \"[C]hatbox 'writing' has a bland, regurgitated quality. Textures are flattened, sharp edges are sanded. No chatbox could ever have said that April is the cruelest month or that fog comes on little cat feet (though they might now, because one of their chief skills is plagiarism ). And when synthetically extruded text turns out wrong, it can be comically wrong. When a movie fan asked Google whether a certain actor was in Heat , he received this 'AI Overview': 'No, Angelina Jolie is not in heat.'\" (p. 44.) Halpern, Sue, \"The Coming Tech Autocracy\" (review of Verity Harding , AI Needs You: How We Can Change AI's Future and Save Our Own , Princeton University Press, 274 pp.; Gary Marcus , Taming Silicon Valley: How We Can Ensure That AI Works for Us , MIT Press, 235 pp.; Daniela Rus and Gregory Mone , The Mind's Mirror: Risk and Reward in the Age of AI , Norton, 280 pp.; Madhumita Murgia , Code Dependent: Living in the Shadow of AI , Henry Holt, 311 pp.), The New York Review of Books , vol. LXXI, no. 17 (7 November 2024), pp. 44–46. \"'We can't realistically expect that those who hope to get rich from AI are going to have the interests of the rest of us close at heart,' ... writes [Gary Marcus]. 'We can't count on governments driven by campaign finance contributions [from tech companies] to push back.'... Marcus details the demands that citizens should make of their governments and the tech companies . They include transparency on how AI systems work; compensation for individuals if their data [are] used to train LLMs ( large language model )s and the right to consent to this use; and the ability to hold tech companies liable for the harms they cause by eliminating Section 230 , imposing cash penalties, and passing stricter product liability laws... Marcus also suggests... that a new, AI-specific federal agency, akin to the FDA , the FCC , or the FTC , might provide the most robust oversight.... [T]he Fordham law professor Chinmayi Sharma ... suggests... establish[ing] a professional licensing regime for engineers that would function in a similar way to medical licenses , malpractice suits, and the Hippocratic oath in medicine. 'What if, like doctors,' she asks..., 'AI engineers also vowed to do no harm ?'\" (p. 46.) Henderson, Mark (24 April 2007). \"Human rights for robots? We're getting carried away\" . The Times Online . London. Archived from the original on 31 May 2014 . Retrieved 31 May 2014 . Hughes-Castleberry, Kenna, \"A Murder Mystery Puzzle: The literary puzzle Cain's Jawbone , which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\", Scientific American ,"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 38,
    "text": "vol. 329, no. 4 (November 2023), pp. 81–82. \"This murder mystery competition has revealed that although NLP ( natural-language processing ) models are capable of incredible feats, their abilities are very much limited by the amount of context they receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyze ancient languages . In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose.\" (p. 82.) Immerwahr, Daniel , \"Your Lying Eyes: People now use A.I. to generate fake videos indistinguishable from real ones. How much does it matter?\", The New Yorker , 20 November 2023, pp. 54–59. \"If by ' deepfakes ' we mean realistic videos produced using artificial intelligence that actually deceive people, then they barely exist. The fakes aren't deep, and the deeps aren't fake. [...] A.I.-generated videos are not, in general, operating in our media as counterfeited evidence. Their role better resembles that of cartoons , especially smutty ones.\" (p. 59.) Johnston, John (2008) The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI , MIT Press. Jumper, John; Evans, Richard; Pritzel, Alexander; et al. (26 August 2021). \"Highly accurate protein structure prediction with AlphaFold\" . Nature . 596 (7873): 583– 589. Bibcode : 2021Natur.596..583J . doi : 10.1038/s41586-021-03819-2 . PMC 8371605 . PMID 34265844 . S2CID 235959867 . LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (28 May 2015). \"Deep learning\" . Nature . 521 (7553): 436– 444. Bibcode : 2015Natur.521..436L . doi : 10.1038/nature14539 . PMID 26017442 . S2CID 3074096 . Archived from the original on 5 June 2023 . Retrieved 19 June 2023 . Leffer, Lauren, \"The Risks of Trusting AI: We must avoid humanizing machine-learning models used in scientific research\", Scientific American , vol. 330, no. 6 (June 2024), pp. 80–81. Lepore, Jill , \"The Chit-Chatbot: Is talking with a machine a conversation?\", The New Yorker , 7 October 2024, pp. 12–16. Maschafilm (2010). \"Content: Plug & Pray Film – Artificial Intelligence – Robots\" . plugandpray-film.de . Archived from the original on 12 February 2016. Marcus, Gary , \"Artificial Confidence: Even the newest, buzziest systems of artificial general intelligence are stymmied by the same old problems\", Scientific American , vol. 327, no. 4 (October 2022), pp. 42–45. Mitchell, Melanie (2019). Artificial intelligence: a guide for thinking humans . New York: Farrar, Straus and Giroux. ISBN 978-0-3742-5783-5 . Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; et al. (26 February 2015). \"Human-level control through deep reinforcement learning\" . Nature . 518 (7540): 529– 533. Bibcode : 2015Natur.518..529M . doi : 10.1038/nature14236 . PMID 25719670 . S2CID 205242740 . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Introduced DQN , which produced human-level performance on some Atari games. Press, Eyal , \"In Front of Their Faces: Does facial-recognition technology lead police to ignore contradictory evidence?\", The New Yorker , 20 November 2023, pp. 20–26. \"Robots could demand legal rights\" . BBC News . 21 December 2006. Archived from the original on 15 October 2019 . Retrieved 3 February 2011 . Roivainen, Eka, \"AI's IQ: ChatGPT aced a [standard intelligence] test but showed that intelligence cannot be measured by IQ alone\", Scientific American , vol. 329, no. 1 (July/August 2023), p. 7. \"Despite its high IQ, ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world.... ChatGPT seemed unable to reason logically and tried to rely on its vast database of... facts derived from online texts.\" Scharre, Paul, \"Killer Apps: The Real Dangers of an AI Arms Race\", Foreign Affairs , vol. 98, no. 3 (May/June 2019), pp. 135–144. \"Today's AI technologies are powerful but unreliable. Rules-based systems cannot deal with circumstances their programmers did not anticipate. Learning systems are limited by the data on which they were trained. AI failures have already led to tragedy. Advanced autopilot features in cars, although they perform well in some circumstances, have driven cars without warning into trucks, concrete barriers, and parked cars. In the wrong situation, AI systems go from supersmart to superdumb in an instant. When an enemy is trying to manipulate and hack an AI system, the risks are even greater.\" (p. 140.) Schulz, Hannes; Behnke, Sven (1 November 2012). \"Deep Learning\" . KI – Künstliche Intelligenz . 26 (4): 357– 363. doi : 10.1007/s13218-012-0198-z . ISSN 1610-1987 . S2CID 220523562 . Serenko, Alexander; Michael Dohan (2011). \"Comparing the expert survey and citation impact journal ranking methods: Example from the field of Artificial Intelligence\" (PDF) . Journal of Informetrics . 5 (4): 629– 649. doi : 10.1016/j.joi.2011.06.002 . Archived (PDF) from the original on 4 October 2013 . Retrieved 12 September 2013 . Silver, David; Huang, Aja; Maddison, Chris J.; et al. (28 January 2016). \"Mastering"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 39,
    "text": "the game of Go with deep neural networks and tree search\" . Nature . 529 (7587): 484– 489. Bibcode : 2016Natur.529..484S . doi : 10.1038/nature16961 . PMID 26819042 . S2CID 515925 . Archived from the original on 18 June 2023 . Retrieved 19 June 2023 . Tarnoff, Ben , \"The Labor Theory of AI\" (review of Matteo Pasquinelli , The Eye of the Master: A Social History of Artificial Intelligence , Verso, 2024, 264 pp.), The New York Review of Books , vol. LXXII, no. 5 (27 March 2025), pp. 30–32. The reviewer, Ben Tarnoff, writes: \"The strangeness at the heart of the generative AI boom is that nobody really knows how the technology works. We know how the large language models within ChatGPT and its counterparts are trained, even if we don't always know which data they're being trained on: they are asked to predict the next string of characters in a sequence. But exactly how they arrive at any given prediction is a mystery. The computations that occur inside the model are simply too intricate for any human to comprehend.\" (p. 32.) Vaswani, Ashish , Noam Shazeer, Niki Parmar et al. \" Attention is all you need .\" Advances in neural information processing systems 30 (2017). Seminal paper on transformers . Vincent, James, \"Horny Robot Baby Voice: James Vincent on AI chatbots\", London Review of Books , vol. 46, no. 19 (10 October 2024), pp. 29–32. \"[AI chatbot] programs are made possible by new technologies but rely on the timelelss human tendency to anthropomorphise .\" (p. 29.) White Paper: On Artificial Intelligence – A European approach to excellence and trust (PDF) . Brussels: European Commission. 2020. Archived (PDF) from the original on 20 February 2020 . Retrieved 20 February 2020 . External links Articles related to artificial intelligence"
  },
  {
    "type": "doc",
    "url": "https://www.technologyreview.com/topic/artificial-intelligence/",
    "title": "Artificial intelligence",
    "chunk": 0,
    "text": "The latest iteration of a legacy Founded at the Massachusetts Institute of Technology in 1899, MIT Technology Review is a world-renowned, independent media company whose insight, analysis, reviews, interviews and live events explain the newest technologies and their commercial, social and political impact."
  },
  {
    "type": "doc",
    "url": "https://developers.google.com/machine-learning/crash-course/ml-intro",
    "title": "Linear regression",
    "chunk": 0,
    "text": "Estimated module length: 70 minutes This module introduces linear regression concepts. Learning objectives: Explain a loss function and how it works. Define and describe how gradient descent finds the optimal model parameters. Describe how to tune hyperparameters to efficiently train a linear model. Linear regression is a statistical technique used to find the relationship between variables. In an ML context, linear regression finds the relationship between features and a label . For example, suppose we want to predict a car's fuel efficiency in miles per gallon based on how heavy the car is, and we have the following dataset: Pounds in 1000s (feature) Miles per gallon (label) 3.5 18 3.69 15 3.44 18 3.43 16 4.34 15 4.42 14 2.37 24 If we plotted these points, we'd get the following graph: Figure 1 . Car heaviness (in pounds) versus miles per gallon rating. As a car gets heavier, its miles per gallon rating generally decreases. We could create our own model by drawing a best fit line through the points: Figure 2 . A best fit line drawn through the data from the previous figure. Linear regression equation In algebraic terms, the model would be defined as $ y = mx + b $, where $ y $ is miles per gallon—the value we want to predict. $ m $ is the slope of the line. $ x $ is pounds—our input value. $ b $ is the y-intercept. In ML, we write the equation for a linear regression model as follows: $$ y' = b + w_1x_1 $$ where: $ y' $ is the predicted label—the output. $ b $ is the bias of the model. Bias is the same concept as the y-intercept in the algebraic equation for a line. In ML, bias is sometimes referred to as $ w_0 $. Bias is a parameter of the model and is calculated during training. $ w_1 $ is the weight of the feature. Weight is the same concept as the slope $ m $ in the algebraic equation for a line. Weight is a parameter of the model and is calculated during training. $ x_1 $ is a feature —the input. During training, the model calculates the weight and bias that produce the best model. Figure 3 . Mathematical representation of a linear model. In our example, we'd calculate the weight and bias from the line we drew. The bias is 34 (where the line intersects the y-axis), and the weight is –4.6 (the slope of the line). The model would be defined as $ y' = 34 + (-4.6)(x_1) $, and we could use it to make predictions. For instance, using this model, a 4,000-pound car would have a predicted fuel efficiency of 15.6 miles per gallon. Figure 4 . Using the model, a 4,000-pound car has a predicted fuel efficiency of 15.6 miles per gallon. Models with multiple features Although the example in this section uses only one feature—the heaviness of the car—a more sophisticated model might rely on multiple features, each having a separate weight ($ w_1 $, $ w_2 $, etc.). For example, a model that relies on five features would be written as follows: $ y' = b + w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5 $ For example, a model that predicts gas mileage could additionally use features such as the following: Engine displacement Acceleration Number of cylinders Horsepower This model would be written as follows: Figure 5 . A model with five features to predict a car's miles per gallon rating. By graphing some of these additional features, we can see that they also have a linear relationship to the label, miles per gallon: Figure 6 . A car's displacement in cubic centimeters and its miles per gallon rating. As a car's engine gets bigger, its miles per gallon rating generally decreases. Figure 7 . A car's acceleration and its miles per gallon rating. As a car's acceleration takes longer, the miles per gallon rating generally increases. Figure 8 . A car's horsepower and its miles per gallon rating. As a car's horsepower increases, the miles per gallon rating generally decreases. Exercise: Check your understanding What parts of the linear regression equation are updated during training? The bias and weights During training, the model updates the bias and weights. The prediction Predictions are not updated during training. The feature values Feature values are part of the dataset, so they're not updated during training."
  },
  {
    "type": "doc",
    "url": "https://www.ibm.com/cloud/learn/deep-learning",
    "title": "What Is Deep Learning?",
    "chunk": 0,
    "text": "Transformer models combine an encoder-decoder architecture with a text-processing mechanism and have revolutionized how language models are trained. An encoder converts raw, unannotated text into representations known as embeddings; the decoder takes these embeddings together with previous outputs of the model, and successively predicts each word in a sentence. Using fill-in-the-blank guessing, the encoder learns how words and sentences relate to each other, building up a powerful representation of language without having to label parts of speech and other grammatical features. Transformers, in fact, can be pretrained at the outset without a particular task in mind. After these powerful representations are learned, the models can later be specialized with much less data to perform a requested task. Several innovations make this possible. Transformers process words in a sentence simultaneously, enabling text processing in parallel, speeding up training. Earlier techniques including recurrent neural networks (RNNs) processed words one by one. Transformers also learned the positions of words and their relationships, this context enables them to infer meaning and disambiguate words such as “it” in long sentences. By eliminating the need to define a task upfront, transformers made it practical to pretrain language models on vast amounts of raw text, enabling them to grow dramatically in size. Previously, labeled data was gathered to train one model on a specific task. With transformers, one model trained on a massive amount of data can be adapted to multiple tasks by fine-tuning it on a small amount of labeled task-specific data. Language transformers today are used for nongenerative tasks such as classification and entity extraction as well as generative tasks including machine translation, summarization and question answering. Transformers have surprised many people with their ability to generate convincing dialog, essays and other content. Natural language processing (NLP) transformers provide remarkable power since they can run in parallel, processing multiple portions of a sequence simultaneously, which then greatly speeds training. Transformers also track long-term dependencies in text, which enables them to understand the overall context more clearly and create superior output. In addition, transformers are more scalable and flexible in order to be customized by task. As to limitations, because of their complexity, transformers require huge computational resources and a long training time. Also, the training data must be accurately on-target, unbiased and plentiful to produce accurate results."
  },
  {
    "type": "doc",
    "url": "https://huggingface.co/transformers/index.html",
    "title": "Transformers",
    "chunk": 0,
    "text": "Transformers Transformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer vision, audio, video, and multimodal model, for both inference and training. It centralizes the model definition so that this definition is agreed upon across the ecosystem. transformers is the pivot across frameworks: if a model definition is supported, it will be compatible with the majority of training frameworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, …), inference engines (vLLM, SGLang, TGI, …), and adjacent modeling libraries (llama.cpp, mlx, …) which leverage the model definition from transformers . We pledge to help support new state-of-the-art models and democratize their usage by having their model definition be simple, customizable, and efficient. There are over 1M+ Transformers model checkpoints on the Hugging Face Hub you can use. Explore the Hub today to find a model and use Transformers to help you get started right away. Features Transformers provides everything you need for inference or training with state-of-the-art pretrained models. Some of the main features include: Pipeline : Simple and optimized inference class for many machine learning tasks like text generation, image segmentation, automatic speech recognition, document question answering, and more. Trainer : A comprehensive trainer that supports features such as mixed precision, torch.compile, and FlashAttention for training and distributed training for PyTorch models. generate : Fast text generation with large language models (LLMs) and vision language models (VLMs), including support for streaming and multiple decoding strategies. Design Read our Philosophy to learn more about Transformers’ design principles. Transformers is designed for developers and machine learning engineers and researchers. Its main design principles are: Fast and easy to use: Every model is implemented from only three main classes (configuration, model, and preprocessor) and can be quickly used for inference or training with Pipeline or Trainer . Pretrained models: Reduce your carbon footprint, compute cost and time by using a pretrained model instead of training an entirely new one. Each pretrained model is reproduced as closely as possible to the original model and offers state-of-the-art performance. Learn If you’re new to Transformers or want to learn more about transformer models, we recommend starting with the LLM course . This comprehensive course covers everything from the fundamentals of how transformer models work to practical applications across various tasks. You’ll learn the complete workflow, from curating high-quality datasets to fine-tuning large language models and implementing reasoning capabilities. The course contains both theoretical and hands-on exercises to build a solid foundational knowledge of transformer models as you learn. < > Update on GitHub"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Neural_network",
    "title": "Neural network - Wikipedia",
    "chunk": 0,
    "text": "Structure in biology and artificial intelligence A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or signal pathways . While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural networks. Animated confocal micrograph of part of a biological neural network in a mouse's striatum In the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses . A given neuron can be connected to hundreds of thousands of synapses. [ 1 ] Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead. [ 1 ] Populations of interconnected neurons that are smaller than neural networks are called neural circuits . Very large interconnected networks are called large scale brain networks , and many of these together form brains and nervous systems . Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells , where they cause contraction and thereby motion. [ 2 ] In machine learning [ edit ] Schematic of a simple feedforward artificial neural network In machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, [ 3 ] today they are almost always implemented in software . Neurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers ( the hidden layers ) to the final layer (the output layer). [ 4 ] The \"signal\" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function . The behavior of the network depends on the strengths (or weights ) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset. [ 5 ] The term deep neural network refers to neural networks that have more than three layers, typically including at least two hidden layers in addition to the input and output layers. Neural networks are used to solve problems in artificial intelligence , and have thereby found applications in many disciplines, including predictive modeling , adaptive control , facial recognition , handwriting recognition , general game playing , and generative AI . The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 [ 6 ] and William James in 1890. [ 7 ] Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning , the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it. [ 8 ] Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism . However, starting with the invention of the perceptron , a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, [ 9 ] followed by the implementation of one in hardware by Frank Rosenblatt in 1957, [ 3 ] artificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts. ^ a b Shao, Feng; Shen, Zheng (January 9, 2022). \"How can artificial neural networks approximate the brain?\" . Front. Psychol . 13 : 970214. doi : 10.3389/fpsyg.2022.970214 . PMC 9868316 . PMID 36698593 . ^ Levitan, Irwin; Kaczmarek, Leonard (August 19, 2015). \"Intercellular communication\". The Neuron: Cell and Molecular Biology (4th ed.). New York, NY: Oxford University Press. pp. 153– 328. ISBN 978-0199773893 . ^ a b Rosenblatt, F. (1958). \"The Perceptron: A Probabilistic Model For Information Storage And Organization In The Brain\". Psychological Review . 65 (6): 386– 408. CiteSeerX 10.1.1.588.3775 . doi : 10.1037/h0042519 . PMID 13602029 . S2CID 12781225 . ^ Bishop, Christopher M. (August 17, 2006). Pattern Recognition and Machine Learning . New York: Springer. ISBN 978-0-387-31073-2 . ^ Vapnik, Vladimir N.; Vapnik, Vladimir Naumovich (1998). The nature of statistical learning theory (Corrected 2nd print. ed.). New York Berlin Heidelberg: Springer. ISBN 978-0-387-94559-0 . ^ Bain (1873). Mind and Body: The Theories of Their Relation . New York: D. Appleton and Company. ^ James (1890). The Principles of Psychology . New York: H. Holt and Company. ^ Hebb, D.O. (1949). The Organization of"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Neural_network",
    "title": "Neural network - Wikipedia",
    "chunk": 1,
    "text": "Behavior . New York: Wiley & Sons. ^ McCulloch, W; Pitts, W (1943). \"A Logical Calculus of Ideas Immanent in Nervous Activity\" (PDF) . Bulletin of Mathematical Biophysics . 5 (4): 115– 133. doi : 10.1007/BF02478259 . Archived from the original on May 17, 2024 . Retrieved February 17, 2024 ."
  },
  {
    "type": "doc",
    "url": "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html",
    "title": "Part 1: Key Concepts in RL",
    "chunk": 0,
    "text": "Welcome to our introduction to reinforcement learning! Here, we aim to acquaint you with In a nutshell, RL is the study of agents and how they learn by trial and error. It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future. Agent-environment interaction loop. The main characters of RL are the agent and the environment . The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment changes when the agent acts on it, but may also change on its own. The agent also perceives a reward signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward, called return . Reinforcement learning methods are ways that the agent can learn behaviors to achieve its goal. To talk more specifically what RL does, we need to introduce additional terminology. We need to talk about states and observations, action spaces, policies, trajectories, different formulations of return, the RL optimization problem, and value functions. States and Observations A state is a complete description of the state of the world. There is no information about the world which is hidden from the state. An observation is a partial description of a state, which may omit information. In deep RL, we almost always represent states and observations by a real-valued vector, matrix, or higher-order tensor . For instance, a visual observation could be represented by the RGB matrix of its pixel values; the state of a robot might be represented by its joint angles and velocities. When the agent is able to observe the complete state of the environment, we say that the environment is fully observed . When the agent can only see a partial observation, we say that the environment is partially observed . You Should Know Reinforcement learning notation sometimes puts the symbol for state, , in places where it would be technically more appropriate to write the symbol for observation, . Specifically, this happens when talking about how the agent decides an action: we often signal in notation that the action is conditioned on the state, when in practice, the action is conditioned on the observation because the agent does not have access to the state. In our guide, we’ll follow standard conventions for notation, but it should be clear from context which is meant. If something is unclear, though, please raise an issue! Our goal is to teach, not to confuse. Action Spaces Different environments allow different kinds of actions. The set of all valid actions in a given environment is often called the action space . Some environments, like Atari and Go, have discrete action spaces , where only a finite number of moves are available to the agent. Other environments, like where the agent controls a robot in a physical world, have continuous action spaces . In continuous spaces, actions are real-valued vectors. This distinction has some quite-profound consequences for methods in deep RL. Some families of algorithms can only be directly applied in one case, and would have to be substantially reworked for the other. Policies A policy is a rule used by an agent to decide what actions to take. It can be deterministic, in which case it is usually denoted by : or it may be stochastic, in which case it is usually denoted by : Because the policy is essentially the agent’s brain, it’s not uncommon to substitute the word “policy” for “agent”, eg saying “The policy is trying to maximize reward.” In deep RL, we deal with parameterized policies : policies whose outputs are computable functions that depend on a set of parameters (eg the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm. We often denote the parameters of such a policy by or , and then write this as a subscript on the policy symbol to highlight the connection: Deterministic Policies Example: Deterministic Policies. Here is a code snippet for building a simple deterministic policy for a continuous action space in PyTorch, using the torch.nn package: pi_net = nn . Sequential ( nn . Linear ( obs_dim , 64 ), nn . Tanh (), nn . Linear ( 64 , 64 ), nn . Tanh (), nn . Linear ( 64 , act_dim ) ) This builds a multi-layer perceptron (MLP) network with two hidden layers of size 64 and activation functions. If obs is a Numpy array containing a batch of observations, pi_net can be"
  },
  {
    "type": "doc",
    "url": "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html",
    "title": "Part 1: Key Concepts in RL",
    "chunk": 1,
    "text": "used to obtain a batch of actions as follows: obs_tensor = torch . as_tensor ( obs , dtype = torch . float32 ) actions = pi_net ( obs_tensor ) You Should Know Don’t worry about it if this neural network stuff is unfamiliar to you—this tutorial will focus on RL, and not on the neural network side of things. So you can skip this example and come back to it later. But we figured that if you already knew, it could be helpful. Stochastic Policies The two most common kinds of stochastic policies in deep RL are categorical policies and diagonal Gaussian policies . Categorical policies can be used in discrete action spaces, while diagonal Gaussian policies are used in continuous action spaces. Two key computations are centrally important for using and training stochastic policies: sampling actions from the policy, and computing log likelihoods of particular actions, . In what follows, we’ll describe how to do these for both categorical and diagonal Gaussian policies. Categorical Policies A categorical policy is like a classifier over discrete actions. You build the neural network for a categorical policy the same way you would for a classifier: the input is the observation, followed by some number of layers (possibly convolutional or densely-connected, depending on the kind of input), and then you have one final linear layer that gives you logits for each action, followed by a softmax to convert the logits into probabilities. Sampling. Given the probabilities for each action, frameworks like PyTorch and Tensorflow have built-in tools for sampling. For example, see the documentation for Categorical distributions in PyTorch , torch.multinomial , tf.distributions.Categorical , or tf.multinomial . Log-Likelihood. Denote the last layer of probabilities as . It is a vector with however many entries as there are actions, so we can treat the actions as indices for the vector. The log likelihood for an action can then be obtained by indexing into the vector: Diagonal Gaussian Policies A multivariate Gaussian distribution (or multivariate normal distribution, if you prefer) is described by a mean vector, , and a covariance matrix, . A diagonal Gaussian distribution is a special case where the covariance matrix only has entries on the diagonal. As a result, we can represent it by a vector. A diagonal Gaussian policy always has a neural network that maps from observations to mean actions, . There are two different ways that the covariance matrix is typically represented. The first way: There is a single vector of log standard deviations, , which is not a function of state: the are standalone parameters. (You Should Know: our implementations of VPG, TRPO, and PPO do it this way.) The second way: There is a neural network that maps from states to log standard deviations, . It may optionally share some layers with the mean network. Note that in both cases we output log standard deviations instead of standard deviations directly. This is because log stds are free to take on any values in , while stds must be nonnegative. It’s easier to train parameters if you don’t have to enforce those kinds of constraints. The standard deviations can be obtained immediately from the log standard deviations by exponentiating them, so we do not lose anything by representing them this way. Sampling. Given the mean action and standard deviation , and a vector of noise from a spherical Gaussian ( ), an action sample can be computed with where denotes the elementwise product of two vectors. Standard frameworks have built-in ways to generate the noise vectors, such as torch.normal or tf.random_normal . Alternatively, you can build distribution objects, eg through torch.distributions.Normal or tf.distributions.Normal , and use them to generate samples. (The advantage of the latter approach is that those objects can also calculate log-likelihoods for you.) Log-Likelihood. The log-likelihood of a -dimensional action , for a diagonal Gaussian with mean and standard deviation , is given by Trajectories A trajectory is a sequence of states and actions in the world, The very first state of the world, , is randomly sampled from the start-state distribution , sometimes denoted by : State transitions (what happens to the world between the state at time , , and the state at , ), are governed by the natural laws of the environment, and depend on only the most recent action, . They can be either deterministic, or stochastic, Actions come from an agent according to its policy. You Should Know Trajectories are also frequently called episodes or rollouts . Reward and Return The reward function is critically important in reinforcement learning. It depends on the current state of the world, the action just taken, and the next state of the world: although frequently this is simplified to just a dependence on the current state, , or state-action pair . The"
  },
  {
    "type": "doc",
    "url": "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html",
    "title": "Part 1: Key Concepts in RL",
    "chunk": 2,
    "text": "goal of the agent is to maximize some notion of cumulative reward over a trajectory, but this actually can mean a few things. We’ll notate all of these cases with , and it will either be clear from context which case we mean, or it won’t matter (because the same equations will apply to all cases). One kind of return is the finite-horizon undiscounted return , which is just the sum of rewards obtained in a fixed window of steps: Another kind of return is the infinite-horizon discounted return , which is the sum of all rewards ever obtained by the agent, but discounted by how far off in the future they’re obtained. This formulation of reward includes a discount factor : Why would we ever want a discount factor, though? Don’t we just want to get all rewards? We do, but the discount factor is both intuitively appealing and mathematically convenient. On an intuitive level: cash now is better than cash later. Mathematically: an infinite-horizon sum of rewards may not converge to a finite value, and is hard to deal with in equations. But with a discount factor and under reasonable conditions, the infinite sum converges. You Should Know While the line between these two formulations of return are quite stark in RL formalism, deep RL practice tends to blur the line a fair bit—for instance, we frequently set up algorithms to optimize the undiscounted return, but use discount factors in estimating value functions . The RL Problem Whatever the choice of return measure (whether infinite-horizon discounted, or finite-horizon undiscounted), and whatever the choice of policy, the goal in RL is to select a policy which maximizes expected return when the agent acts according to it. To talk about expected return, we first have to talk about probability distributions over trajectories. Let’s suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a -step trajectory is: The expected return (for whichever measure), denoted by , is then: The central optimization problem in RL can then be expressed by with being the optimal policy . Value Functions It’s often useful to know the value of a state, or state-action pair. By value, we mean the expected return if you start in that state or state-action pair, and then act according to a particular policy forever after. Value functions are used, one way or another, in almost every RL algorithm. There are four main functions of note here. The On-Policy Value Function , , which gives the expected return if you start in state and always act according to policy : The On-Policy Action-Value Function , , which gives the expected return if you start in state , take an arbitrary action (which may not have come from the policy), and then forever after act according to policy : The Optimal Value Function , , which gives the expected return if you start in state and always act according to the optimal policy in the environment: The Optimal Action-Value Function , , which gives the expected return if you start in state , take an arbitrary action , and then forever after act according to the optimal policy in the environment: You Should Know When we talk about value functions, if we do not make reference to time-dependence, we only mean expected infinite-horizon discounted return . Value functions for finite-horizon undiscounted return would need to accept time as an argument. Can you think about why? Hint: what happens when time’s up? You Should Know There are two key connections between the value function and the action-value function that come up pretty often: and These relations follow pretty directly from the definitions just given: can you prove them? The Optimal Q-Function and the Optimal Action There is an important connection between the optimal action-value function and the action selected by the optimal policy. By definition, gives the expected return for starting in state , taking (arbitrary) action , and then acting according to the optimal policy forever after. The optimal policy in will select whichever action maximizes the expected return from starting in . As a result, if we have , we can directly obtain the optimal action, , via Note: there may be multiple actions which maximize , in which case, all of them are optimal, and the optimal policy may randomly select any of them. But there is always an optimal policy which deterministically selects an action. Bellman Equations All four of the value functions obey special self-consistency equations called Bellman equations . The basic idea behind the Bellman equations is this: The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next. The Bellman equations for the on-policy value"
  },
  {
    "type": "doc",
    "url": "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html",
    "title": "Part 1: Key Concepts in RL",
    "chunk": 3,
    "text": "functions are where is shorthand for , indicating that the next state is sampled from the environment’s transition rules; is shorthand for ; and is shorthand for . The Bellman equations for the optimal value functions are The crucial difference between the Bellman equations for the on-policy value functions and the optimal value functions, is the absence or presence of the over actions. Its inclusion reflects the fact that whenever the agent gets to choose its action, in order to act optimally, it has to pick whichever action leads to the highest value. You Should Know The term “Bellman backup” comes up quite frequently in the RL literature. The Bellman backup for a state, or state-action pair, is the right-hand side of the Bellman equation: the reward-plus-next-value. Advantage Functions Sometimes in RL, we don’t need to describe how good an action is in an absolute sense, but only how much better it is than others on average. That is to say, we want to know the relative advantage of that action. We make this concept precise with the advantage function. The advantage function corresponding to a policy describes how much better it is to take a specific action in state , over randomly selecting an action according to , assuming you act according to forever after. Mathematically, the advantage function is defined by You Should Know We’ll discuss this more later, but the advantage function is crucially important to policy gradient methods."
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 0,
    "text": "[Updated on 2020-09-03: Updated the algorithm of SARSA and Q-learning so that the difference is more pronounced. [Updated on 2021-09-19: Thanks to 爱吃猫的鱼, we have this post in Chinese ]. A couple of exciting news in Artificial Intelligence (AI) has just happened in recent years. AlphaGo defeated the best professional human player in the game of Go. Very soon the extended algorithm AlphaGo Zero beat AlphaGo by 100-0 without supervised learning on human knowledge. Top professional game players lost to the bot developed by OpenAI on DOTA2 1v1 competition. After knowing these, it is pretty hard not to be curious about the magic behind these algorithms — Reinforcement Learning (RL). I’m writing this post to briefly go over the field. We will first introduce several fundamental concepts and then dive into classic approaches to solving RL problems. Hopefully, this post could be a good starting point for newbies, bridging the future study on the cutting-edge research. What is Reinforcement Learning? # Say, we have an agent in an unknown environment and this agent can obtain some rewards by interacting with the environment. The agent ought to take actions so as to maximize cumulative rewards. In reality, the scenario could be a bot playing a game to achieve high scores, or a robot trying to complete physical tasks with physical items; and not just limited to these. An agent interacts with the environment, trying to take smart actions to maximize cumulative rewards. The goal of Reinforcement Learning (RL) is to learn a good strategy for the agent from experimental trials and relative simple feedback received. With the optimal strategy, the agent is capable to actively adapt to the environment to maximize future rewards. Key Concepts # Now Let’s formally define a set of key concepts in RL. The agent is acting in an environment . How the environment reacts to certain actions is defined by a model which we may or may not know. The agent can stay in one of many states ($s \\in \\mathcal{S}$) of the environment, and choose to take one of many actions ($a \\in \\mathcal{A}$) to switch from one state to another. Which state the agent will arrive in is decided by transition probabilities between states ($P$). Once an action is taken, the environment delivers a reward ($r \\in \\mathcal{R}$) as feedback. The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances: Know the model : planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by Dynamic Programming (DP). Do you still remember “longest increasing subsequence” or “traveling salesmen problem” from your Algorithms 101 class? LOL. This is not the focus of this post though. Does not know the model : learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm. Most of the following content serves the scenarios when the model is unknown. The agent’s policy $\\pi(s)$ provides the guideline on what is the optimal action to take in a certain state with the goal to maximize the total rewards . Each state is associated with a value function $V(s)$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy. In other words, the value function quantifies how good a state is. Both policy and value functions are what we try to learn in reinforcement learning. Summary of approaches in RL based on whether we want to model the value, policy, or the environment. (Image source: reproduced from David Silver's RL course lecture 1 .) The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, $t=1, 2, \\dots, T$. During the process, the agent accumulates the knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy. Let’s label the state, action, and reward at time step t as $S_t$, $A_t$, and $R_t$, respectively. Thus the interaction sequence is fully described by one episode (also known as “trial” or “trajectory”) and the sequence ends at the terminal state $S_T$: $$ S_1, A_1, R_2, S_2, A_2, \\dots, S_T $$ Terms you will encounter a lot when diving into different categories of RL algorithms: Model-based : Rely on the model of the environment; either the model is known or the algorithm learns it explicitly. Model-free : No dependency on the model during learning. On-policy : Use the deterministic outcomes or samples from the target policy to train the algorithm. Off-policy : Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 1,
    "text": "policy. Model: Transition and Reward # The model is a descriptor of the environment. With the model, we can learn or infer how the environment would interact with and provide feedback to the agent. The model has two major parts, transition probability function $P$ and reward function $R$. Let’s say when we are in state s, we decide to take action a to arrive in the next state s’ and obtain reward r. This is known as one transition step, represented by a tuple (s, a, s’, r). The transition function P records the probability of transitioning from state s to s’ after taking action a while obtaining reward r. We use $\\mathbb{P}$ as a symbol of “probability”. $$ P(s', r \\vert s, a) = \\mathbb{P} [S_{t+1} = s', R_{t+1} = r \\vert S_t = s, A_t = a] $$ Thus the state-transition function can be defined as a function of $P(s’, r \\vert s, a)$: $$ P_{ss'}^a = P(s' \\vert s, a) = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a] = \\sum_{r \\in \\mathcal{R}} P(s', r \\vert s, a) $$ The reward function R predicts the next reward triggered by one action: $$ R(s, a) = \\mathbb{E} [R_{t+1} \\vert S_t = s, A_t = a] = \\sum_{r\\in\\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} P(s', r \\vert s, a) $$ Policy # Policy, as the agent’s behavior function $\\pi$, tells us which action to take in state s. It is a mapping from state s to action a and can be either deterministic or stochastic: Deterministic: $\\pi(s) = a$. Stochastic: $\\pi(a \\vert s) = \\mathbb{P}_\\pi [A=a \\vert S=s]$. Value Function # Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of future reward. The future reward, also known as return , is a total sum of discounted rewards going forward. Let’s compute the return $G_t$ starting from time t: $$ G_t = R_{t+1} + \\gamma R_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$ The discounting factor $\\gamma \\in [0, 1]$ penalize the rewards in the future, because: The future rewards may have higher uncertainty; i.e. stock market. The future rewards do not provide immediate benefits; i.e. As human beings, we might prefer to have fun today rather than 5 years later ;). Discounting provides mathematical convenience; i.e., we don’t need to track future steps forever to compute return. We don’t need to worry about the infinite loops in the state transition graph. The state-value of a state s is the expected return if we are in this state at time t, $S_t = s$: $$ V_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\vert S_t = s] $$ Similarly, we define the action-value (“Q-value”; Q as “Quality” I believe?) of a state-action pair as: $$ Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a] $$ Additionally, since we follow the target policy $\\pi$, we can make use of the probility distribution over possible actions and the Q-values to recover the state-value: $$ V_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} Q_{\\pi}(s, a) \\pi(a \\vert s) $$ The difference between action-value and state-value is the action advantage function (“A-value”): $$ A_{\\pi}(s, a) = Q_{\\pi}(s, a) - V_{\\pi}(s) $$ Optimal Value and Policy # The optimal value function produces the maximum return: $$ V_{*}(s) = \\max_{\\pi} V_{\\pi}(s), Q_{*}(s, a) = \\max_{\\pi} Q_{\\pi}(s, a) $$ The optimal policy achieves optimal value functions: $$ \\pi_{*} = \\arg\\max_{\\pi} V_{\\pi}(s), \\pi_{*} = \\arg\\max_{\\pi} Q_{\\pi}(s, a) $$ And of course, we have $V_{\\pi_{*}}(s)=V_{*}(s)$ and $Q_{\\pi_{*}}(s, a) = Q_{*}(s, a)$. Markov Decision Processes # In more formal terms, almost all the RL problems can be framed as Markov Decision Processes (MDPs). All states in MDP has “Markov” property, referring to the fact that the future only depends on the current state, not the history: $$ \\mathbb{P}[ S_{t+1} \\vert S_t ] = \\mathbb{P} [S_{t+1} \\vert S_1, \\dots, S_t] $$ Or in other words, the future and the past are conditionally independent given the present, as the current state encapsulates all the statistics we need to decide the future. The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton & Barto (2017).) A Markov deicison process consists of five elements $\\mathcal{M} = \\langle \\mathcal{S}, \\mathcal{A}, P, R, \\gamma \\rangle$, where the symbols carry the same meanings as key concepts in the previous section, well aligned with RL problem settings: $\\mathcal{S}$ - a set of states; $\\mathcal{A}$ - a set of actions; $P$ - transition probability function; $R$ - reward function; $\\gamma$ - discounting factor for future rewards. In an unknown environment, we do not have perfect knowledge about $P$ and $R$. A fun example of Markov decision process: a typical work day. (Image source: randomant.net/reinforcement-learning-concepts ) Bellman Equations # Bellman equations refer to a set of equations that decompose the"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 2,
    "text": "value function into the immediate reward plus the discounted future values. $$ \\begin{aligned} V(s) &= \\mathbb{E}[G_t \\vert S_t = s] \\\\ &= \\mathbb{E} [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\vert S_t = s] \\\\ &= \\mathbb{E} [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\dots) \\vert S_t = s] \\\\ &= \\mathbb{E} [R_{t+1} + \\gamma G_{t+1} \\vert S_t = s] \\\\ &= \\mathbb{E} [R_{t+1} + \\gamma V(S_{t+1}) \\vert S_t = s] \\end{aligned} $$ Similarly for Q-value, $$ \\begin{aligned} Q(s, a) &= \\mathbb{E} [R_{t+1} + \\gamma V(S_{t+1}) \\mid S_t = s, A_t = a] \\\\ &= \\mathbb{E} [R_{t+1} + \\gamma \\mathbb{E}_{a\\sim\\pi} Q(S_{t+1}, a) \\mid S_t = s, A_t = a] \\end{aligned} $$ Bellman Expectation Equations # The recursive update process can be further decomposed to be equations built on both state-value and action-value functions. As we go further in future action steps, we extend V and Q alternatively by following the policy $\\pi$. Illustration of how Bellman expection equations update state-value and action-value functions. $$ \\begin{aligned} V_{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s) Q_{\\pi}(s, a) \\\\ Q_{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_{\\pi} (s') \\\\ V_{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s) \\big( R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_{\\pi} (s') \\big) \\\\ Q_{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\vert s') Q_{\\pi} (s', a') \\end{aligned} $$ Bellman Optimality Equations # If we are only interested in the optimal values, rather than computing the expectation following a policy, we could jump right into the maximum returns during the alternative updates without using a policy. RECAP: the optimal values $V_*$ and $Q_*$ are the best returns we can obtain, defined here . $$ \\begin{aligned} V_*(s) &= \\max_{a \\in \\mathcal{A}} Q_*(s,a)\\\\ Q_*(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_*(s') \\\\ V_*(s) &= \\max_{a \\in \\mathcal{A}} \\big( R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_*(s') \\big) \\\\ Q_*(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a \\max_{a' \\in \\mathcal{A}} Q_*(s', a') \\end{aligned} $$ Unsurprisingly they look very similar to Bellman expectation equations. If we have complete information of the environment, this turns into a planning problem, solvable by DP. Unfortunately, in most scenarios, we do not know $P_{ss’}^a$ or $R(s, a)$, so we cannot solve MDPs by directly applying Bellmen equations, but it lays the theoretical foundation for many RL algorithms. Common Approaches # Now it is the time to go through the major approaches and classic algorithms for solving RL problems. In future posts, I plan to dive into each approach further. Dynamic Programming # When the model is fully known, following Bellman equations, we can use Dynamic Programming (DP) to iteratively evaluate value functions and improve policy. Policy Evaluation # Policy Evaluation is to compute the state-value $V_\\pi$ for a given policy $\\pi$: $$ V_{t+1}(s) = \\mathbb{E}_\\pi [r + \\gamma V_t(s') | S_t = s] = \\sum_a \\pi(a \\vert s) \\sum_{s', r} P(s', r \\vert s, a) (r + \\gamma V_t(s')) $$ Policy Improvement # Based on the value functions, Policy Improvement generates a better policy $\\pi’ \\geq \\pi$ by acting greedily. $$ Q_\\pi(s, a) = \\mathbb{E} [R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\vert S_t=s, A_t=a] = \\sum_{s', r} P(s', r \\vert s, a) (r + \\gamma V_\\pi(s')) $$ Policy Iteration # The Generalized Policy Iteration (GPI) algorithm refers to an iterative procedure to improve the policy when combining policy evaluation and improvement. $$ \\pi_0 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_0} \\xrightarrow[]{\\text{improve}} \\pi_1 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_1} \\xrightarrow[]{\\text{improve}} \\pi_2 \\xrightarrow[]{\\text{evaluation}} \\dots \\xrightarrow[]{\\text{improve}} \\pi_* \\xrightarrow[]{\\text{evaluation}} V_* $$ In GPI, the value function is approximated repeatedly to be closer to the true value of the current policy and in the meantime, the policy is improved repeatedly to approach optimality. This policy iteration process works and always converges to the optimality, but why this is the case? Say, we have a policy $\\pi$ and then generate an improved version $\\pi’$ by greedily taking actions, $\\pi’(s) = \\arg\\max_{a \\in \\mathcal{A}} Q_\\pi(s, a)$. The value of this improved $\\pi’$ is guaranteed to be better because: $$ \\begin{aligned} Q_\\pi(s, \\pi'(s)) &= Q_\\pi(s, \\arg\\max_{a \\in \\mathcal{A}} Q_\\pi(s, a)) \\\\ &= \\max_{a \\in \\mathcal{A}} Q_\\pi(s, a) \\geq Q_\\pi(s, \\pi(s)) = V_\\pi(s) \\end{aligned} $$ Monte-Carlo Methods # First, let’s recall that $V(s) = \\mathbb{E}[ G_t \\vert S_t=s]$. Monte-Carlo (MC) methods uses a simple idea: It learns from episodes of raw experience without modeling the environmental dynamics and computes the observed mean return as an approximation of the expected return. To compute the empirical return $G_t$, MC methods need to learn from complete episodes $S_1, A_1, R_2, \\dots, S_T$ to compute $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$ and all the episodes must eventually terminate. The empirical mean return for state s is: $$ V(s) = \\frac{\\sum_{t=1}^T \\mathbb{1}[S_t = s] G_t}{\\sum_{t=1}^T \\mathbb{1}[S_t = s]} $$"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 3,
    "text": "where $\\mathbb{1}[S_t = s]$ is a binary indicator function. We may count the visit of state s every time so that there could exist multiple visits of one state in one episode (“every-visit”), or only count it the first time we encounter a state in one episode (“first-visit”). This way of approximation can be easily extended to action-value functions by counting (s, a) pair. $$ Q(s, a) = \\frac{\\sum_{t=1}^T \\mathbb{1}[S_t = s, A_t = a] G_t}{\\sum_{t=1}^T \\mathbb{1}[S_t = s, A_t = a]} $$ To learn the optimal policy by MC, we iterate it by following a similar idea to GPI . Improve the policy greedily with respect to the current value function: $\\pi(s) = \\arg\\max_{a \\in \\mathcal{A}} Q(s, a)$. Generate a new episode with the new policy $\\pi$ (i.e. using algorithms like ε-greedy helps us balance between exploitation and exploration.) Estimate Q using the new episode: $q_\\pi(s, a) = \\frac{\\sum_{t=1}^T \\big( \\mathbb{1}[S_t = s, A_t = a] \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1} \\big)}{\\sum_{t=1}^T \\mathbb{1}[S_t = s, A_t = a]}$ Temporal-Difference Learning # Similar to Monte-Carlo methods, Temporal-Difference (TD) Learning is model-free and learns from episodes of experience. However, TD learning can learn from incomplete episodes and hence we don’t need to track the episode up to termination. TD learning is so important that Sutton & Barto (2017) in their RL book describes it as “one idea … central and novel to reinforcement learning”. Bootstrapping # TD learning methods update targets with regard to existing estimates rather than exclusively relying on actual rewards and complete returns as in MC methods. This approach is known as bootstrapping . Value Estimation # The key idea in TD learning is to update the value function $V(S_t)$ towards an estimated return $R_{t+1} + \\gamma V(S_{t+1})$ (known as “ TD target ”). To what extent we want to update the value function is controlled by the learning rate hyperparameter α: $$ \\begin{aligned} V(S_t) &\\leftarrow (1- \\alpha) V(S_t) + \\alpha G_t \\\\ V(S_t) &\\leftarrow V(S_t) + \\alpha (G_t - V(S_t)) \\\\ V(S_t) &\\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)) \\end{aligned} $$ Similarly, for action-value estimation: $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) $$ Next, let’s dig into the fun part on how to learn optimal policy in TD learning (aka “TD control”). Be prepared, you are gonna see many famous names of classic algorithms in this section. SARSA: On-Policy TD control # “SARSA” refers to the procedure of updaing Q-value by following a sequence of $\\dots, S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \\dots$. The idea follows the same route of GPI . Within one episode, it works as follows: Initialize $t=0$. Start with $S_0$ and choose action $A_0 = \\arg\\max_{a \\in \\mathcal{A}} Q(S_0, a)$, where $\\epsilon$-greedy is commonly applied. At time $t$, after applying action $A_t$, we observe reward $R_{t+1}$ and get into the next state $S_{t+1}$. Then pick the next action in the same way as in step 2: $A_{t+1} = \\arg\\max_{a \\in \\mathcal{A}} Q(S_{t+1}, a)$. Update the Q-value function: $ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) $. Set $t = t+1$ and repeat from step 3. In each step of SARSA, we need to choose the next action according to the current policy. Q-Learning: Off-policy TD control # The development of Q-learning ( Watkins & Dayan, 1992 ) is a big breakout in the early days of Reinforcement Learning. Within one episode, it works as follows: Initialize $t=0$. Starts with $S_0$. At time step $t$, we pick the action according to Q values, $A_t = \\arg\\max_{a \\in \\mathcal{A}} Q(S_t, a)$ and $\\epsilon$-greedy is commonly applied. After applying action $A_t$, we observe reward $R_{t+1}$ and get into the next state $S_{t+1}$. Update the Q-value function: $Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t))$. $t = t+1$ and repeat from step 3. The key difference from SARSA is that Q-learning does not follow the current policy to pick the second action $A_{t+1}$. It estimates $Q^*$ out of the best Q values, but which action (denoted as $a^*$) leads to this maximal Q does not matter and in the next step Q-learning may not follow $a^*$. The backup diagrams for Q-learning and SARSA. (Image source: Replotted based on Figure 6.5 in Sutton & Barto (2017)) Deep Q-Network # Theoretically, we can memorize $Q_*(.)$ for all state-action pairs in Q-learning, like in a gigantic table. However, it quickly becomes computationally infeasible when the state and action space are large. Thus people use functions (i.e. a machine learning model) to approximate Q values and this is called function approximation . For example, if we use a function with parameter $\\theta$ to calculate Q values, we can label Q value function as $Q(s,"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 4,
    "text": "a; \\theta)$. Unfortunately Q-learning may suffer from instability and divergence when combined with an nonlinear Q-value function approximation and bootstrapping (See Problems #2 ). Deep Q-Network (“DQN”; Mnih et al. 2015) aims to greatly improve and stabilize the training procedure of Q-learning by two innovative mechanisms: Experience Replay : All the episode steps $e_t = (S_t, A_t, R_t, S_{t+1})$ are stored in one replay memory $D_t = \\{ e_1, \\dots, e_t \\}$. $D_t$ has experience tuples over many episodes. During Q-learning updates, samples are drawn at random from the replay memory and thus one sample could be used multiple times. Experience replay improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution. Periodically Updated Target : Q is optimized towards target values that are only periodically updated. The Q network is cloned and kept frozen as the optimization target every C steps (C is a hyperparameter). This modification makes the training more stable as it overcomes the short-term oscillations. The loss function looks like this: $$ \\mathcal{L}(\\theta) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\Big[ \\big( r + \\gamma \\max_{a'} Q(s', a'; \\theta^{-}) - Q(s, a; \\theta) \\big)^2 \\Big] $$ where $U(D)$ is a uniform distribution over the replay memory D; $\\theta^{-}$ is the parameters of the frozen target Q-network. In addition, it is also found to be helpful to clip the error term to be between [-1, 1]. (I always get mixed feeling with parameter clipping, as many studies have shown that it works empirically but it makes the math much less pretty. :/) Algorithm for DQN with experience replay and occasionally frozen optimization target. The prepossessed sequence is the output of some processes running on the input images of Atari games. Don't worry too much about it; just consider them as input feature vectors. (Image source: Mnih et al. 2015) There are many extensions of DQN to improve the original design, such as DQN with dueling architecture (Wang et al. 2016) which estimates state-value function V(s) and advantage function A(s, a) with shared network parameters. Combining TD and MC Learning # In the previous section on value estimation in TD learning, we only trace one step further down the action chain when calculating the TD target. One can easily extend it to take multiple steps to estimate the return. Let’s label the estimated return following n steps as $G_t^{(n)}, n=1, \\dots, \\infty$, then: $n$ $G_t$ Notes $n=1$ $G_t^{(1)} = R_{t+1} + \\gamma V(S_{t+1})$ TD learning $n=2$ $G_t^{(2)} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})$ … $n=n$ $ G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n}) $ … $n=\\infty$ $G_t^{(\\infty)} = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-t-1} R_T + \\gamma^{T-t} V(S_T) $ MC estimation The generalized n-step TD learning still has the same form for updating the value function: $$ V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^{(n)} - V(S_t)) $$ We are free to pick any $n$ in TD learning as we like. Now the question becomes what is the best $n$? Which $G_t^{(n)}$ gives us the best return approximation? A common yet smart solution is to apply a weighted sum of all possible n-step TD targets rather than to pick a single best n. The weights decay by a factor λ with n, $\\lambda^{n-1}$; the intuition is similar to why we want to discount future rewards when computing the return: the more future we look into the less confident we would be. To make all the weight (n → ∞) sum up to 1, we multiply every weight by (1-λ), because: $$ \\begin{aligned} \\text{let } S &= 1 + \\lambda + \\lambda^2 + \\dots \\\\ S &= 1 + \\lambda(1 + \\lambda + \\lambda^2 + \\dots) \\\\ S &= 1 + \\lambda S \\\\ S &= 1 / (1-\\lambda) \\end{aligned} $$ This weighted sum of many n-step returns is called λ-return $G_t^{\\lambda} = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t^{(n)}$. TD learning that adopts λ-return for value updating is labeled as TD(λ) . The original version we introduced above is equivalent to TD(0) . Comparison of the backup diagrams of Monte-Carlo, Temporal-Difference learning, and Dynamic Programming for state value functions. (Image source: David Silver's RL course lecture 4 : \"Model-Free Prediction\") Policy Gradient # All the methods we have introduced above aim to learn the state/action value function and then to select actions accordingly. Policy Gradient methods instead learn the policy directly with a parameterized function respect to $\\theta$, $\\pi(a \\vert s; \\theta)$. Let’s define the reward function (opposite of loss function) as the expected return and train the algorithm with the goal to maximize the reward function. My next post described why the policy gradient theorem works (proof) and introduced a number of policy gradient algorithms. In discrete space: $$ \\mathcal{J}(\\theta) = V_{\\pi_\\theta}(S_1) = \\mathbb{E}_{\\pi_\\theta}[V_1]"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 5,
    "text": "$$ where $S_1$ is the initial starting state. Or in continuous space: $$ \\mathcal{J}(\\theta) = \\sum_{s \\in \\mathcal{S}} d_{\\pi_\\theta}(s) V_{\\pi_\\theta}(s) = \\sum_{s \\in \\mathcal{S}} \\Big( d_{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s, \\theta) Q_\\pi(s, a) \\Big) $$ where $d_{\\pi_\\theta}(s)$ is stationary distribution of Markov chain for $\\pi_\\theta$. If you are unfamiliar with the definition of a “stationary distribution,” please check this reference . Using gradient ascent we can find the best θ that produces the highest return. It is natural to expect policy-based methods are more useful in continuous space, because there is an infinite number of actions and/or states to estimate the values for in continuous space and hence value-based approaches are computationally much more expensive. Policy Gradient Theorem # Computing the gradient numerically can be done by perturbing θ by a small amount ε in the k-th dimension. It works even when $J(\\theta)$ is not differentiable (nice!), but unsurprisingly very slow. $$ \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial \\theta_k} \\approx \\frac{\\mathcal{J}(\\theta + \\epsilon u_k) - \\mathcal{J}(\\theta)}{\\epsilon} $$ Or analytically , $$ \\mathcal{J}(\\theta) = \\mathbb{E}_{\\pi_\\theta} [r] = \\sum_{s \\in \\mathcal{S}} d_{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) R(s, a) $$ Actually we have nice theoretical support for (replacing $d(.)$ with $d_\\pi(.)$): $$ \\mathcal{J}(\\theta) = \\sum_{s \\in \\mathcal{S}} d_{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) Q_\\pi(s, a) \\propto \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) Q_\\pi(s, a) $$ Check Sec 13.1 in Sutton & Barto (2017) for why this is the case. Then, $$ \\begin{aligned} \\mathcal{J}(\\theta) &= \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) Q_\\pi(s, a) \\\\ \\nabla \\mathcal{J}(\\theta) &= \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\nabla \\pi(a \\vert s; \\theta) Q_\\pi(s, a) \\\\ &= \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) \\frac{\\nabla \\pi(a \\vert s; \\theta)}{\\pi(a \\vert s; \\theta)} Q_\\pi(s, a) \\\\ & = \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) \\nabla \\ln \\pi(a \\vert s; \\theta) Q_\\pi(s, a) \\\\ & = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\ln \\pi(a \\vert s; \\theta) Q_\\pi(s, a)] \\end{aligned} $$ This result is named “Policy Gradient Theorem” which lays the theoretical foundation for various policy gradient algorithms: $$ \\nabla \\mathcal{J}(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\ln \\pi(a \\vert s, \\theta) Q_\\pi(s, a)] $$ REINFORCE # REINFORCE, also known as Monte-Carlo policy gradient, relies on $Q_\\pi(s, a)$, an estimated return by MC methods using episode samples, to update the policy parameter $\\theta$. A commonly used variation of REINFORCE is to subtract a baseline value from the return $G_t$ to reduce the variance of gradient estimation while keeping the bias unchanged. For example, a common baseline is state-value, and if applied, we would use $A(s, a) = Q(s, a) - V(s)$ in the gradient ascent update. Initialize θ at random Generate one episode $S_1, A_1, R_2, S_2, A_2, \\dots, S_T$ For t=1, 2, … , T: Estimate the the return G_t since the time step t. $\\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla \\ln \\pi(A_t \\vert S_t, \\theta)$. Actor-Critic # If the value function is learned in addition to the policy, we would get Actor-Critic algorithm. Critic : updates value function parameters w and depending on the algorithm it could be action-value $Q(a \\vert s; w)$ or state-value $V(s; w)$. Actor : updates policy parameters θ, in the direction suggested by the critic, $\\pi(a \\vert s; \\theta)$. Let’s see how it works in an action-value actor-critic algorithm. Initialize s, θ, w at random; sample $a \\sim \\pi(a \\vert s; \\theta)$. For t = 1… T: Sample reward $r_t \\sim R(s, a)$ and next state $s’ \\sim P(s’ \\vert s, a)$. Then sample the next action $a’ \\sim \\pi(s’, a’; \\theta)$. Update policy parameters: $\\theta \\leftarrow \\theta + \\alpha_\\theta Q(s, a; w) \\nabla_\\theta \\ln \\pi(a \\vert s; \\theta)$. Compute the correction for action-value at time t: $G_{t:t+1} = r_t + \\gamma Q(s’, a’; w) - Q(s, a; w)$ and use it to update value function parameters: $w \\leftarrow w + \\alpha_w G_{t:t+1} \\nabla_w Q(s, a; w) $. Update $a \\leftarrow a’$ and $s \\leftarrow s’$. $\\alpha_\\theta$ and $\\alpha_w$ are two learning rates for policy and value function parameter updates, respectively. A3C # Asynchronous Advantage Actor-Critic (Mnih et al., 2016), short for A3C, is a classic policy gradient method with the special focus on parallel training. In A3C, the critics learn the state-value function, $V(s; w)$, while multiple actors are trained in parallel and get synced with global parameters from time to time. Hence, A3C is good for parallel training by default, i.e. on one machine with multi-core CPU. The loss function for state-value is to minimize the mean squared error, $\\mathcal{J}_v (w) = (G_t - V(s; w))^2$ and we use gradient descent to find the optimal w. This state-value function is used as the baseline in the policy gradient update. Here is the algorithm outline:"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 6,
    "text": "We have global parameters, θ and w; similar thread-specific parameters, θ’ and w'. Initialize the time step t = 1 While T <= T_MAX: Reset gradient: dθ = 0 and dw = 0. Synchronize thread-specific parameters with global ones: θ’ = θ and w’ = w. $t_\\text{start}$ = t and get $s_t$. While ($s_t \\neq \\text{TERMINAL}$) and ($t - t_\\text{start} <= t_\\text{max}$): Pick the action $a_t \\sim \\pi(a_t \\vert s_t; \\theta’)$ and receive a new reward $r_t$ and a new state $s_{t+1}$. Update t = t + 1 and T = T + 1. Initialize the variable that holds the return estimation $$R = \\begin{cases} 0 & \\text{if } s_t \\text{ is TERMINAL} \\ V(s_t; w’) & \\text{otherwise} \\end{cases}$$. For $i = t-1, \\dots, t_\\text{start}$: $R \\leftarrow r_i + \\gamma R$; here R is a MC measure of $G_i$. Accumulate gradients w.r.t. θ’: $d\\theta \\leftarrow d\\theta + \\nabla_{\\theta’} \\log \\pi(a_i \\vert s_i; \\theta’)(R - V(s_i; w’))$; Accumulate gradients w.r.t. w’: $dw \\leftarrow dw + \\nabla_{w’} (R - V(s_i; w’))^2$. Update synchronously θ using dθ, and w using dw. A3C enables the parallelism in multiple agent training. The gradient accumulation step (6.2) can be considered as a reformation of minibatch-based stochastic gradient update: the values of w or θ get corrected by a little bit in the direction of each training thread independently. Evolution Strategies # Evolution Strategies (ES) is a type of model-agnostic optimization approach. It learns the optimal solution by imitating Darwin’s theory of the evolution of species by natural selection. Two prerequisites for applying ES: (1) our solutions can freely interact with the environment and see whether they can solve the problem; (2) we are able to compute a fitness score of how good each solution is. We don’t have to know the environment configuration to solve the problem. Say, we start with a population of random solutions. All of them are capable of interacting with the environment and only candidates with high fitness scores can survive ( only the fittest can survive in a competition for limited resources ). A new generation is then created by recombining the settings ( gene mutation ) of high-fitness survivors. This process is repeated until the new solutions are good enough. Very different from the popular MDP-based approaches as what we have introduced above, ES aims to learn the policy parameter $\\theta$ without value approximation. Let’s assume the distribution over the parameter $\\theta$ is an isotropic multivariate Gaussian with mean $\\mu$ and fixed covariance $\\sigma^2I$. The gradient of $F(\\theta)$ is calculated: $$ \\begin{aligned} & \\nabla_\\theta \\mathbb{E}_{\\theta \\sim N(\\mu, \\sigma^2)} F(\\theta) \\\\ =& \\nabla_\\theta \\int_\\theta F(\\theta) \\Pr(\\theta) && \\text{Pr(.) is the Gaussian density function.} \\\\ =& \\int_\\theta F(\\theta) \\Pr(\\theta) \\frac{\\nabla_\\theta \\Pr(\\theta)}{\\Pr(\\theta)} \\\\ =& \\int_\\theta F(\\theta) \\Pr(\\theta) \\nabla_\\theta \\log \\Pr(\\theta) \\\\ =& \\mathbb{E}_{\\theta \\sim N(\\mu, \\sigma^2)} [F(\\theta) \\nabla_\\theta \\log \\Pr(\\theta)] && \\text{Similar to how we do policy gradient update.} \\\\ =& \\mathbb{E}_{\\theta \\sim N(\\mu, \\sigma^2)} \\Big[ F(\\theta) \\nabla_\\theta \\log \\Big( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(\\theta - \\mu)^2}{2 \\sigma^2 }} \\Big) \\Big] \\\\ =& \\mathbb{E}_{\\theta \\sim N(\\mu, \\sigma^2)} \\Big[ F(\\theta) \\nabla_\\theta \\Big( -\\log \\sqrt{2\\pi\\sigma^2} - \\frac{(\\theta - \\mu)^2}{2 \\sigma^2} \\Big) \\Big] \\\\ =& \\mathbb{E}_{\\theta \\sim N(\\mu, \\sigma^2)} \\Big[ F(\\theta) \\frac{\\theta - \\mu}{\\sigma^2} \\Big] \\end{aligned} $$ We can rewrite this formula in terms of a “mean” parameter $\\theta$ (different from the $\\theta$ above; this $\\theta$ is the base gene for further mutation), $\\epsilon \\sim N(0, I)$ and therefore $\\theta + \\epsilon \\sigma \\sim N(\\theta, \\sigma^2)$. $\\epsilon$ controls how much Gaussian noises should be added to create mutation: $$ \\nabla_\\theta \\mathbb{E}_{\\epsilon \\sim N(0, I)} F(\\theta + \\sigma \\epsilon) = \\frac{1}{\\sigma} \\mathbb{E}_{\\epsilon \\sim N(0, I)} [F(\\theta + \\sigma \\epsilon) \\epsilon] $$ A simple parallel evolution-strategies-based RL algorithm. Parallel workers share the random seeds so that they can reconstruct the Gaussian noises with tiny communication bandwidth. (Image source: Salimans et al. 2017.) ES, as a black-box optimization algorithm, is another approach to RL problems ( In my original writing, I used the phrase “a nice alternative”; Seita pointed me to this discussion and thus I updated my wording. ). It has a couple of good characteristics (Salimans et al., 2017) keeping it fast and easy to train: ES does not need value function approximation; ES does not perform gradient back-propagation; ES is invariant to delayed or long-term rewards; ES is highly parallelizable with very little data communication. Known Problems # Exploration-Exploitation Dilemma # The problem of exploration vs exploitation dilemma has been discussed in my previous post . When the RL problem faces an unknown environment, this issue is especially a key to finding a good solution: without enough exploration, we cannot learn the environment well enough; without enough exploitation, we cannot complete our reward optimization task. Different RL algorithms balance between exploration and exploitation in different ways. In MC methods, Q-learning or many on-policy algorithms, the exploration is commonly implemented"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 7,
    "text": "by ε-greedy ; In ES , the exploration is captured by the policy parameter perturbation. Please keep this into consideration when developing a new RL algorithm. Deadly Triad Issue # We do seek the efficiency and flexibility of TD methods that involve bootstrapping. However, when off-policy, nonlinear function approximation, and bootstrapping are combined in one RL algorithm, the training could be unstable and hard to converge. This issue is known as the deadly triad (Sutton & Barto, 2017). Many architectures using deep learning models were proposed to resolve the problem, including DQN to stabilize the training with experience replay and occasionally frozen target network. Case Study: AlphaGo Zero # The game of Go has been an extremely hard problem in the field of Artificial Intelligence for decades until recent years. AlphaGo and AlphaGo Zero are two programs developed by a team at DeepMind. Both involve deep Convolutional Neural Networks ( CNN ) and Monte Carlo Tree Search (MCTS) and both have been approved to achieve the level of professional human Go players. Different from AlphaGo that relied on supervised learning from expert human moves, AlphaGo Zero used only reinforcement learning and self-play without human knowledge beyond the basic rules. The board of Go. Two players play black and white stones alternatively on the vacant intersections of a board with 19 x 19 lines. A group of stones must have at least one open point (an intersection, called a \"liberty\") to remain on the board and must have at least two or more enclosed liberties (called \"eyes\") to stay \"alive\". No stone shall repeat a previous position. With all the knowledge of RL above, let’s take a look at how AlphaGo Zero works. The main component is a deep CNN over the game board configuration (precisely, a ResNet with batch normalization and ReLU). This network outputs two values: $$ (p, v) = f_\\theta(s) $$ $s$: the game board configuration, 19 x 19 x 17 stacked feature planes; 17 features for each position, 8 past configurations (including current) for the current player + 8 past configurations for the opponent + 1 feature indicating the color (1=black, 0=white). We need to code the color specifically because the network is playing with itself and the colors of current player and opponents are switching between steps. $p$: the probability of selecting a move over 19^2 + 1 candidates (19^2 positions on the board, in addition to passing). $v$: the winning probability given the current setting. During self-play, MCTS further improves the action probability distribution $\\pi \\sim p(.)$ and then the action $a_t$ is sampled from this improved policy. The reward $z_t$ is a binary value indicating whether the current player eventually wins the game. Each move generates an episode tuple $(s_t, \\pi_t, z_t)$ and it is saved into the replay memory. The details on MCTS are skipped for the sake of space in this post; please read the original paper if you are interested. AlphaGo Zero is trained by self-play while MCTS improves the output policy further in every step. (Image source: Figure 1a in Silver et al., 2017). The network is trained with the samples in the replay memory to minimize the loss: $$ \\mathcal{L} = (z - v)^2 - \\pi^\\top \\log p + c \\| \\theta \\|^2 $$ where $c$ is a hyperparameter controlling the intensity of L2 penalty to avoid overfitting. AlphaGo Zero simplified AlphaGo by removing supervised learning and merging separated policy and value networks into one. It turns out that AlphaGo Zero achieved largely improved performance with a much shorter training time! I strongly recommend reading these two papers side by side and compare the difference, super fun. I know this is a long read, but hopefully worth it. If you notice mistakes and errors in this post, don’t hesitate to contact me at [lilian dot wengweng at gmail dot com]. See you in the next post! :) Cited as: @article{weng2018bandit, title = \"A (Long) Peek into Reinforcement Learning\", author = \"Weng, Lilian\", journal = \"lilianweng.github.io\", year = \"2018\", url = \"https://lilianweng.github.io/posts/2018-02-19-rl-overview/\" } References # [1] Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274. 2017. [2] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition . 2017. [3] Volodymyr Mnih, et al. Asynchronous methods for deep reinforcement learning. ICML. 2016. [4] Tim Salimans, et al. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864 (2017). [5] David Silver, et al. Mastering the game of go without human knowledge . Nature 550.7676 (2017): 354. [6] David Silver, et al. Mastering the game of Go with deep neural networks and tree search. Nature 529.7587 (2016): 484-489. [7] Volodymyr Mnih, et al. Human-level control through deep reinforcement learning. Nature 518.7540 (2015): 529. [8] Ziyu Wang, et al. Dueling network architectures for deep reinforcement learning."
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 8,
    "text": "ICML. 2016. [9] Reinforcement Learning lectures by David Silver on YouTube. [10] OpenAI Blog: Evolution Strategies as a Scalable Alternative to Reinforcement Learning [11] Frank Sehnke, et al. Parameter-exploring policy gradients. Neural Networks 23.4 (2010): 551-559. [12] Csaba Szepesvári. Algorithms for reinforcement learning. 1st Edition. Synthesis lectures on artificial intelligence and machine learning 4.1 (2010): 1-103. If you notice mistakes and errors in this post, please don’t hesitate to contact me at [lilian dot wengweng at gmail dot com] and I would be super happy to correct them right away!"
  },
  {
    "type": "doc",
    "url": "https://huggingface.co/learn/deep-rl-course/unit0/introduction",
    "title": "Welcome to the 🤗 Deep Reinforcement Learning Course",
    "chunk": 0,
    "text": "Welcome to the 🤗 Deep Reinforcement Learning Course Welcome to the most fascinating topic in Artificial Intelligence: Deep Reinforcement Learning . This course will teach you about Deep Reinforcement Learning from beginner to expert . It’s completely free and open-source! In this introduction unit you’ll: Learn more about the course content . Define the path you’re going to take (either self-audit or certification process). Learn more about the AI vs. AI challenges you’re going to participate in. Learn more about us . Create your Hugging Face account (it’s free). Sign-up to our Discord server , the place where you can chat with your classmates and us (the Hugging Face team). Let’s get started! What to expect? In this course, you will: 📖 Study Deep Reinforcement Learning in theory and practice. 🧑‍💻 Learn to use famous Deep RL libraries such as Stable Baselines3 , RL Baselines3 Zoo , Sample Factory and CleanRL . 🤖 Train agents in unique environments such as SnowballFight , Huggy the Doggo 🐶 , VizDoom (Doom) and classical ones such as Space Invaders , PyBullet and more. 💾 Share your trained agents with one line of code to the Hub and also download powerful agents from the community. 🏆 Participate in challenges where you will evaluate your agents against other teams. You’ll also get to play against the agents you’ll train. 🎓 Earn a certificate of completion by completing 80% of the assignments. And more! At the end of this course, you’ll get a solid foundation from the basics to the SOTA (state-of-the-art) of methods . Don’t forget to sign up to the course (we are collecting your email to be able to send you the links when each Unit is published and give you information about the challenges and updates). Sign up 👉 here Course Maintenance Notice 🚧 Please note that this Deep Reinforcement Learning course is now in a low-maintenance state . However, it remains an excellent resource to learn both the theory and practical aspects of Deep Reinforcement Learning . Keep in mind the following points: Unit 7 (AI vs AI) : This feature is currently non-functional. However, you can still train your agent to play soccer and observe its performance. Leaderboard : The leaderboard is no longer operational. Aside from these points, all theory content and practical exercises remain fully accessible and effective for learning. If you have any problem with one of the hands-on please check the issue sections where the community give some solutions to bugs . What does the course look like? The course is composed of: A theory part : where you learn a concept in theory . A hands-on : where you’ll learn to use famous Deep RL libraries to train your agents in unique environments. These hands-on will be Google Colab notebooks with companion tutorial videos if you prefer learning with video format! Challenges : you’ll get to put your agent to compete against other agents in different challenges. There will also be a leaderboard for you to compare the agents’ performance. What’s the syllabus? This is the course’s syllabus: Two paths: choose your own adventure You can choose to follow this course either: To get a certificate of completion : you need to complete 80% of the assignments. To get a certificate of honors : you need to complete 100% of the assignments. As a simple audit : you can participate in all challenges and do assignments if you want. There’s no deadlines, the course is self-paced . Both paths are completely free . Whatever path you choose, we advise you to follow the recommended pace to enjoy the course and challenges with your fellow classmates. You don’t need to tell us which path you choose. If you get more than 80% of the assignments done, you’ll get a certificate. The Certification Process The certification process is completely free : To get a certificate of completion : you need to complete 80% of the assignments. To get a certificate of honors : you need to complete 100% of the assignments. Again, there’s no deadline since the course is self paced. But our advice is to follow the recommended pace section . How to get most of the course? To get most of the course, we have some advice: Join study groups in Discord : studying in groups is always easier. To do that, you need to join our discord server. If you're new to Discord, no worries! We have some tools that will help you learn about it. Do the quizzes and assignments : the best way to learn is to do and test yourself. Define a schedule to stay in sync : you can use our recommended pace schedule below or create yours. What tools do I need? You need only 3 things: A computer"
  },
  {
    "type": "doc",
    "url": "https://huggingface.co/learn/deep-rl-course/unit0/introduction",
    "title": "Welcome to the 🤗 Deep Reinforcement Learning Course",
    "chunk": 1,
    "text": "with an internet connection. Google Colab (free version) : most of our hands-on will use Google Colab, the free version is enough. A Hugging Face Account : to push and load models. If you don’t have an account yet, you can create one here (it’s free). What is the recommended pace? Each chapter in this course is designed to be completed in 1 week, with approximately 3-4 hours of work per week . However, you can take as much time as necessary to complete the course. If you want to dive into a topic more in-depth, we’ll provide additional resources to help you achieve that. Who are we About the author: Thomas Simonini is a Developer Advocate at Hugging Face 🤗 specializing in Deep Reinforcement Learning. He founded the Deep Reinforcement Learning Course in 2018, which became one of the most used courses in Deep RL. About the team: Omar Sanseviero is a Machine Learning Engineer at Hugging Face where he works in the intersection of ML, Community and Open Source. Previously, Omar worked as a Software Engineer at Google in the teams of Assistant and TensorFlow Graphics. He is from Peru and likes llamas 🦙. Sayak Paul is a Developer Advocate Engineer at Hugging Face. He's interested in the area of representation learning (self-supervision, semi-supervision, model robustness). And he loves watching crime and action thrillers 🔪. What are the challenges in this course? In this new version of the course, you have two types of challenges: A leaderboard to compare your agent’s performance to other classmates’. AI vs. AI challenges where you can train your agent and compete against other classmates’ agents. I still have questions Please ask your question in our discord server #rl-discussions. < > Update on GitHub"
  },
  {
    "type": "doc",
    "url": "https://huggingface.co/learn/deep-rl-course/unit1/introduction",
    "title": "Introduction to Deep Reinforcement Learning",
    "chunk": 0,
    "text": "Introduction to Deep Reinforcement Learning Welcome to the most fascinating topic in Artificial Intelligence: Deep Reinforcement Learning. Deep RL is a type of Machine Learning where an agent learns how to behave in an environment by performing actions and seeing the results. In this first unit, you’ll learn the foundations of Deep Reinforcement Learning. Then, you’ll train your Deep Reinforcement Learning agent, a lunar lander to land correctly on the Moon using Stable-Baselines3 , a Deep Reinforcement Learning library. And finally, you’ll upload this trained agent to the Hugging Face Hub 🤗, a free, open platform where people can share ML models, datasets, and demos. It’s essential to master these elements before diving into implementing Deep Reinforcement Learning agents. The goal of this chapter is to give you solid foundations. After this unit, in a bonus unit, you’ll be able to train Huggy the Dog 🐶 to fetch the stick and play with him 🤗 . So let’s get started! 🚀 < > Update on GitHub"
  },
  {
    "type": "doc",
    "url": "https://spinningup.openai.com/en/latest/algorithms/ppo.html",
    "title": "Proximal Policy Optimization - Spinning Up documentation",
    "chunk": 0,
    "text": "PPO is motivated by the same question as TRPO: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse? Where TRPO tries to solve this problem with a complex second-order method, PPO is a family of first-order methods that use a few other tricks to keep new policies close to old. PPO methods are significantly simpler to implement, and empirically seem to perform at least as well as TRPO. There are two primary variants of PPO: PPO-Penalty and PPO-Clip. Here, we’ll focus only on PPO-Clip (the primary variant used at OpenAI). PPO-clip updates policies via typically taking multiple steps of (usually minibatch) SGD to maximize the objective. Here is given by in which is a (small) hyperparameter which roughly says how far away the new policy is allowed to go from the old. This is a pretty complex expression, and it’s hard to tell at first glance what it’s doing, or how it helps keep the new policy close to the old policy. As it turns out, there’s a considerably simplified version of this objective which is a bit easier to grapple with (and is also the version we implement in our code): where To figure out what intuition to take away from this, let’s look at a single state-action pair , and think of cases. Advantage is positive : Suppose the advantage for that state-action pair is positive, in which case its contribution to the objective reduces to Because the advantage is positive, the objective will increase if the action becomes more likely—that is, if increases. But the min in this term puts a limit to how much the objective can increase. Once , the min kicks in and this term hits a ceiling of . Thus: the new policy does not benefit by going far away from the old policy . Advantage is negative : Suppose the advantage for that state-action pair is negative, in which case its contribution to the objective reduces to Because the advantage is negative, the objective will increase if the action becomes less likely—that is, if decreases. But the max in this term puts a limit to how much the objective can increase. Once , the max kicks in and this term hits a ceiling of . Thus, again: the new policy does not benefit by going far away from the old policy . What we have seen so far is that clipping serves as a regularizer by removing incentives for the policy to change dramatically, and the hyperparameter corresponds to how far away the new policy can go from the old while still profiting the objective. You Should Know While this kind of clipping goes a long way towards ensuring reasonable policy updates, it is still possible to end up with a new policy which is too far from the old policy, and there are a bunch of tricks used by different PPO implementations to stave this off. In our implementation here, we use a particularly simple method: early stopping. If the mean KL-divergence of the new policy from the old grows beyond a threshold, we stop taking gradient steps. When you feel comfortable with the basic math and implementation details, it’s worth checking out other implementations to see how they handle this issue!"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 0,
    "text": "Mathematical model for sequential decision making under uncertainty Markov decision process ( MDP ), also called a stochastic dynamic program or stochastic control problem, is a model for sequential decision making when outcomes are uncertain. [ 1 ] Originating from operations research in the 1950s, [ 2 ] [ 3 ] MDPs have since gained recognition in a variety of fields, including ecology , economics , healthcare , telecommunications and reinforcement learning . [ 4 ] Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. In this framework, the interaction is characterized by states, actions, and rewards. The MDP framework is designed to provide a simplified representation of key elements of artificial intelligence challenges. These elements encompass the understanding of cause and effect , the management of uncertainty and nondeterminism, and the pursuit of explicit goals. [ 4 ] The name comes from its connection to Markov chains , a concept developed by the Russian mathematician Andrey Markov . The \"Markov\" in \"Markov decision process\" refers to the underlying structure of state transitions that still follow the Markov property . The process is called a \"decision process\" because it involves making decisions that influence these state transitions, extending the concept of a Markov chain into the realm of decision-making under uncertainty. Example of a simple MDP with three states (green circles) and two actions (orange circles), with two rewards (orange arrows) A Markov decision process is a 4- tuple ( S , A , P a , R a ) {\\displaystyle (S,A,P_{a},R_{a})} , where: S {\\displaystyle S} is a set of states called the state space . The state space may be discrete or continuous, like the set of real numbers . A {\\displaystyle A} is a set of actions called the action space (alternatively, A s {\\displaystyle A_{s}} is the set of actions available from state s {\\displaystyle s} ). As for state, this set may be discrete or continuous. P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} is, on an intuitive level, the probability that action a {\\displaystyle a} in state s {\\displaystyle s} at time t {\\displaystyle t} will lead to state s ′ {\\displaystyle s'} at time t + 1 {\\displaystyle t+1} . In general, this probability transition is defined to satisfy Pr ( s t + 1 ∈ S ′ ∣ s t = s , a t = a ) = ∫ S ′ P a ( s , s ′ ) d s ′ , {\\displaystyle \\Pr(s_{t+1}\\in S'\\mid s_{t}=s,a_{t}=a)=\\int _{S'}P_{a}(s,s')ds',} for every S ′ ⊆ S {\\displaystyle S'\\subseteq S} measurable. In case the state space is discrete, the integral is intended with respect to the counting measure , so that the latter simplifies as P a ( s , s ′ ) = Pr ( s t + 1 = s ′ ∣ s t = s , a t = a ) {\\displaystyle P_{a}(s,s')=\\Pr(s_{t+1}=s'\\mid s_{t}=s,a_{t}=a)} ; In case S ⊆ R d {\\displaystyle S\\subseteq \\mathbb {R} ^{d}} , the integral is usually intended with respect to the Lebesgue measure . R a ( s , s ′ ) {\\displaystyle R_{a}(s,s')} is the immediate reward (or expected immediate reward) received after transitioning from state s {\\displaystyle s} to state s ′ {\\displaystyle s'} , due to action a {\\displaystyle a} . A policy function π {\\displaystyle \\pi } is a (potentially probabilistic) mapping from state space ( S {\\displaystyle S} ) to action space ( A {\\displaystyle A} ). Optimization objective [ edit ] The goal in a Markov decision process is to find a good \"policy\" for the decision maker: a function π {\\displaystyle \\pi } that specifies the action π ( s ) {\\displaystyle \\pi (s)} that the decision maker will choose when in state s {\\displaystyle s} . Once a Markov decision process is combined with a policy in this way, this fixes the action for each state and the resulting combination behaves like a Markov chain (since the action chosen in state s {\\displaystyle s} is completely determined by π ( s ) {\\displaystyle \\pi (s)} ). The objective is to choose a policy π {\\displaystyle \\pi } that will maximize some cumulative function of the random rewards, typically the expected discounted sum over a potentially infinite horizon: E [ ∑ t = 0 ∞ γ t R a t ( s t , s t + 1 ) ] {\\displaystyle E\\left[\\sum _{t=0}^{\\infty }{\\gamma ^{t}R_{a_{t}}(s_{t},s_{t+1})}\\right]} (where we choose a t = π ( s t ) {\\displaystyle a_{t}=\\pi (s_{t})} , i.e. actions given by the policy). And the expectation is taken over s t + 1 ∼ P a t ( s t , s t + 1 ) {\\displaystyle s_{t+1}\\sim P_{a_{t}}(s_{t},s_{t+1})} where γ {\\displaystyle \\ \\gamma \\ }"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 1,
    "text": "is the discount factor satisfying 0 ≤ γ ≤ 1 {\\displaystyle 0\\leq \\ \\gamma \\ \\leq \\ 1} , which is usually close to 1 {\\displaystyle 1} (for example, γ = 1 / ( 1 + r ) {\\displaystyle \\gamma =1/(1+r)} for some discount rate r {\\displaystyle r} ). A lower discount factor makes the decision maker more short-sighted, in that it comparatively disregards the effect that following its current policy has at times lying further in the future. Another possible, but strictly related, objective that is commonly used is the H − {\\displaystyle H-} step return. This time, instead of using a discount factor γ {\\displaystyle \\ \\gamma \\ } , the agent is interested only in the first H {\\displaystyle H} steps of the process, with each reward having the same weight. E [ ∑ t = 0 H − 1 R a t ( s t , s t + 1 ) ] {\\displaystyle E\\left[\\sum _{t=0}^{H-1}{R_{a_{t}}(s_{t},s_{t+1})}\\right]} (where we choose a t = π ( s t ) {\\displaystyle a_{t}=\\pi (s_{t})} , i.e. actions given by the policy). And the expectation is taken over s t + 1 ∼ P a t ( s t , s t + 1 ) {\\displaystyle s_{t+1}\\sim P_{a_{t}}(s_{t},s_{t+1})} where H {\\displaystyle \\ H\\ } is the time horizon. Compared to the previous objective, the latter one is more used in Learning Theory . A policy that maximizes the function above is called an optimal policy and is usually denoted π ∗ {\\displaystyle \\pi ^{*}} . A particular MDP may have multiple distinct optimal policies. Because of the Markov property , it can be shown that the optimal policy is a function of the current state, as assumed above. In many cases, it is difficult to represent the transition probability distributions, P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} , explicitly. In such cases, a simulator can be used to model the MDP implicitly by providing samples from the transition distributions. One common form of implicit MDP model is an episodic environment simulator that can be started from an initial state and yields a subsequent state and reward every time it receives an action input. In this manner, trajectories of states, actions, and rewards, often called episodes may be produced. Another form of simulator is a generative model , a single step simulator that can generate samples of the next state and reward given any state and action. [ 5 ] (Note that this is a different meaning from the term generative model in the context of statistical classification .) In algorithms that are expressed using pseudocode , G {\\displaystyle G} is often used to represent a generative model. For example, the expression s ′ , r ← G ( s , a ) {\\displaystyle s',r\\gets G(s,a)} might denote the action of sampling from the generative model where s {\\displaystyle s} and a {\\displaystyle a} are the current state and action, and s ′ {\\displaystyle s'} and r {\\displaystyle r} are the new state and reward. Compared to an episodic simulator, a generative model has the advantage that it can yield data from any state, not only those encountered in a trajectory. These model classes form a hierarchy of information content: an explicit model trivially yields a generative model through sampling from the distributions, and repeated application of a generative model yields an episodic simulator. In the opposite direction, it is only possible to learn approximate models through regression . The type of model available for a particular MDP plays a significant role in determining which solution algorithms are appropriate. For example, the dynamic programming algorithms described in the next section require an explicit model, and Monte Carlo tree search requires a generative model (or an episodic simulator that can be copied at any state), whereas most reinforcement learning algorithms require only an episodic simulator. Pole Balancing example (rendering of the environment from the Open AI gym benchmark ) An example of MDP is the Pole-Balancing model, which comes from classic control theory. In this example, we have S {\\displaystyle S} is the set of ordered tuples ( θ , θ ˙ , x , x ˙ ) ⊂ R 4 {\\displaystyle (\\theta ,{\\dot {\\theta }},x,{\\dot {x}})\\subset \\mathbb {R} ^{4}} given by pole angle, angular velocity, position of the cart and its speed. A {\\displaystyle A} is { − 1 , 1 } {\\displaystyle \\{-1,1\\}} , corresponding to applying a force on the left (right) on the cart. P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} is the transition of the system, which in this case is going to be deterministic and driven by the laws of mechanics. R a ( s , s ′ ) {\\displaystyle R_{a}(s,s')} is 1 {\\displaystyle 1} if the pole is up after the"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 2,
    "text": "transition, zero otherwise. Therefore, this function only depend on s ′ {\\displaystyle s'} in this specific case. Solutions for MDPs with finite state and action spaces may be found through a variety of methods such as dynamic programming . The algorithms in this section apply to MDPs with finite state and action spaces and explicitly given transition probabilities and reward functions, but the basic concepts may be extended to handle other problem classes, for example using function approximation . Also, some processes with countably infinite state and action spaces can be exactly reduced to ones with finite state and action spaces. [ 6 ] The standard family of algorithms to calculate optimal policies for finite state and action MDPs requires storage for two arrays indexed by state: value V {\\displaystyle V} , which contains real values, and policy π {\\displaystyle \\pi } , which contains actions. At the end of the algorithm, π {\\displaystyle \\pi } will contain the solution and V ( s ) {\\displaystyle V(s)} will contain the discounted sum of the rewards to be earned (on average) by following that solution from state s {\\displaystyle s} . The algorithm has two steps, (1) a value update and (2) a policy update, which are repeated in some order for all the states until no further changes take place. Both recursively update a new estimation of the optimal policy and state value using an older estimation of those values. V ( s ) := ∑ s ′ P π ( s ) ( s , s ′ ) ( R π ( s ) ( s , s ′ ) + γ V ( s ′ ) ) {\\displaystyle V(s):=\\sum _{s'}P_{\\pi (s)}(s,s')\\left(R_{\\pi (s)}(s,s')+\\gamma V(s')\\right)} π ( s ) := argmax a ⁡ { ∑ s ′ P a ( s , s ′ ) ( R a ( s , s ′ ) + γ V ( s ′ ) ) } {\\displaystyle \\pi (s):=\\operatorname {argmax} _{a}\\left\\{\\sum _{s'}P_{a}(s,s')\\left(R_{a}(s,s')+\\gamma V(s')\\right)\\right\\}} Their order depends on the variant of the algorithm; one can also do them for all states at once or state by state, and more often to some states than others. As long as no state is permanently excluded from either of the steps, the algorithm will eventually arrive at the correct solution. [ 7 ] In value iteration ( Bellman 1957 ), which is also called backward induction , the π {\\displaystyle \\pi } function is not used; instead, the value of π ( s ) {\\displaystyle \\pi (s)} is calculated within V ( s ) {\\displaystyle V(s)} whenever it is needed. Substituting the calculation of π ( s ) {\\displaystyle \\pi (s)} into the calculation of V ( s ) {\\displaystyle V(s)} gives the combined step; [ further explanation needed ] V i + 1 ( s ) := max a { ∑ s ′ P a ( s , s ′ ) ( R a ( s , s ′ ) + γ V i ( s ′ ) ) } , {\\displaystyle V_{i+1}(s):=\\max _{a}\\left\\{\\sum _{s'}P_{a}(s,s')\\left(R_{a}(s,s')+\\gamma V_{i}(s')\\right)\\right\\},} where i {\\displaystyle i} is the iteration number. Value iteration starts at i = 0 {\\displaystyle i=0} and V 0 {\\displaystyle V_{0}} as a guess of the value function . It then iterates, repeatedly computing V i + 1 {\\displaystyle V_{i+1}} for all states s {\\displaystyle s} , until V {\\displaystyle V} converges with the left-hand side equal to the right-hand side (which is the \" Bellman equation \" for this problem [ clarification needed ] ). Lloyd Shapley 's 1953 paper on stochastic games included as a special case the value iteration method for MDPs, [ 8 ] but this was recognized only later on. [ 9 ] In policy iteration ( Howard 1960 ) harv error: no target: CITEREFHoward1960 ( help ) , step one is performed once, and then step two is performed once, then both are repeated until policy converges. Then step one is again performed once and so on. (Policy iteration was invented by Howard to optimize Sears catalogue mailing, which he had been optimizing using value iteration. [ 10 ] ) Instead of repeating step two to convergence, it may be formulated and solved as a set of linear equations. These equations are merely obtained by making s = s ′ {\\displaystyle s=s'} in the step two equation. [ clarification needed ] Thus, repeating step two to convergence can be interpreted as solving the linear equations by relaxation . This variant has the advantage that there is a definite stopping condition: when the array π {\\displaystyle \\pi } does not change in the course of applying step 1 to all states, the algorithm is completed. Policy iteration is usually slower than value iteration for a large number of possible states. Modified policy iteration ["
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 3,
    "text": "edit ] In modified policy iteration ( van Nunen 1976 ; Puterman & Shin 1978 ), step one is performed once, and then step two is repeated several times. [ 11 ] [ 12 ] Then step one is again performed once and so on. Prioritized sweeping [ edit ] In this variant, the steps are preferentially applied to states which are in some way important – whether based on the algorithm (there were large changes in V {\\displaystyle V} or π {\\displaystyle \\pi } around those states recently) or based on use (those states are near the starting state, or otherwise of interest to the person or program using the algorithm). Computational complexity [ edit ] Algorithms for finding optimal policies with time complexity polynomial in the size of the problem representation exist for finite MDPs. Thus, decision problems based on MDPs are in computational complexity class P . [ 13 ] However, due to the curse of dimensionality , the size of the problem representation is often exponential in the number of state and action variables, limiting exact solution techniques to problems that have a compact representation. In practice, online planning techniques such as Monte Carlo tree search can find useful solutions in larger problems, and, in theory, it is possible to construct online planning algorithms that can find an arbitrarily near-optimal policy with no computational complexity dependence on the size of the state space. [ 14 ] Extensions and generalizations [ edit ] A Markov decision process is a stochastic game with only one player. Partial observability [ edit ] The solution above assumes that the state s {\\displaystyle s} is known when action is to be taken; otherwise π ( s ) {\\displaystyle \\pi (s)} cannot be calculated. When this assumption is not true, the problem is called a partially observable Markov decision process or POMDP. Constrained Markov decision processes [ edit ] Constrained Markov decision processes (CMDPS) are extensions to Markov decision process (MDPs). There are three fundamental differences between MDPs and CMDPs. [ 15 ] There are multiple costs incurred after applying an action instead of one. CMDPs are solved with linear programs only, and dynamic programming does not work. The final policy depends on the starting state. The method of Lagrange multipliers applies to CMDPs. Many Lagrangian-based algorithms have been developed. Natural policy gradient primal-dual method. [ 16 ] There are a number of applications for CMDPs. It has recently been used in motion planning scenarios in robotics. [ 17 ] Continuous-time Markov decision process [ edit ] In discrete-time Markov Decision Processes, decisions are made at discrete time intervals. However, for continuous-time Markov decision processes , decisions can be made at any time the decision maker chooses. In comparison to discrete-time Markov decision processes, continuous-time Markov decision processes can better model the decision-making process for a system that has continuous dynamics , i.e., the system dynamics is defined by ordinary differential equations (ODEs). These kind of applications raise in queueing systems , epidemic processes, and population processes . Like the discrete-time Markov decision processes, in continuous-time Markov decision processes the agent aims at finding the optimal policy which could maximize the expected cumulated reward. The only difference with the standard case stays in the fact that, due to the continuous nature of the time variable, the sum is replaced by an integral: max E π ⁡ [ ∫ 0 ∞ γ t r ( s ( t ) , π ( s ( t ) ) ) d t | s 0 ] {\\displaystyle \\max \\operatorname {E} _{\\pi }\\left[\\left.\\int _{0}^{\\infty }\\gamma ^{t}r(s(t),\\pi (s(t)))\\,dt\\;\\right|s_{0}\\right]} where 0 ≤ γ < 1. {\\displaystyle 0\\leq \\gamma <1.} Discrete space: Linear programming formulation [ edit ] If the state space and action space are finite, we could use linear programming to find the optimal policy, which was one of the earliest approaches applied. Here we only consider the ergodic model, which means our continuous-time MDP becomes an ergodic continuous-time Markov chain under a stationary policy . Under this assumption, although the decision maker can make a decision at any time in the current state, there is no benefit in taking multiple actions. It is better to take an action only at the time when system is transitioning from the current state to another state. Under some conditions, [ 18 ] if our optimal value function V ∗ {\\displaystyle V^{*}} is independent of state i {\\displaystyle i} , we will have the following inequality: g ≥ R ( i , a ) + ∑ j ∈ S q ( j ∣ i , a ) h ( j ) ∀ i ∈ S and a ∈ A ( i ) {\\displaystyle g\\geq R(i,a)+\\sum _{j\\in S}q(j\\mid i,a)h(j)\\quad \\forall i\\in S{\\text{ and }}a\\in A(i)} If there exists a function h"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 4,
    "text": "{\\displaystyle h} , then V ¯ ∗ {\\displaystyle {\\bar {V}}^{*}} will be the smallest g {\\displaystyle g} satisfying the above equation. In order to find V ¯ ∗ {\\displaystyle {\\bar {V}}^{*}} , we could use the following linear programming model: Primal linear program(P-LP) Minimize g s.t g − ∑ j ∈ S q ( j ∣ i , a ) h ( j ) ≥ R ( i , a ) ∀ i ∈ S , a ∈ A ( i ) {\\displaystyle {\\begin{aligned}{\\text{Minimize}}\\quad &g\\\\{\\text{s.t}}\\quad &g-\\sum _{j\\in S}q(j\\mid i,a)h(j)\\geq R(i,a)\\,\\,\\forall i\\in S,\\,a\\in A(i)\\end{aligned}}} Dual linear program(D-LP) Maximize ∑ i ∈ S ∑ a ∈ A ( i ) R ( i , a ) y ( i , a ) s.t. ∑ i ∈ S ∑ a ∈ A ( i ) q ( j ∣ i , a ) y ( i , a ) = 0 ∀ j ∈ S , ∑ i ∈ S ∑ a ∈ A ( i ) y ( i , a ) = 1 , y ( i , a ) ≥ 0 ∀ a ∈ A ( i ) and ∀ i ∈ S {\\displaystyle {\\begin{aligned}{\\text{Maximize}}&\\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y(i,a)\\\\{\\text{s.t.}}&\\sum _{i\\in S}\\sum _{a\\in A(i)}q(j\\mid i,a)y(i,a)=0\\quad \\forall j\\in S,\\\\&\\sum _{i\\in S}\\sum _{a\\in A(i)}y(i,a)=1,\\\\&y(i,a)\\geq 0\\qquad \\forall a\\in A(i){\\text{ and }}\\forall i\\in S\\end{aligned}}} y ( i , a ) {\\displaystyle y(i,a)} is a feasible solution to the D-LP if y ( i , a ) {\\displaystyle y(i,a)} is nonnative and satisfied the constraints in the D-LP problem. A feasible solution y ∗ ( i , a ) {\\displaystyle y^{*}(i,a)} to the D-LP is said to be an optimal solution if ∑ i ∈ S ∑ a ∈ A ( i ) R ( i , a ) y ∗ ( i , a ) ≥ ∑ i ∈ S ∑ a ∈ A ( i ) R ( i , a ) y ( i , a ) {\\displaystyle {\\begin{aligned}\\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y^{*}(i,a)\\geq \\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y(i,a)\\end{aligned}}} for all feasible solution y ( i , a ) {\\displaystyle y(i,a)} to the D-LP. Once we have found the optimal solution y ∗ ( i , a ) {\\displaystyle y^{*}(i,a)} , we can use it to establish the optimal policies. Continuous space: Hamilton–Jacobi–Bellman equation [ edit ] In continuous-time MDP, if the state space and action space are continuous, the optimal criterion could be found by solving Hamilton–Jacobi–Bellman (HJB) partial differential equation . In order to discuss the HJB equation, we need to reformulate our problem V ( s ( 0 ) , 0 ) = max a ( t ) = π ( s ( t ) ) ∫ 0 T r ( s ( t ) , a ( t ) ) d t + D [ s ( T ) ] s.t. d s ( t ) d t = f [ t , s ( t ) , a ( t ) ] {\\displaystyle {\\begin{aligned}V(s(0),0)={}&\\max _{a(t)=\\pi (s(t))}\\int _{0}^{T}r(s(t),a(t))\\,dt+D[s(T)]\\\\{\\text{s.t.}}\\quad &{\\frac {ds(t)}{dt}}=f[t,s(t),a(t)]\\end{aligned}}} D ( ⋅ ) {\\displaystyle D(\\cdot )} is the terminal reward function, s ( t ) {\\displaystyle s(t)} is the system state vector, a ( t ) {\\displaystyle a(t)} is the system control vector we try to find. f ( ⋅ ) {\\displaystyle f(\\cdot )} shows how the state vector changes over time. The Hamilton–Jacobi–Bellman equation is as follows: 0 = max u ( r ( t , s , a ) + ∂ V ( t , s ) ∂ x f ( t , s , a ) ) {\\displaystyle 0=\\max _{u}(r(t,s,a)+{\\frac {\\partial V(t,s)}{\\partial x}}f(t,s,a))} We could solve the equation to find the optimal control a ( t ) {\\displaystyle a(t)} , which could give us the optimal value function V ∗ {\\displaystyle V^{*}} Reinforcement learning [ edit ] Reinforcement learning is an interdisciplinary area of machine learning and optimal control that has, as main objective, finding an approximately optimal policy for MDPs where transition probabilities and rewards are unknown. [ 19 ] Reinforcement learning can solve Markov-Decision processes without explicit specification of the transition probabilities which are instead needed to perform policy iteration. In this setting, transition probabilities and rewards must be learned from experience, i.e. by letting an agent interact with the MDP for a given number of steps. Both on a theoretical and on a practical level, effort is put in maximizing the sample efficiency, i.e. minimimizing the number of samples needed to learn a policy whose performance is ε − {\\displaystyle \\varepsilon -} close to the optimal one (due to the stochastic nature of the process, learning the optimal policy with a finite number of samples is, in general, impossible). Reinforcement Learning for discrete MDPs [ edit ] For the purpose of this section, it is useful to define a further function, which corresponds"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 5,
    "text": "to taking the action a {\\displaystyle a} and then continuing optimally (or according to whatever policy one currently has): Q ( s , a ) = ∑ s ′ P a ( s , s ′ ) ( R a ( s , s ′ ) + γ V ( s ′ ) ) . {\\displaystyle \\ Q(s,a)=\\sum _{s'}P_{a}(s,s')(R_{a}(s,s')+\\gamma V(s')).\\ } While this function is also unknown, experience during learning is based on ( s , a ) {\\displaystyle (s,a)} pairs (together with the outcome s ′ {\\displaystyle s'} ; that is, \"I was in state s {\\displaystyle s} and I tried doing a {\\displaystyle a} and s ′ {\\displaystyle s'} happened\"). Thus, one has an array Q {\\displaystyle Q} and uses experience to update it directly. This is known as Q-learning . Another application of MDP process in machine learning theory is called learning automata. This is also one type of reinforcement learning if the environment is stochastic. The first detail learning automata paper is surveyed by Narendra and Thathachar (1974), which were originally described explicitly as finite-state automata . [ 20 ] Similar to reinforcement learning, a learning automata algorithm also has the advantage of solving the problem when probability or rewards are unknown. The difference between learning automata and Q-learning is that the former technique omits the memory of Q-values, but updates the action probability directly to find the learning result. Learning automata is a learning scheme with a rigorous proof of convergence. [ 21 ] In learning automata theory, a stochastic automaton consists of: a set x of possible inputs, a set Φ = { Φ 1 , ..., Φ s } of possible internal states, a set α = { α 1 , ..., α r } of possible outputs, or actions, with r ≤ s , an initial state probability vector p (0) = ≪ p 1 (0), ..., p s (0) ≫, a computable function A which after each time step t generates p ( t + 1) from p ( t ), the current input, and the current state, and a function G : Φ → α which generates the output at each time step. The states of such an automaton correspond to the states of a \"discrete-state discrete-parameter Markov process \". At each time step t = 0,1,2,3,..., the automaton reads an input from its environment, updates P( t ) to P( t + 1) by A , randomly chooses a successor state according to the probabilities P( t + 1) and outputs the corresponding action. The automaton's environment, in turn, reads the action and sends the next input to the automaton. [ 21 ] Category theoretic interpretation [ edit ] Other than the rewards, a Markov decision process ( S , A , P ) {\\displaystyle (S,A,P)} can be understood in terms of Category theory . Namely, let A {\\displaystyle {\\mathcal {A}}} denote the free monoid with generating set A . Let Dist denote the Kleisli category of the Giry monad . Then a functor A → D i s t {\\displaystyle {\\mathcal {A}}\\to \\mathbf {Dist} } encodes both the set S of states and the probability function P . In this way, Markov decision processes could be generalized from monoids (categories with one object) to arbitrary categories. One can call the result ( C , F : C → D i s t ) {\\displaystyle ({\\mathcal {C}},F:{\\mathcal {C}}\\to \\mathbf {Dist} )} a context-dependent Markov decision process , because moving from one object to another in C {\\displaystyle {\\mathcal {C}}} changes the set of available actions and the set of possible states. [ citation needed ] Alternative notations [ edit ] The terminology and notation for MDPs are not entirely settled. There are two main streams — one focuses on maximization problems from contexts like economics, using the terms action, reward, value, and calling the discount factor β or γ , while the other focuses on minimization problems from engineering and navigation [ citation needed ] , using the terms control, cost, cost-to-go, and calling the discount factor α . In addition, the notation for the transition probability varies. in this article alternative comment action a control u reward R cost g g is the negative of R value V cost-to-go J J is the negative of V policy π policy μ discounting factor γ discounting factor α transition probability P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} transition probability p s s ′ ( a ) {\\displaystyle p_{ss'}(a)} In addition, transition probability is sometimes written Pr ( s , a , s ′ ) {\\displaystyle \\Pr(s,a,s')} , Pr ( s ′ ∣ s , a ) {\\displaystyle \\Pr(s'\\mid s,a)} or, rarely, p s ′ s ( a ) . {\\displaystyle p_{s's}(a).} ^ Puterman, Martin L. (1994). Markov"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 6,
    "text": "decision processes: discrete stochastic dynamic programming . Wiley series in probability and mathematical statistics. Applied probability and statistics section. New York: Wiley. ISBN 978-0-471-61977-2 . ^ Schneider, S.; Wagner, D. H. (1957-02-26). \"Error detection in redundant systems\" . Papers presented at the February 26-28, 1957, western joint computer conference: Techniques for reliability on - IRE-AIEE-ACM '57 (Western) . New York, NY, USA: Association for Computing Machinery. pp. 115– 121. doi : 10.1145/1455567.1455587 . ISBN 978-1-4503-7861-1 . ^ Bellman, Richard (1958-09-01). \"Dynamic programming and stochastic control processes\" . Information and Control . 1 (3): 228– 239. doi : 10.1016/S0019-9958(58)80003-0 . ISSN 0019-9958 . ^ a b Sutton, Richard S.; Barto, Andrew G. (2018). Reinforcement learning: an introduction . Adaptive computation and machine learning series (2nd ed.). Cambridge, Massachusetts: The MIT Press. ISBN 978-0-262-03924-6 . ^ Kearns, Michael; Mansour, Yishay; Ng, Andrew (2002). \"A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes\" . Machine Learning . 49 ( 193– 208): 193– 208. doi : 10.1023/A:1017932429737 . ^ Wrobel, A. (1984). \"On Markovian decision models with a finite skeleton\". Zeitschrift für Operations Research . 28 (1): 17– 27. doi : 10.1007/bf01919083 . S2CID 2545336 . ^ Reinforcement Learning: Theory and Python Implementation . Beijing: China Machine Press. 2019. p. 44. ISBN 9787111631774 . ^ Shapley, Lloyd (1953). \"Stochastic Games\" . Proceedings of the National Academy of Sciences of the United States of America . 39 (10): 1095– 1100. Bibcode : 1953PNAS...39.1095S . doi : 10.1073/pnas.39.10.1095 . PMC 1063912 . PMID 16589380 . ^ Kallenberg, Lodewijk (2002). \"Finite state and action MDPs\". In Feinberg, Eugene A. ; Shwartz, Adam (eds.). Handbook of Markov decision processes: methods and applications . Springer. ISBN 978-0-7923-7459-6 . ^ Howard 2002, \"Comments on the Origin and Application of Markov Decision Processes\" ^ Puterman, M. L.; Shin, M. C. (1978). \"Modified Policy Iteration Algorithms for Discounted Markov Decision Problems\". Management Science . 24 (11): 1127– 1137. doi : 10.1287/mnsc.24.11.1127 . ^ van Nunen, J.A. E. E (1976). \"A set of successive approximation methods for discounted Markovian decision problems\". Zeitschrift für Operations Research . 20 (5): 203– 208. doi : 10.1007/bf01920264 . S2CID 5167748 . ^ Papadimitriou, Christos ; Tsitsiklis, John (1987). \"The Complexity of Markov Decision Processes\" . Mathematics of Operations Research . 12 (3): 441– 450. doi : 10.1287/moor.12.3.441 . hdl : 1721.1/2893 . Retrieved November 2, 2023 . ^ Kearns, Michael; Mansour, Yishay; Ng, Andrew (November 2002). \"A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes\" . Machine Learning . 49 (2/3): 193– 208. doi : 10.1023/A:1017932429737 . ^ Altman, Eitan (1999). Constrained Markov decision processes . Vol. 7. CRC Press. ^ Ding, Dongsheng; Zhang, Kaiqing; Jovanovic, Mihailo; Basar, Tamer (2020). Natural policy gradient primal-dual method for constrained Markov decision processes . Advances in Neural Information Processing Systems. ^ Feyzabadi, S.; Carpin, S. (18–22 Aug 2014). \"Risk-aware path planning using hierarchical constrained Markov Decision Processes\" . Automation Science and Engineering (CASE) . IEEE International Conference. pp. 297, 303. ^ Continuous-Time Markov Decision Processes . Stochastic Modelling and Applied Probability. Vol. 62. 2009. doi : 10.1007/978-3-642-02547-1 . ISBN 978-3-642-02546-4 . ^ Shoham, Y.; Powers, R.; Grenager, T. (2003). \"Multi-agent reinforcement learning: a critical survey\" (PDF) . Technical Report, Stanford University : 1– 13 . Retrieved 2018-12-12 . ^ Narendra, K. S. ; Thathachar, M. A. L. (1974). \"Learning Automata – A Survey\". IEEE Transactions on Systems, Man, and Cybernetics . SMC-4 (4): 323– 334. CiteSeerX 10.1.1.295.2280 . doi : 10.1109/TSMC.1974.5408453 . ISSN 0018-9472 . ^ a b Narendra, Kumpati S. ; Thathachar, Mandayam A. L. (1989). Learning automata: An introduction . Prentice Hall. ISBN 9780134855585 . Bellman., R. E. (2003) [1957]. Dynamic Programming (Dover paperback ed.). Princeton, NJ: Princeton University Press. ISBN 978-0-486-42809-3 . Bertsekas, D. (1995). Dynamic Programming and Optimal Control . Vol. 2. MA: Athena. Derman, C. (1970). Finite state Markovian decision processes . Academic Press. Feinberg, E.A.; Shwartz, A., eds. (2002). Handbook of Markov Decision Processes . Boston, MA: Kluwer. ISBN 9781461508052 . Guo, X.; Hernández-Lerma, O. (2009). Continuous-Time Markov Decision Processes . Stochastic Modelling and Applied Probability. Springer. ISBN 9783642025464 . Meyn, S. P. (2007). Control Techniques for Complex Networks . Cambridge University Press. ISBN 978-0-521-88441-9 . Archived from the original on 19 June 2010. Appendix contains abridged \"Meyn & Tweedie\" . Archived from the original on 18 December 2012. Puterman., M. L. (1994). Markov Decision Processes . Wiley. Ross, S. M. (1983). Introduction to stochastic dynamic programming (PDF) . Academic press. Sutton, R. S.; Barto, A. G. (2017). Reinforcement Learning: An Introduction . Cambridge, MA: The MIT Press. Tijms., H.C. (2003). A First Course in Stochastic Models . Wiley. ISBN 9780470864289 ."
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Value_function",
    "title": "Value function - Wikipedia",
    "chunk": 0,
    "text": "The value function of an optimization problem gives the value attained by the objective function at a solution, while only depending on the parameters of the problem. [ 1 ] [ 2 ] In a controlled dynamical system , the value function represents the optimal payoff of the system over the interval [t, t 1 ] when started at the time- t state variable x(t)=x . [ 3 ] If the objective function represents some cost that is to be minimized, the value function can be interpreted as the cost to finish the optimal program, and is thus referred to as \"cost-to-go function.\" [ 4 ] [ 5 ] In an economic context, where the objective function usually represents utility , the value function is conceptually equivalent to the indirect utility function . [ 6 ] [ 7 ] In a problem of optimal control , the value function is defined as the supremum of the objective function taken over the set of admissible controls. Given ( t 0 , x 0 ) ∈ [ 0 , t 1 ] × R d {\\displaystyle (t_{0},x_{0})\\in [0,t_{1}]\\times \\mathbb {R} ^{d}} , a typical optimal control problem is to maximize J ( t 0 , x 0 ; u ) = ∫ t 0 t 1 I ( t , x ( t ) , u ( t ) ) d t + ϕ ( x ( t 1 ) ) {\\displaystyle {\\text{maximize}}\\quad J(t_{0},x_{0};u)=\\int _{t_{0}}^{t_{1}}I(t,x(t),u(t))\\,\\mathrm {d} t+\\phi (x(t_{1}))} subject to d x ( t ) d t = f ( t , x ( t ) , u ( t ) ) {\\displaystyle {\\frac {\\mathrm {d} x(t)}{\\mathrm {d} t}}=f(t,x(t),u(t))} with initial state variable x ( t 0 ) = x 0 {\\displaystyle x(t_{0})=x_{0}} . [ 8 ] The objective function J ( t 0 , x 0 ; u ) {\\displaystyle J(t_{0},x_{0};u)} is to be maximized over all admissible controls u ∈ U [ t 0 , t 1 ] {\\displaystyle u\\in U[t_{0},t_{1}]} , where u {\\displaystyle u} is a Lebesgue measurable function from [ t 0 , t 1 ] {\\displaystyle [t_{0},t_{1}]} to some prescribed arbitrary set in R m {\\displaystyle \\mathbb {R} ^{m}} . The value function is then defined as V ( t , x ( t ) ) = max u ∈ U ∫ t t 1 I ( τ , x ( τ ) , u ( τ ) ) d τ + ϕ ( x ( t 1 ) ) {\\displaystyle V(t,x(t))=\\max _{u\\in U}\\int _{t}^{t_{1}}I(\\tau ,x(\\tau ),u(\\tau ))\\,\\mathrm {d} \\tau +\\phi (x(t_{1}))} with V ( t 1 , x ( t 1 ) ) = ϕ ( x ( t 1 ) ) {\\displaystyle V(t_{1},x(t_{1}))=\\phi (x(t_{1}))} , where ϕ ( x ( t 1 ) ) {\\displaystyle \\phi (x(t_{1}))} is the \"scrap value\". If the optimal pair of control and state trajectories is ( x ∗ , u ∗ ) {\\displaystyle (x^{\\ast },u^{\\ast })} , then V ( t 0 , x 0 ) = J ( t 0 , x 0 ; u ∗ ) {\\displaystyle V(t_{0},x_{0})=J(t_{0},x_{0};u^{\\ast })} . The function h {\\displaystyle h} that gives the optimal control u ∗ {\\displaystyle u^{\\ast }} based on the current state x {\\displaystyle x} is called a feedback control policy, [ 4 ] or simply a policy function. [ 9 ] Bellman's principle of optimality roughly states that any optimal policy at time t {\\displaystyle t} , t 0 ≤ t ≤ t 1 {\\displaystyle t_{0}\\leq t\\leq t_{1}} taking the current state x ( t ) {\\displaystyle x(t)} as \"new\" initial condition must be optimal for the remaining problem. If the value function happens to be continuously differentiable , [ 10 ] this gives rise to an important partial differential equation known as Hamilton–Jacobi–Bellman equation , − ∂ V ( t , x ) ∂ t = max u { I ( t , x , u ) + ∂ V ( t , x ) ∂ x f ( t , x , u ) } {\\displaystyle -{\\frac {\\partial V(t,x)}{\\partial t}}=\\max _{u}\\left\\{I(t,x,u)+{\\frac {\\partial V(t,x)}{\\partial x}}f(t,x,u)\\right\\}} where the maximand on the right-hand side can also be re-written as the Hamiltonian , H ( t , x , u , λ ) = I ( t , x , u ) + λ ( t ) f ( t , x , u ) {\\displaystyle H\\left(t,x,u,\\lambda \\right)=I(t,x,u)+\\lambda (t)f(t,x,u)} , as − ∂ V ( t , x ) ∂ t = max u H ( t , x , u , λ ) {\\displaystyle -{\\frac {\\partial V(t,x)}{\\partial t}}=\\max _{u}H(t,x,u,\\lambda )} with ∂ V ( t , x ) / ∂ x = λ ( t ) {\\displaystyle \\partial V(t,x)/\\partial x=\\lambda (t)} playing the role of the costate variables . [ 11 ] Given this definition, we further have"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Value_function",
    "title": "Value function - Wikipedia",
    "chunk": 1,
    "text": "d λ ( t ) / d t = ∂ 2 V ( t , x ) / ∂ x ∂ t + ∂ 2 V ( t , x ) / ∂ x 2 ⋅ f ( x ) {\\displaystyle \\mathrm {d} \\lambda (t)/\\mathrm {d} t=\\partial ^{2}V(t,x)/\\partial x\\partial t+\\partial ^{2}V(t,x)/\\partial x^{2}\\cdot f(x)} , and after differentiating both sides of the HJB equation with respect to x {\\displaystyle x} , − ∂ 2 V ( t , x ) ∂ t ∂ x = ∂ I ∂ x + ∂ 2 V ( t , x ) ∂ x 2 f ( x ) + ∂ V ( t , x ) ∂ x ∂ f ( x ) ∂ x {\\displaystyle -{\\frac {\\partial ^{2}V(t,x)}{\\partial t\\partial x}}={\\frac {\\partial I}{\\partial x}}+{\\frac {\\partial ^{2}V(t,x)}{\\partial x^{2}}}f(x)+{\\frac {\\partial V(t,x)}{\\partial x}}{\\frac {\\partial f(x)}{\\partial x}}} which after replacing the appropriate terms recovers the costate equation − λ ˙ ( t ) = ∂ I ∂ x + λ ( t ) ∂ f ( x ) ∂ x ⏟ = ∂ H ∂ x {\\displaystyle -{\\dot {\\lambda }}(t)=\\underbrace {{\\frac {\\partial I}{\\partial x}}+\\lambda (t){\\frac {\\partial f(x)}{\\partial x}}} _{={\\frac {\\partial H}{\\partial x}}}} where λ ˙ ( t ) {\\displaystyle {\\dot {\\lambda }}(t)} is Newton notation for the derivative with respect to time. [ 12 ] The value function is the unique viscosity solution to the Hamilton–Jacobi–Bellman equation. [ 13 ] In an online closed-loop approximate optimal control, the value function is also a Lyapunov function that establishes global asymptotic stability of the closed-loop system. [ 14 ] ^ Fleming, Wendell H. ; Rishel, Raymond W. (1975). Deterministic and Stochastic Optimal Control . New York: Springer. pp. 81– 83. ISBN 0-387-90155-8 . ^ Caputo, Michael R. (2005). Foundations of Dynamic Economic Analysis : Optimal Control Theory and Applications . New York: Cambridge University Press. p. 185. ISBN 0-521-60368-4 . ^ Weber, Thomas A. (2011). Optimal Control Theory : with Applications in Economics . Cambridge: The MIT Press. p. 82. ISBN 978-0-262-01573-8 . ^ a b Bertsekas, Dimitri P.; Tsitsiklis, John N. (1996). Neuro-Dynamic Programming . Belmont: Athena Scientific. p. 2. ISBN 1-886529-10-8 . ^ \"EE365: Dynamic Programming\" (PDF) . ^ Mas-Colell, Andreu ; Whinston, Michael D. ; Green, Jerry R. (1995). Microeconomic Theory . New York: Oxford University Press. p. 964. ISBN 0-19-507340-1 . ^ Corbae, Dean; Stinchcombe, Maxwell B.; Zeman, Juraj (2009). An Introduction to Mathematical Analysis for Economic Theory and Econometrics . Princeton University Press. p. 145. ISBN 978-0-691-11867-3 . ^ Kamien, Morton I. ; Schwartz, Nancy L. (1991). Dynamic Optimization : The Calculus of Variations and Optimal Control in Economics and Management (2nd ed.). Amsterdam: North-Holland. p. 259. ISBN 0-444-01609-0 . ^ Ljungqvist, Lars ; Sargent, Thomas J. (2018). Recursive Macroeconomic Theory (Fourth ed.). Cambridge: MIT Press. p. 106. ISBN 978-0-262-03866-9 . ^ Benveniste and Scheinkman established sufficient conditions for the differentiability of the value function, which in turn allows an application of the envelope theorem , see Benveniste, L. M.; Scheinkman, J. A. (1979). \"On the Differentiability of the Value Function in Dynamic Models of Economics\". Econometrica . 47 (3): 727– 732. doi : 10.2307/1910417 . JSTOR 1910417 . Also see Seierstad, Atle (1982). \"Differentiability Properties of the Optimal Value Function in Control Theory\". Journal of Economic Dynamics and Control . 4 : 303– 310. doi : 10.1016/0165-1889(82)90019-7 . ^ Kirk, Donald E. (1970). Optimal Control Theory . Englewood Cliffs, NJ: Prentice-Hall. p. 88. ISBN 0-13-638098-0 . ^ Zhou, X. Y. (1990). \"Maximum Principle, Dynamic Programming, and their Connection in Deterministic Control\". Journal of Optimization Theory and Applications . 65 (2): 363– 373. doi : 10.1007/BF01102352 . S2CID 122333807 . ^ Theorem 10.1 in Bressan, Alberto (2019). \"Viscosity Solutions of Hamilton-Jacobi Equations and Optimal Control Problems\" (PDF) . Lecture Notes . ^ Kamalapurkar, Rushikesh; Walters, Patrick; Rosenfeld, Joel; Dixon, Warren (2018). \"Optimal Control and Lyapunov Stability\" . Reinforcement Learning for Optimal Feedback Control: A Lyapunov-Based Approach . Berlin: Springer. pp. 26– 27. ISBN 978-3-319-78383-3 ."
  },
  {
    "type": "doc",
    "url": "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html",
    "title": "Part 1: Key Concepts in RL",
    "chunk": 0,
    "text": "Welcome to our introduction to reinforcement learning! Here, we aim to acquaint you with In a nutshell, RL is the study of agents and how they learn by trial and error. It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future. Agent-environment interaction loop. The main characters of RL are the agent and the environment . The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment changes when the agent acts on it, but may also change on its own. The agent also perceives a reward signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward, called return . Reinforcement learning methods are ways that the agent can learn behaviors to achieve its goal. To talk more specifically what RL does, we need to introduce additional terminology. We need to talk about states and observations, action spaces, policies, trajectories, different formulations of return, the RL optimization problem, and value functions. States and Observations A state is a complete description of the state of the world. There is no information about the world which is hidden from the state. An observation is a partial description of a state, which may omit information. In deep RL, we almost always represent states and observations by a real-valued vector, matrix, or higher-order tensor . For instance, a visual observation could be represented by the RGB matrix of its pixel values; the state of a robot might be represented by its joint angles and velocities. When the agent is able to observe the complete state of the environment, we say that the environment is fully observed . When the agent can only see a partial observation, we say that the environment is partially observed . You Should Know Reinforcement learning notation sometimes puts the symbol for state, , in places where it would be technically more appropriate to write the symbol for observation, . Specifically, this happens when talking about how the agent decides an action: we often signal in notation that the action is conditioned on the state, when in practice, the action is conditioned on the observation because the agent does not have access to the state. In our guide, we’ll follow standard conventions for notation, but it should be clear from context which is meant. If something is unclear, though, please raise an issue! Our goal is to teach, not to confuse. Action Spaces Different environments allow different kinds of actions. The set of all valid actions in a given environment is often called the action space . Some environments, like Atari and Go, have discrete action spaces , where only a finite number of moves are available to the agent. Other environments, like where the agent controls a robot in a physical world, have continuous action spaces . In continuous spaces, actions are real-valued vectors. This distinction has some quite-profound consequences for methods in deep RL. Some families of algorithms can only be directly applied in one case, and would have to be substantially reworked for the other. Policies A policy is a rule used by an agent to decide what actions to take. It can be deterministic, in which case it is usually denoted by : or it may be stochastic, in which case it is usually denoted by : Because the policy is essentially the agent’s brain, it’s not uncommon to substitute the word “policy” for “agent”, eg saying “The policy is trying to maximize reward.” In deep RL, we deal with parameterized policies : policies whose outputs are computable functions that depend on a set of parameters (eg the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm. We often denote the parameters of such a policy by or , and then write this as a subscript on the policy symbol to highlight the connection: Deterministic Policies Example: Deterministic Policies. Here is a code snippet for building a simple deterministic policy for a continuous action space in PyTorch, using the torch.nn package: pi_net = nn . Sequential ( nn . Linear ( obs_dim , 64 ), nn . Tanh (), nn . Linear ( 64 , 64 ), nn . Tanh (), nn . Linear ( 64 , act_dim ) ) This builds a multi-layer perceptron (MLP) network with two hidden layers of size 64 and activation functions. If obs is a Numpy array containing a batch of observations, pi_net can be"
  },
  {
    "type": "doc",
    "url": "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html",
    "title": "Part 1: Key Concepts in RL",
    "chunk": 1,
    "text": "used to obtain a batch of actions as follows: obs_tensor = torch . as_tensor ( obs , dtype = torch . float32 ) actions = pi_net ( obs_tensor ) You Should Know Don’t worry about it if this neural network stuff is unfamiliar to you—this tutorial will focus on RL, and not on the neural network side of things. So you can skip this example and come back to it later. But we figured that if you already knew, it could be helpful. Stochastic Policies The two most common kinds of stochastic policies in deep RL are categorical policies and diagonal Gaussian policies . Categorical policies can be used in discrete action spaces, while diagonal Gaussian policies are used in continuous action spaces. Two key computations are centrally important for using and training stochastic policies: sampling actions from the policy, and computing log likelihoods of particular actions, . In what follows, we’ll describe how to do these for both categorical and diagonal Gaussian policies. Categorical Policies A categorical policy is like a classifier over discrete actions. You build the neural network for a categorical policy the same way you would for a classifier: the input is the observation, followed by some number of layers (possibly convolutional or densely-connected, depending on the kind of input), and then you have one final linear layer that gives you logits for each action, followed by a softmax to convert the logits into probabilities. Sampling. Given the probabilities for each action, frameworks like PyTorch and Tensorflow have built-in tools for sampling. For example, see the documentation for Categorical distributions in PyTorch , torch.multinomial , tf.distributions.Categorical , or tf.multinomial . Log-Likelihood. Denote the last layer of probabilities as . It is a vector with however many entries as there are actions, so we can treat the actions as indices for the vector. The log likelihood for an action can then be obtained by indexing into the vector: Diagonal Gaussian Policies A multivariate Gaussian distribution (or multivariate normal distribution, if you prefer) is described by a mean vector, , and a covariance matrix, . A diagonal Gaussian distribution is a special case where the covariance matrix only has entries on the diagonal. As a result, we can represent it by a vector. A diagonal Gaussian policy always has a neural network that maps from observations to mean actions, . There are two different ways that the covariance matrix is typically represented. The first way: There is a single vector of log standard deviations, , which is not a function of state: the are standalone parameters. (You Should Know: our implementations of VPG, TRPO, and PPO do it this way.) The second way: There is a neural network that maps from states to log standard deviations, . It may optionally share some layers with the mean network. Note that in both cases we output log standard deviations instead of standard deviations directly. This is because log stds are free to take on any values in , while stds must be nonnegative. It’s easier to train parameters if you don’t have to enforce those kinds of constraints. The standard deviations can be obtained immediately from the log standard deviations by exponentiating them, so we do not lose anything by representing them this way. Sampling. Given the mean action and standard deviation , and a vector of noise from a spherical Gaussian ( ), an action sample can be computed with where denotes the elementwise product of two vectors. Standard frameworks have built-in ways to generate the noise vectors, such as torch.normal or tf.random_normal . Alternatively, you can build distribution objects, eg through torch.distributions.Normal or tf.distributions.Normal , and use them to generate samples. (The advantage of the latter approach is that those objects can also calculate log-likelihoods for you.) Log-Likelihood. The log-likelihood of a -dimensional action , for a diagonal Gaussian with mean and standard deviation , is given by Trajectories A trajectory is a sequence of states and actions in the world, The very first state of the world, , is randomly sampled from the start-state distribution , sometimes denoted by : State transitions (what happens to the world between the state at time , , and the state at , ), are governed by the natural laws of the environment, and depend on only the most recent action, . They can be either deterministic, or stochastic, Actions come from an agent according to its policy. You Should Know Trajectories are also frequently called episodes or rollouts . Reward and Return The reward function is critically important in reinforcement learning. It depends on the current state of the world, the action just taken, and the next state of the world: although frequently this is simplified to just a dependence on the current state, , or state-action pair . The"
  },
  {
    "type": "doc",
    "url": "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html",
    "title": "Part 1: Key Concepts in RL",
    "chunk": 2,
    "text": "goal of the agent is to maximize some notion of cumulative reward over a trajectory, but this actually can mean a few things. We’ll notate all of these cases with , and it will either be clear from context which case we mean, or it won’t matter (because the same equations will apply to all cases). One kind of return is the finite-horizon undiscounted return , which is just the sum of rewards obtained in a fixed window of steps: Another kind of return is the infinite-horizon discounted return , which is the sum of all rewards ever obtained by the agent, but discounted by how far off in the future they’re obtained. This formulation of reward includes a discount factor : Why would we ever want a discount factor, though? Don’t we just want to get all rewards? We do, but the discount factor is both intuitively appealing and mathematically convenient. On an intuitive level: cash now is better than cash later. Mathematically: an infinite-horizon sum of rewards may not converge to a finite value, and is hard to deal with in equations. But with a discount factor and under reasonable conditions, the infinite sum converges. You Should Know While the line between these two formulations of return are quite stark in RL formalism, deep RL practice tends to blur the line a fair bit—for instance, we frequently set up algorithms to optimize the undiscounted return, but use discount factors in estimating value functions . The RL Problem Whatever the choice of return measure (whether infinite-horizon discounted, or finite-horizon undiscounted), and whatever the choice of policy, the goal in RL is to select a policy which maximizes expected return when the agent acts according to it. To talk about expected return, we first have to talk about probability distributions over trajectories. Let’s suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a -step trajectory is: The expected return (for whichever measure), denoted by , is then: The central optimization problem in RL can then be expressed by with being the optimal policy . Value Functions It’s often useful to know the value of a state, or state-action pair. By value, we mean the expected return if you start in that state or state-action pair, and then act according to a particular policy forever after. Value functions are used, one way or another, in almost every RL algorithm. There are four main functions of note here. The On-Policy Value Function , , which gives the expected return if you start in state and always act according to policy : The On-Policy Action-Value Function , , which gives the expected return if you start in state , take an arbitrary action (which may not have come from the policy), and then forever after act according to policy : The Optimal Value Function , , which gives the expected return if you start in state and always act according to the optimal policy in the environment: The Optimal Action-Value Function , , which gives the expected return if you start in state , take an arbitrary action , and then forever after act according to the optimal policy in the environment: You Should Know When we talk about value functions, if we do not make reference to time-dependence, we only mean expected infinite-horizon discounted return . Value functions for finite-horizon undiscounted return would need to accept time as an argument. Can you think about why? Hint: what happens when time’s up? You Should Know There are two key connections between the value function and the action-value function that come up pretty often: and These relations follow pretty directly from the definitions just given: can you prove them? The Optimal Q-Function and the Optimal Action There is an important connection between the optimal action-value function and the action selected by the optimal policy. By definition, gives the expected return for starting in state , taking (arbitrary) action , and then acting according to the optimal policy forever after. The optimal policy in will select whichever action maximizes the expected return from starting in . As a result, if we have , we can directly obtain the optimal action, , via Note: there may be multiple actions which maximize , in which case, all of them are optimal, and the optimal policy may randomly select any of them. But there is always an optimal policy which deterministically selects an action. Bellman Equations All four of the value functions obey special self-consistency equations called Bellman equations . The basic idea behind the Bellman equations is this: The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next. The Bellman equations for the on-policy value"
  },
  {
    "type": "doc",
    "url": "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html",
    "title": "Part 1: Key Concepts in RL",
    "chunk": 3,
    "text": "functions are where is shorthand for , indicating that the next state is sampled from the environment’s transition rules; is shorthand for ; and is shorthand for . The Bellman equations for the optimal value functions are The crucial difference between the Bellman equations for the on-policy value functions and the optimal value functions, is the absence or presence of the over actions. Its inclusion reflects the fact that whenever the agent gets to choose its action, in order to act optimally, it has to pick whichever action leads to the highest value. You Should Know The term “Bellman backup” comes up quite frequently in the RL literature. The Bellman backup for a state, or state-action pair, is the right-hand side of the Bellman equation: the reward-plus-next-value. Advantage Functions Sometimes in RL, we don’t need to describe how good an action is in an absolute sense, but only how much better it is than others on average. That is to say, we want to know the relative advantage of that action. We make this concept precise with the advantage function. The advantage function corresponding to a policy describes how much better it is to take a specific action in state , over randomly selecting an action according to , assuming you act according to forever after. Mathematically, the advantage function is defined by You Should Know We’ll discuss this more later, but the advantage function is crucially important to policy gradient methods."
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 0,
    "text": "Mathematical model for sequential decision making under uncertainty Markov decision process ( MDP ), also called a stochastic dynamic program or stochastic control problem, is a model for sequential decision making when outcomes are uncertain. [ 1 ] Originating from operations research in the 1950s, [ 2 ] [ 3 ] MDPs have since gained recognition in a variety of fields, including ecology , economics , healthcare , telecommunications and reinforcement learning . [ 4 ] Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. In this framework, the interaction is characterized by states, actions, and rewards. The MDP framework is designed to provide a simplified representation of key elements of artificial intelligence challenges. These elements encompass the understanding of cause and effect , the management of uncertainty and nondeterminism, and the pursuit of explicit goals. [ 4 ] The name comes from its connection to Markov chains , a concept developed by the Russian mathematician Andrey Markov . The \"Markov\" in \"Markov decision process\" refers to the underlying structure of state transitions that still follow the Markov property . The process is called a \"decision process\" because it involves making decisions that influence these state transitions, extending the concept of a Markov chain into the realm of decision-making under uncertainty. Example of a simple MDP with three states (green circles) and two actions (orange circles), with two rewards (orange arrows) A Markov decision process is a 4- tuple ( S , A , P a , R a ) {\\displaystyle (S,A,P_{a},R_{a})} , where: S {\\displaystyle S} is a set of states called the state space . The state space may be discrete or continuous, like the set of real numbers . A {\\displaystyle A} is a set of actions called the action space (alternatively, A s {\\displaystyle A_{s}} is the set of actions available from state s {\\displaystyle s} ). As for state, this set may be discrete or continuous. P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} is, on an intuitive level, the probability that action a {\\displaystyle a} in state s {\\displaystyle s} at time t {\\displaystyle t} will lead to state s ′ {\\displaystyle s'} at time t + 1 {\\displaystyle t+1} . In general, this probability transition is defined to satisfy Pr ( s t + 1 ∈ S ′ ∣ s t = s , a t = a ) = ∫ S ′ P a ( s , s ′ ) d s ′ , {\\displaystyle \\Pr(s_{t+1}\\in S'\\mid s_{t}=s,a_{t}=a)=\\int _{S'}P_{a}(s,s')ds',} for every S ′ ⊆ S {\\displaystyle S'\\subseteq S} measurable. In case the state space is discrete, the integral is intended with respect to the counting measure , so that the latter simplifies as P a ( s , s ′ ) = Pr ( s t + 1 = s ′ ∣ s t = s , a t = a ) {\\displaystyle P_{a}(s,s')=\\Pr(s_{t+1}=s'\\mid s_{t}=s,a_{t}=a)} ; In case S ⊆ R d {\\displaystyle S\\subseteq \\mathbb {R} ^{d}} , the integral is usually intended with respect to the Lebesgue measure . R a ( s , s ′ ) {\\displaystyle R_{a}(s,s')} is the immediate reward (or expected immediate reward) received after transitioning from state s {\\displaystyle s} to state s ′ {\\displaystyle s'} , due to action a {\\displaystyle a} . A policy function π {\\displaystyle \\pi } is a (potentially probabilistic) mapping from state space ( S {\\displaystyle S} ) to action space ( A {\\displaystyle A} ). Optimization objective [ edit ] The goal in a Markov decision process is to find a good \"policy\" for the decision maker: a function π {\\displaystyle \\pi } that specifies the action π ( s ) {\\displaystyle \\pi (s)} that the decision maker will choose when in state s {\\displaystyle s} . Once a Markov decision process is combined with a policy in this way, this fixes the action for each state and the resulting combination behaves like a Markov chain (since the action chosen in state s {\\displaystyle s} is completely determined by π ( s ) {\\displaystyle \\pi (s)} ). The objective is to choose a policy π {\\displaystyle \\pi } that will maximize some cumulative function of the random rewards, typically the expected discounted sum over a potentially infinite horizon: E [ ∑ t = 0 ∞ γ t R a t ( s t , s t + 1 ) ] {\\displaystyle E\\left[\\sum _{t=0}^{\\infty }{\\gamma ^{t}R_{a_{t}}(s_{t},s_{t+1})}\\right]} (where we choose a t = π ( s t ) {\\displaystyle a_{t}=\\pi (s_{t})} , i.e. actions given by the policy). And the expectation is taken over s t + 1 ∼ P a t ( s t , s t + 1 ) {\\displaystyle s_{t+1}\\sim P_{a_{t}}(s_{t},s_{t+1})} where γ {\\displaystyle \\ \\gamma \\ }"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 1,
    "text": "is the discount factor satisfying 0 ≤ γ ≤ 1 {\\displaystyle 0\\leq \\ \\gamma \\ \\leq \\ 1} , which is usually close to 1 {\\displaystyle 1} (for example, γ = 1 / ( 1 + r ) {\\displaystyle \\gamma =1/(1+r)} for some discount rate r {\\displaystyle r} ). A lower discount factor makes the decision maker more short-sighted, in that it comparatively disregards the effect that following its current policy has at times lying further in the future. Another possible, but strictly related, objective that is commonly used is the H − {\\displaystyle H-} step return. This time, instead of using a discount factor γ {\\displaystyle \\ \\gamma \\ } , the agent is interested only in the first H {\\displaystyle H} steps of the process, with each reward having the same weight. E [ ∑ t = 0 H − 1 R a t ( s t , s t + 1 ) ] {\\displaystyle E\\left[\\sum _{t=0}^{H-1}{R_{a_{t}}(s_{t},s_{t+1})}\\right]} (where we choose a t = π ( s t ) {\\displaystyle a_{t}=\\pi (s_{t})} , i.e. actions given by the policy). And the expectation is taken over s t + 1 ∼ P a t ( s t , s t + 1 ) {\\displaystyle s_{t+1}\\sim P_{a_{t}}(s_{t},s_{t+1})} where H {\\displaystyle \\ H\\ } is the time horizon. Compared to the previous objective, the latter one is more used in Learning Theory . A policy that maximizes the function above is called an optimal policy and is usually denoted π ∗ {\\displaystyle \\pi ^{*}} . A particular MDP may have multiple distinct optimal policies. Because of the Markov property , it can be shown that the optimal policy is a function of the current state, as assumed above. In many cases, it is difficult to represent the transition probability distributions, P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} , explicitly. In such cases, a simulator can be used to model the MDP implicitly by providing samples from the transition distributions. One common form of implicit MDP model is an episodic environment simulator that can be started from an initial state and yields a subsequent state and reward every time it receives an action input. In this manner, trajectories of states, actions, and rewards, often called episodes may be produced. Another form of simulator is a generative model , a single step simulator that can generate samples of the next state and reward given any state and action. [ 5 ] (Note that this is a different meaning from the term generative model in the context of statistical classification .) In algorithms that are expressed using pseudocode , G {\\displaystyle G} is often used to represent a generative model. For example, the expression s ′ , r ← G ( s , a ) {\\displaystyle s',r\\gets G(s,a)} might denote the action of sampling from the generative model where s {\\displaystyle s} and a {\\displaystyle a} are the current state and action, and s ′ {\\displaystyle s'} and r {\\displaystyle r} are the new state and reward. Compared to an episodic simulator, a generative model has the advantage that it can yield data from any state, not only those encountered in a trajectory. These model classes form a hierarchy of information content: an explicit model trivially yields a generative model through sampling from the distributions, and repeated application of a generative model yields an episodic simulator. In the opposite direction, it is only possible to learn approximate models through regression . The type of model available for a particular MDP plays a significant role in determining which solution algorithms are appropriate. For example, the dynamic programming algorithms described in the next section require an explicit model, and Monte Carlo tree search requires a generative model (or an episodic simulator that can be copied at any state), whereas most reinforcement learning algorithms require only an episodic simulator. Pole Balancing example (rendering of the environment from the Open AI gym benchmark ) An example of MDP is the Pole-Balancing model, which comes from classic control theory. In this example, we have S {\\displaystyle S} is the set of ordered tuples ( θ , θ ˙ , x , x ˙ ) ⊂ R 4 {\\displaystyle (\\theta ,{\\dot {\\theta }},x,{\\dot {x}})\\subset \\mathbb {R} ^{4}} given by pole angle, angular velocity, position of the cart and its speed. A {\\displaystyle A} is { − 1 , 1 } {\\displaystyle \\{-1,1\\}} , corresponding to applying a force on the left (right) on the cart. P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} is the transition of the system, which in this case is going to be deterministic and driven by the laws of mechanics. R a ( s , s ′ ) {\\displaystyle R_{a}(s,s')} is 1 {\\displaystyle 1} if the pole is up after the"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 2,
    "text": "transition, zero otherwise. Therefore, this function only depend on s ′ {\\displaystyle s'} in this specific case. Solutions for MDPs with finite state and action spaces may be found through a variety of methods such as dynamic programming . The algorithms in this section apply to MDPs with finite state and action spaces and explicitly given transition probabilities and reward functions, but the basic concepts may be extended to handle other problem classes, for example using function approximation . Also, some processes with countably infinite state and action spaces can be exactly reduced to ones with finite state and action spaces. [ 6 ] The standard family of algorithms to calculate optimal policies for finite state and action MDPs requires storage for two arrays indexed by state: value V {\\displaystyle V} , which contains real values, and policy π {\\displaystyle \\pi } , which contains actions. At the end of the algorithm, π {\\displaystyle \\pi } will contain the solution and V ( s ) {\\displaystyle V(s)} will contain the discounted sum of the rewards to be earned (on average) by following that solution from state s {\\displaystyle s} . The algorithm has two steps, (1) a value update and (2) a policy update, which are repeated in some order for all the states until no further changes take place. Both recursively update a new estimation of the optimal policy and state value using an older estimation of those values. V ( s ) := ∑ s ′ P π ( s ) ( s , s ′ ) ( R π ( s ) ( s , s ′ ) + γ V ( s ′ ) ) {\\displaystyle V(s):=\\sum _{s'}P_{\\pi (s)}(s,s')\\left(R_{\\pi (s)}(s,s')+\\gamma V(s')\\right)} π ( s ) := argmax a ⁡ { ∑ s ′ P a ( s , s ′ ) ( R a ( s , s ′ ) + γ V ( s ′ ) ) } {\\displaystyle \\pi (s):=\\operatorname {argmax} _{a}\\left\\{\\sum _{s'}P_{a}(s,s')\\left(R_{a}(s,s')+\\gamma V(s')\\right)\\right\\}} Their order depends on the variant of the algorithm; one can also do them for all states at once or state by state, and more often to some states than others. As long as no state is permanently excluded from either of the steps, the algorithm will eventually arrive at the correct solution. [ 7 ] In value iteration ( Bellman 1957 ), which is also called backward induction , the π {\\displaystyle \\pi } function is not used; instead, the value of π ( s ) {\\displaystyle \\pi (s)} is calculated within V ( s ) {\\displaystyle V(s)} whenever it is needed. Substituting the calculation of π ( s ) {\\displaystyle \\pi (s)} into the calculation of V ( s ) {\\displaystyle V(s)} gives the combined step; [ further explanation needed ] V i + 1 ( s ) := max a { ∑ s ′ P a ( s , s ′ ) ( R a ( s , s ′ ) + γ V i ( s ′ ) ) } , {\\displaystyle V_{i+1}(s):=\\max _{a}\\left\\{\\sum _{s'}P_{a}(s,s')\\left(R_{a}(s,s')+\\gamma V_{i}(s')\\right)\\right\\},} where i {\\displaystyle i} is the iteration number. Value iteration starts at i = 0 {\\displaystyle i=0} and V 0 {\\displaystyle V_{0}} as a guess of the value function . It then iterates, repeatedly computing V i + 1 {\\displaystyle V_{i+1}} for all states s {\\displaystyle s} , until V {\\displaystyle V} converges with the left-hand side equal to the right-hand side (which is the \" Bellman equation \" for this problem [ clarification needed ] ). Lloyd Shapley 's 1953 paper on stochastic games included as a special case the value iteration method for MDPs, [ 8 ] but this was recognized only later on. [ 9 ] In policy iteration ( Howard 1960 ) harv error: no target: CITEREFHoward1960 ( help ) , step one is performed once, and then step two is performed once, then both are repeated until policy converges. Then step one is again performed once and so on. (Policy iteration was invented by Howard to optimize Sears catalogue mailing, which he had been optimizing using value iteration. [ 10 ] ) Instead of repeating step two to convergence, it may be formulated and solved as a set of linear equations. These equations are merely obtained by making s = s ′ {\\displaystyle s=s'} in the step two equation. [ clarification needed ] Thus, repeating step two to convergence can be interpreted as solving the linear equations by relaxation . This variant has the advantage that there is a definite stopping condition: when the array π {\\displaystyle \\pi } does not change in the course of applying step 1 to all states, the algorithm is completed. Policy iteration is usually slower than value iteration for a large number of possible states. Modified policy iteration ["
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 3,
    "text": "edit ] In modified policy iteration ( van Nunen 1976 ; Puterman & Shin 1978 ), step one is performed once, and then step two is repeated several times. [ 11 ] [ 12 ] Then step one is again performed once and so on. Prioritized sweeping [ edit ] In this variant, the steps are preferentially applied to states which are in some way important – whether based on the algorithm (there were large changes in V {\\displaystyle V} or π {\\displaystyle \\pi } around those states recently) or based on use (those states are near the starting state, or otherwise of interest to the person or program using the algorithm). Computational complexity [ edit ] Algorithms for finding optimal policies with time complexity polynomial in the size of the problem representation exist for finite MDPs. Thus, decision problems based on MDPs are in computational complexity class P . [ 13 ] However, due to the curse of dimensionality , the size of the problem representation is often exponential in the number of state and action variables, limiting exact solution techniques to problems that have a compact representation. In practice, online planning techniques such as Monte Carlo tree search can find useful solutions in larger problems, and, in theory, it is possible to construct online planning algorithms that can find an arbitrarily near-optimal policy with no computational complexity dependence on the size of the state space. [ 14 ] Extensions and generalizations [ edit ] A Markov decision process is a stochastic game with only one player. Partial observability [ edit ] The solution above assumes that the state s {\\displaystyle s} is known when action is to be taken; otherwise π ( s ) {\\displaystyle \\pi (s)} cannot be calculated. When this assumption is not true, the problem is called a partially observable Markov decision process or POMDP. Constrained Markov decision processes [ edit ] Constrained Markov decision processes (CMDPS) are extensions to Markov decision process (MDPs). There are three fundamental differences between MDPs and CMDPs. [ 15 ] There are multiple costs incurred after applying an action instead of one. CMDPs are solved with linear programs only, and dynamic programming does not work. The final policy depends on the starting state. The method of Lagrange multipliers applies to CMDPs. Many Lagrangian-based algorithms have been developed. Natural policy gradient primal-dual method. [ 16 ] There are a number of applications for CMDPs. It has recently been used in motion planning scenarios in robotics. [ 17 ] Continuous-time Markov decision process [ edit ] In discrete-time Markov Decision Processes, decisions are made at discrete time intervals. However, for continuous-time Markov decision processes , decisions can be made at any time the decision maker chooses. In comparison to discrete-time Markov decision processes, continuous-time Markov decision processes can better model the decision-making process for a system that has continuous dynamics , i.e., the system dynamics is defined by ordinary differential equations (ODEs). These kind of applications raise in queueing systems , epidemic processes, and population processes . Like the discrete-time Markov decision processes, in continuous-time Markov decision processes the agent aims at finding the optimal policy which could maximize the expected cumulated reward. The only difference with the standard case stays in the fact that, due to the continuous nature of the time variable, the sum is replaced by an integral: max E π ⁡ [ ∫ 0 ∞ γ t r ( s ( t ) , π ( s ( t ) ) ) d t | s 0 ] {\\displaystyle \\max \\operatorname {E} _{\\pi }\\left[\\left.\\int _{0}^{\\infty }\\gamma ^{t}r(s(t),\\pi (s(t)))\\,dt\\;\\right|s_{0}\\right]} where 0 ≤ γ < 1. {\\displaystyle 0\\leq \\gamma <1.} Discrete space: Linear programming formulation [ edit ] If the state space and action space are finite, we could use linear programming to find the optimal policy, which was one of the earliest approaches applied. Here we only consider the ergodic model, which means our continuous-time MDP becomes an ergodic continuous-time Markov chain under a stationary policy . Under this assumption, although the decision maker can make a decision at any time in the current state, there is no benefit in taking multiple actions. It is better to take an action only at the time when system is transitioning from the current state to another state. Under some conditions, [ 18 ] if our optimal value function V ∗ {\\displaystyle V^{*}} is independent of state i {\\displaystyle i} , we will have the following inequality: g ≥ R ( i , a ) + ∑ j ∈ S q ( j ∣ i , a ) h ( j ) ∀ i ∈ S and a ∈ A ( i ) {\\displaystyle g\\geq R(i,a)+\\sum _{j\\in S}q(j\\mid i,a)h(j)\\quad \\forall i\\in S{\\text{ and }}a\\in A(i)} If there exists a function h"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 4,
    "text": "{\\displaystyle h} , then V ¯ ∗ {\\displaystyle {\\bar {V}}^{*}} will be the smallest g {\\displaystyle g} satisfying the above equation. In order to find V ¯ ∗ {\\displaystyle {\\bar {V}}^{*}} , we could use the following linear programming model: Primal linear program(P-LP) Minimize g s.t g − ∑ j ∈ S q ( j ∣ i , a ) h ( j ) ≥ R ( i , a ) ∀ i ∈ S , a ∈ A ( i ) {\\displaystyle {\\begin{aligned}{\\text{Minimize}}\\quad &g\\\\{\\text{s.t}}\\quad &g-\\sum _{j\\in S}q(j\\mid i,a)h(j)\\geq R(i,a)\\,\\,\\forall i\\in S,\\,a\\in A(i)\\end{aligned}}} Dual linear program(D-LP) Maximize ∑ i ∈ S ∑ a ∈ A ( i ) R ( i , a ) y ( i , a ) s.t. ∑ i ∈ S ∑ a ∈ A ( i ) q ( j ∣ i , a ) y ( i , a ) = 0 ∀ j ∈ S , ∑ i ∈ S ∑ a ∈ A ( i ) y ( i , a ) = 1 , y ( i , a ) ≥ 0 ∀ a ∈ A ( i ) and ∀ i ∈ S {\\displaystyle {\\begin{aligned}{\\text{Maximize}}&\\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y(i,a)\\\\{\\text{s.t.}}&\\sum _{i\\in S}\\sum _{a\\in A(i)}q(j\\mid i,a)y(i,a)=0\\quad \\forall j\\in S,\\\\&\\sum _{i\\in S}\\sum _{a\\in A(i)}y(i,a)=1,\\\\&y(i,a)\\geq 0\\qquad \\forall a\\in A(i){\\text{ and }}\\forall i\\in S\\end{aligned}}} y ( i , a ) {\\displaystyle y(i,a)} is a feasible solution to the D-LP if y ( i , a ) {\\displaystyle y(i,a)} is nonnative and satisfied the constraints in the D-LP problem. A feasible solution y ∗ ( i , a ) {\\displaystyle y^{*}(i,a)} to the D-LP is said to be an optimal solution if ∑ i ∈ S ∑ a ∈ A ( i ) R ( i , a ) y ∗ ( i , a ) ≥ ∑ i ∈ S ∑ a ∈ A ( i ) R ( i , a ) y ( i , a ) {\\displaystyle {\\begin{aligned}\\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y^{*}(i,a)\\geq \\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y(i,a)\\end{aligned}}} for all feasible solution y ( i , a ) {\\displaystyle y(i,a)} to the D-LP. Once we have found the optimal solution y ∗ ( i , a ) {\\displaystyle y^{*}(i,a)} , we can use it to establish the optimal policies. Continuous space: Hamilton–Jacobi–Bellman equation [ edit ] In continuous-time MDP, if the state space and action space are continuous, the optimal criterion could be found by solving Hamilton–Jacobi–Bellman (HJB) partial differential equation . In order to discuss the HJB equation, we need to reformulate our problem V ( s ( 0 ) , 0 ) = max a ( t ) = π ( s ( t ) ) ∫ 0 T r ( s ( t ) , a ( t ) ) d t + D [ s ( T ) ] s.t. d s ( t ) d t = f [ t , s ( t ) , a ( t ) ] {\\displaystyle {\\begin{aligned}V(s(0),0)={}&\\max _{a(t)=\\pi (s(t))}\\int _{0}^{T}r(s(t),a(t))\\,dt+D[s(T)]\\\\{\\text{s.t.}}\\quad &{\\frac {ds(t)}{dt}}=f[t,s(t),a(t)]\\end{aligned}}} D ( ⋅ ) {\\displaystyle D(\\cdot )} is the terminal reward function, s ( t ) {\\displaystyle s(t)} is the system state vector, a ( t ) {\\displaystyle a(t)} is the system control vector we try to find. f ( ⋅ ) {\\displaystyle f(\\cdot )} shows how the state vector changes over time. The Hamilton–Jacobi–Bellman equation is as follows: 0 = max u ( r ( t , s , a ) + ∂ V ( t , s ) ∂ x f ( t , s , a ) ) {\\displaystyle 0=\\max _{u}(r(t,s,a)+{\\frac {\\partial V(t,s)}{\\partial x}}f(t,s,a))} We could solve the equation to find the optimal control a ( t ) {\\displaystyle a(t)} , which could give us the optimal value function V ∗ {\\displaystyle V^{*}} Reinforcement learning [ edit ] Reinforcement learning is an interdisciplinary area of machine learning and optimal control that has, as main objective, finding an approximately optimal policy for MDPs where transition probabilities and rewards are unknown. [ 19 ] Reinforcement learning can solve Markov-Decision processes without explicit specification of the transition probabilities which are instead needed to perform policy iteration. In this setting, transition probabilities and rewards must be learned from experience, i.e. by letting an agent interact with the MDP for a given number of steps. Both on a theoretical and on a practical level, effort is put in maximizing the sample efficiency, i.e. minimimizing the number of samples needed to learn a policy whose performance is ε − {\\displaystyle \\varepsilon -} close to the optimal one (due to the stochastic nature of the process, learning the optimal policy with a finite number of samples is, in general, impossible). Reinforcement Learning for discrete MDPs [ edit ] For the purpose of this section, it is useful to define a further function, which corresponds"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 5,
    "text": "to taking the action a {\\displaystyle a} and then continuing optimally (or according to whatever policy one currently has): Q ( s , a ) = ∑ s ′ P a ( s , s ′ ) ( R a ( s , s ′ ) + γ V ( s ′ ) ) . {\\displaystyle \\ Q(s,a)=\\sum _{s'}P_{a}(s,s')(R_{a}(s,s')+\\gamma V(s')).\\ } While this function is also unknown, experience during learning is based on ( s , a ) {\\displaystyle (s,a)} pairs (together with the outcome s ′ {\\displaystyle s'} ; that is, \"I was in state s {\\displaystyle s} and I tried doing a {\\displaystyle a} and s ′ {\\displaystyle s'} happened\"). Thus, one has an array Q {\\displaystyle Q} and uses experience to update it directly. This is known as Q-learning . Another application of MDP process in machine learning theory is called learning automata. This is also one type of reinforcement learning if the environment is stochastic. The first detail learning automata paper is surveyed by Narendra and Thathachar (1974), which were originally described explicitly as finite-state automata . [ 20 ] Similar to reinforcement learning, a learning automata algorithm also has the advantage of solving the problem when probability or rewards are unknown. The difference between learning automata and Q-learning is that the former technique omits the memory of Q-values, but updates the action probability directly to find the learning result. Learning automata is a learning scheme with a rigorous proof of convergence. [ 21 ] In learning automata theory, a stochastic automaton consists of: a set x of possible inputs, a set Φ = { Φ 1 , ..., Φ s } of possible internal states, a set α = { α 1 , ..., α r } of possible outputs, or actions, with r ≤ s , an initial state probability vector p (0) = ≪ p 1 (0), ..., p s (0) ≫, a computable function A which after each time step t generates p ( t + 1) from p ( t ), the current input, and the current state, and a function G : Φ → α which generates the output at each time step. The states of such an automaton correspond to the states of a \"discrete-state discrete-parameter Markov process \". At each time step t = 0,1,2,3,..., the automaton reads an input from its environment, updates P( t ) to P( t + 1) by A , randomly chooses a successor state according to the probabilities P( t + 1) and outputs the corresponding action. The automaton's environment, in turn, reads the action and sends the next input to the automaton. [ 21 ] Category theoretic interpretation [ edit ] Other than the rewards, a Markov decision process ( S , A , P ) {\\displaystyle (S,A,P)} can be understood in terms of Category theory . Namely, let A {\\displaystyle {\\mathcal {A}}} denote the free monoid with generating set A . Let Dist denote the Kleisli category of the Giry monad . Then a functor A → D i s t {\\displaystyle {\\mathcal {A}}\\to \\mathbf {Dist} } encodes both the set S of states and the probability function P . In this way, Markov decision processes could be generalized from monoids (categories with one object) to arbitrary categories. One can call the result ( C , F : C → D i s t ) {\\displaystyle ({\\mathcal {C}},F:{\\mathcal {C}}\\to \\mathbf {Dist} )} a context-dependent Markov decision process , because moving from one object to another in C {\\displaystyle {\\mathcal {C}}} changes the set of available actions and the set of possible states. [ citation needed ] Alternative notations [ edit ] The terminology and notation for MDPs are not entirely settled. There are two main streams — one focuses on maximization problems from contexts like economics, using the terms action, reward, value, and calling the discount factor β or γ , while the other focuses on minimization problems from engineering and navigation [ citation needed ] , using the terms control, cost, cost-to-go, and calling the discount factor α . In addition, the notation for the transition probability varies. in this article alternative comment action a control u reward R cost g g is the negative of R value V cost-to-go J J is the negative of V policy π policy μ discounting factor γ discounting factor α transition probability P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} transition probability p s s ′ ( a ) {\\displaystyle p_{ss'}(a)} In addition, transition probability is sometimes written Pr ( s , a , s ′ ) {\\displaystyle \\Pr(s,a,s')} , Pr ( s ′ ∣ s , a ) {\\displaystyle \\Pr(s'\\mid s,a)} or, rarely, p s ′ s ( a ) . {\\displaystyle p_{s's}(a).} ^ Puterman, Martin L. (1994). Markov"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 6,
    "text": "decision processes: discrete stochastic dynamic programming . Wiley series in probability and mathematical statistics. Applied probability and statistics section. New York: Wiley. ISBN 978-0-471-61977-2 . ^ Schneider, S.; Wagner, D. H. (1957-02-26). \"Error detection in redundant systems\" . Papers presented at the February 26-28, 1957, western joint computer conference: Techniques for reliability on - IRE-AIEE-ACM '57 (Western) . New York, NY, USA: Association for Computing Machinery. pp. 115– 121. doi : 10.1145/1455567.1455587 . ISBN 978-1-4503-7861-1 . ^ Bellman, Richard (1958-09-01). \"Dynamic programming and stochastic control processes\" . Information and Control . 1 (3): 228– 239. doi : 10.1016/S0019-9958(58)80003-0 . ISSN 0019-9958 . ^ a b Sutton, Richard S.; Barto, Andrew G. (2018). Reinforcement learning: an introduction . Adaptive computation and machine learning series (2nd ed.). Cambridge, Massachusetts: The MIT Press. ISBN 978-0-262-03924-6 . ^ Kearns, Michael; Mansour, Yishay; Ng, Andrew (2002). \"A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes\" . Machine Learning . 49 ( 193– 208): 193– 208. doi : 10.1023/A:1017932429737 . ^ Wrobel, A. (1984). \"On Markovian decision models with a finite skeleton\". Zeitschrift für Operations Research . 28 (1): 17– 27. doi : 10.1007/bf01919083 . S2CID 2545336 . ^ Reinforcement Learning: Theory and Python Implementation . Beijing: China Machine Press. 2019. p. 44. ISBN 9787111631774 . ^ Shapley, Lloyd (1953). \"Stochastic Games\" . Proceedings of the National Academy of Sciences of the United States of America . 39 (10): 1095– 1100. Bibcode : 1953PNAS...39.1095S . doi : 10.1073/pnas.39.10.1095 . PMC 1063912 . PMID 16589380 . ^ Kallenberg, Lodewijk (2002). \"Finite state and action MDPs\". In Feinberg, Eugene A. ; Shwartz, Adam (eds.). Handbook of Markov decision processes: methods and applications . Springer. ISBN 978-0-7923-7459-6 . ^ Howard 2002, \"Comments on the Origin and Application of Markov Decision Processes\" ^ Puterman, M. L.; Shin, M. C. (1978). \"Modified Policy Iteration Algorithms for Discounted Markov Decision Problems\". Management Science . 24 (11): 1127– 1137. doi : 10.1287/mnsc.24.11.1127 . ^ van Nunen, J.A. E. E (1976). \"A set of successive approximation methods for discounted Markovian decision problems\". Zeitschrift für Operations Research . 20 (5): 203– 208. doi : 10.1007/bf01920264 . S2CID 5167748 . ^ Papadimitriou, Christos ; Tsitsiklis, John (1987). \"The Complexity of Markov Decision Processes\" . Mathematics of Operations Research . 12 (3): 441– 450. doi : 10.1287/moor.12.3.441 . hdl : 1721.1/2893 . Retrieved November 2, 2023 . ^ Kearns, Michael; Mansour, Yishay; Ng, Andrew (November 2002). \"A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes\" . Machine Learning . 49 (2/3): 193– 208. doi : 10.1023/A:1017932429737 . ^ Altman, Eitan (1999). Constrained Markov decision processes . Vol. 7. CRC Press. ^ Ding, Dongsheng; Zhang, Kaiqing; Jovanovic, Mihailo; Basar, Tamer (2020). Natural policy gradient primal-dual method for constrained Markov decision processes . Advances in Neural Information Processing Systems. ^ Feyzabadi, S.; Carpin, S. (18–22 Aug 2014). \"Risk-aware path planning using hierarchical constrained Markov Decision Processes\" . Automation Science and Engineering (CASE) . IEEE International Conference. pp. 297, 303. ^ Continuous-Time Markov Decision Processes . Stochastic Modelling and Applied Probability. Vol. 62. 2009. doi : 10.1007/978-3-642-02547-1 . ISBN 978-3-642-02546-4 . ^ Shoham, Y.; Powers, R.; Grenager, T. (2003). \"Multi-agent reinforcement learning: a critical survey\" (PDF) . Technical Report, Stanford University : 1– 13 . Retrieved 2018-12-12 . ^ Narendra, K. S. ; Thathachar, M. A. L. (1974). \"Learning Automata – A Survey\". IEEE Transactions on Systems, Man, and Cybernetics . SMC-4 (4): 323– 334. CiteSeerX 10.1.1.295.2280 . doi : 10.1109/TSMC.1974.5408453 . ISSN 0018-9472 . ^ a b Narendra, Kumpati S. ; Thathachar, Mandayam A. L. (1989). Learning automata: An introduction . Prentice Hall. ISBN 9780134855585 . Bellman., R. E. (2003) [1957]. Dynamic Programming (Dover paperback ed.). Princeton, NJ: Princeton University Press. ISBN 978-0-486-42809-3 . Bertsekas, D. (1995). Dynamic Programming and Optimal Control . Vol. 2. MA: Athena. Derman, C. (1970). Finite state Markovian decision processes . Academic Press. Feinberg, E.A.; Shwartz, A., eds. (2002). Handbook of Markov Decision Processes . Boston, MA: Kluwer. ISBN 9781461508052 . Guo, X.; Hernández-Lerma, O. (2009). Continuous-Time Markov Decision Processes . Stochastic Modelling and Applied Probability. Springer. ISBN 9783642025464 . Meyn, S. P. (2007). Control Techniques for Complex Networks . Cambridge University Press. ISBN 978-0-521-88441-9 . Archived from the original on 19 June 2010. Appendix contains abridged \"Meyn & Tweedie\" . Archived from the original on 18 December 2012. Puterman., M. L. (1994). Markov Decision Processes . Wiley. Ross, S. M. (1983). Introduction to stochastic dynamic programming (PDF) . Academic press. Sutton, R. S.; Barto, A. G. (2017). Reinforcement Learning: An Introduction . Cambridge, MA: The MIT Press. Tijms., H.C. (2003). A First Course in Stochastic Models . Wiley. ISBN 9780470864289 ."
  },
  {
    "type": "doc",
    "url": "https://spinningup.openai.com/en/latest/algorithms/ppo.html",
    "title": "Proximal Policy Optimization - Spinning Up documentation",
    "chunk": 0,
    "text": "PPO is motivated by the same question as TRPO: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse? Where TRPO tries to solve this problem with a complex second-order method, PPO is a family of first-order methods that use a few other tricks to keep new policies close to old. PPO methods are significantly simpler to implement, and empirically seem to perform at least as well as TRPO. There are two primary variants of PPO: PPO-Penalty and PPO-Clip. Here, we’ll focus only on PPO-Clip (the primary variant used at OpenAI). PPO-clip updates policies via typically taking multiple steps of (usually minibatch) SGD to maximize the objective. Here is given by in which is a (small) hyperparameter which roughly says how far away the new policy is allowed to go from the old. This is a pretty complex expression, and it’s hard to tell at first glance what it’s doing, or how it helps keep the new policy close to the old policy. As it turns out, there’s a considerably simplified version of this objective which is a bit easier to grapple with (and is also the version we implement in our code): where To figure out what intuition to take away from this, let’s look at a single state-action pair , and think of cases. Advantage is positive : Suppose the advantage for that state-action pair is positive, in which case its contribution to the objective reduces to Because the advantage is positive, the objective will increase if the action becomes more likely—that is, if increases. But the min in this term puts a limit to how much the objective can increase. Once , the min kicks in and this term hits a ceiling of . Thus: the new policy does not benefit by going far away from the old policy . Advantage is negative : Suppose the advantage for that state-action pair is negative, in which case its contribution to the objective reduces to Because the advantage is negative, the objective will increase if the action becomes less likely—that is, if decreases. But the max in this term puts a limit to how much the objective can increase. Once , the max kicks in and this term hits a ceiling of . Thus, again: the new policy does not benefit by going far away from the old policy . What we have seen so far is that clipping serves as a regularizer by removing incentives for the policy to change dramatically, and the hyperparameter corresponds to how far away the new policy can go from the old while still profiting the objective. You Should Know While this kind of clipping goes a long way towards ensuring reasonable policy updates, it is still possible to end up with a new policy which is too far from the old policy, and there are a bunch of tricks used by different PPO implementations to stave this off. In our implementation here, we use a particularly simple method: early stopping. If the mean KL-divergence of the new policy from the old grows beyond a threshold, we stop taking gradient steps. When you feel comfortable with the basic math and implementation details, it’s worth checking out other implementations to see how they handle this issue!"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 0,
    "text": "Mathematical model for sequential decision making under uncertainty Markov decision process ( MDP ), also called a stochastic dynamic program or stochastic control problem, is a model for sequential decision making when outcomes are uncertain. [ 1 ] Originating from operations research in the 1950s, [ 2 ] [ 3 ] MDPs have since gained recognition in a variety of fields, including ecology , economics , healthcare , telecommunications and reinforcement learning . [ 4 ] Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. In this framework, the interaction is characterized by states, actions, and rewards. The MDP framework is designed to provide a simplified representation of key elements of artificial intelligence challenges. These elements encompass the understanding of cause and effect , the management of uncertainty and nondeterminism, and the pursuit of explicit goals. [ 4 ] The name comes from its connection to Markov chains , a concept developed by the Russian mathematician Andrey Markov . The \"Markov\" in \"Markov decision process\" refers to the underlying structure of state transitions that still follow the Markov property . The process is called a \"decision process\" because it involves making decisions that influence these state transitions, extending the concept of a Markov chain into the realm of decision-making under uncertainty. Example of a simple MDP with three states (green circles) and two actions (orange circles), with two rewards (orange arrows) A Markov decision process is a 4- tuple ( S , A , P a , R a ) {\\displaystyle (S,A,P_{a},R_{a})} , where: S {\\displaystyle S} is a set of states called the state space . The state space may be discrete or continuous, like the set of real numbers . A {\\displaystyle A} is a set of actions called the action space (alternatively, A s {\\displaystyle A_{s}} is the set of actions available from state s {\\displaystyle s} ). As for state, this set may be discrete or continuous. P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} is, on an intuitive level, the probability that action a {\\displaystyle a} in state s {\\displaystyle s} at time t {\\displaystyle t} will lead to state s ′ {\\displaystyle s'} at time t + 1 {\\displaystyle t+1} . In general, this probability transition is defined to satisfy Pr ( s t + 1 ∈ S ′ ∣ s t = s , a t = a ) = ∫ S ′ P a ( s , s ′ ) d s ′ , {\\displaystyle \\Pr(s_{t+1}\\in S'\\mid s_{t}=s,a_{t}=a)=\\int _{S'}P_{a}(s,s')ds',} for every S ′ ⊆ S {\\displaystyle S'\\subseteq S} measurable. In case the state space is discrete, the integral is intended with respect to the counting measure , so that the latter simplifies as P a ( s , s ′ ) = Pr ( s t + 1 = s ′ ∣ s t = s , a t = a ) {\\displaystyle P_{a}(s,s')=\\Pr(s_{t+1}=s'\\mid s_{t}=s,a_{t}=a)} ; In case S ⊆ R d {\\displaystyle S\\subseteq \\mathbb {R} ^{d}} , the integral is usually intended with respect to the Lebesgue measure . R a ( s , s ′ ) {\\displaystyle R_{a}(s,s')} is the immediate reward (or expected immediate reward) received after transitioning from state s {\\displaystyle s} to state s ′ {\\displaystyle s'} , due to action a {\\displaystyle a} . A policy function π {\\displaystyle \\pi } is a (potentially probabilistic) mapping from state space ( S {\\displaystyle S} ) to action space ( A {\\displaystyle A} ). Optimization objective [ edit ] The goal in a Markov decision process is to find a good \"policy\" for the decision maker: a function π {\\displaystyle \\pi } that specifies the action π ( s ) {\\displaystyle \\pi (s)} that the decision maker will choose when in state s {\\displaystyle s} . Once a Markov decision process is combined with a policy in this way, this fixes the action for each state and the resulting combination behaves like a Markov chain (since the action chosen in state s {\\displaystyle s} is completely determined by π ( s ) {\\displaystyle \\pi (s)} ). The objective is to choose a policy π {\\displaystyle \\pi } that will maximize some cumulative function of the random rewards, typically the expected discounted sum over a potentially infinite horizon: E [ ∑ t = 0 ∞ γ t R a t ( s t , s t + 1 ) ] {\\displaystyle E\\left[\\sum _{t=0}^{\\infty }{\\gamma ^{t}R_{a_{t}}(s_{t},s_{t+1})}\\right]} (where we choose a t = π ( s t ) {\\displaystyle a_{t}=\\pi (s_{t})} , i.e. actions given by the policy). And the expectation is taken over s t + 1 ∼ P a t ( s t , s t + 1 ) {\\displaystyle s_{t+1}\\sim P_{a_{t}}(s_{t},s_{t+1})} where γ {\\displaystyle \\ \\gamma \\ }"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 1,
    "text": "is the discount factor satisfying 0 ≤ γ ≤ 1 {\\displaystyle 0\\leq \\ \\gamma \\ \\leq \\ 1} , which is usually close to 1 {\\displaystyle 1} (for example, γ = 1 / ( 1 + r ) {\\displaystyle \\gamma =1/(1+r)} for some discount rate r {\\displaystyle r} ). A lower discount factor makes the decision maker more short-sighted, in that it comparatively disregards the effect that following its current policy has at times lying further in the future. Another possible, but strictly related, objective that is commonly used is the H − {\\displaystyle H-} step return. This time, instead of using a discount factor γ {\\displaystyle \\ \\gamma \\ } , the agent is interested only in the first H {\\displaystyle H} steps of the process, with each reward having the same weight. E [ ∑ t = 0 H − 1 R a t ( s t , s t + 1 ) ] {\\displaystyle E\\left[\\sum _{t=0}^{H-1}{R_{a_{t}}(s_{t},s_{t+1})}\\right]} (where we choose a t = π ( s t ) {\\displaystyle a_{t}=\\pi (s_{t})} , i.e. actions given by the policy). And the expectation is taken over s t + 1 ∼ P a t ( s t , s t + 1 ) {\\displaystyle s_{t+1}\\sim P_{a_{t}}(s_{t},s_{t+1})} where H {\\displaystyle \\ H\\ } is the time horizon. Compared to the previous objective, the latter one is more used in Learning Theory . A policy that maximizes the function above is called an optimal policy and is usually denoted π ∗ {\\displaystyle \\pi ^{*}} . A particular MDP may have multiple distinct optimal policies. Because of the Markov property , it can be shown that the optimal policy is a function of the current state, as assumed above. In many cases, it is difficult to represent the transition probability distributions, P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} , explicitly. In such cases, a simulator can be used to model the MDP implicitly by providing samples from the transition distributions. One common form of implicit MDP model is an episodic environment simulator that can be started from an initial state and yields a subsequent state and reward every time it receives an action input. In this manner, trajectories of states, actions, and rewards, often called episodes may be produced. Another form of simulator is a generative model , a single step simulator that can generate samples of the next state and reward given any state and action. [ 5 ] (Note that this is a different meaning from the term generative model in the context of statistical classification .) In algorithms that are expressed using pseudocode , G {\\displaystyle G} is often used to represent a generative model. For example, the expression s ′ , r ← G ( s , a ) {\\displaystyle s',r\\gets G(s,a)} might denote the action of sampling from the generative model where s {\\displaystyle s} and a {\\displaystyle a} are the current state and action, and s ′ {\\displaystyle s'} and r {\\displaystyle r} are the new state and reward. Compared to an episodic simulator, a generative model has the advantage that it can yield data from any state, not only those encountered in a trajectory. These model classes form a hierarchy of information content: an explicit model trivially yields a generative model through sampling from the distributions, and repeated application of a generative model yields an episodic simulator. In the opposite direction, it is only possible to learn approximate models through regression . The type of model available for a particular MDP plays a significant role in determining which solution algorithms are appropriate. For example, the dynamic programming algorithms described in the next section require an explicit model, and Monte Carlo tree search requires a generative model (or an episodic simulator that can be copied at any state), whereas most reinforcement learning algorithms require only an episodic simulator. Pole Balancing example (rendering of the environment from the Open AI gym benchmark ) An example of MDP is the Pole-Balancing model, which comes from classic control theory. In this example, we have S {\\displaystyle S} is the set of ordered tuples ( θ , θ ˙ , x , x ˙ ) ⊂ R 4 {\\displaystyle (\\theta ,{\\dot {\\theta }},x,{\\dot {x}})\\subset \\mathbb {R} ^{4}} given by pole angle, angular velocity, position of the cart and its speed. A {\\displaystyle A} is { − 1 , 1 } {\\displaystyle \\{-1,1\\}} , corresponding to applying a force on the left (right) on the cart. P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} is the transition of the system, which in this case is going to be deterministic and driven by the laws of mechanics. R a ( s , s ′ ) {\\displaystyle R_{a}(s,s')} is 1 {\\displaystyle 1} if the pole is up after the"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 2,
    "text": "transition, zero otherwise. Therefore, this function only depend on s ′ {\\displaystyle s'} in this specific case. Solutions for MDPs with finite state and action spaces may be found through a variety of methods such as dynamic programming . The algorithms in this section apply to MDPs with finite state and action spaces and explicitly given transition probabilities and reward functions, but the basic concepts may be extended to handle other problem classes, for example using function approximation . Also, some processes with countably infinite state and action spaces can be exactly reduced to ones with finite state and action spaces. [ 6 ] The standard family of algorithms to calculate optimal policies for finite state and action MDPs requires storage for two arrays indexed by state: value V {\\displaystyle V} , which contains real values, and policy π {\\displaystyle \\pi } , which contains actions. At the end of the algorithm, π {\\displaystyle \\pi } will contain the solution and V ( s ) {\\displaystyle V(s)} will contain the discounted sum of the rewards to be earned (on average) by following that solution from state s {\\displaystyle s} . The algorithm has two steps, (1) a value update and (2) a policy update, which are repeated in some order for all the states until no further changes take place. Both recursively update a new estimation of the optimal policy and state value using an older estimation of those values. V ( s ) := ∑ s ′ P π ( s ) ( s , s ′ ) ( R π ( s ) ( s , s ′ ) + γ V ( s ′ ) ) {\\displaystyle V(s):=\\sum _{s'}P_{\\pi (s)}(s,s')\\left(R_{\\pi (s)}(s,s')+\\gamma V(s')\\right)} π ( s ) := argmax a ⁡ { ∑ s ′ P a ( s , s ′ ) ( R a ( s , s ′ ) + γ V ( s ′ ) ) } {\\displaystyle \\pi (s):=\\operatorname {argmax} _{a}\\left\\{\\sum _{s'}P_{a}(s,s')\\left(R_{a}(s,s')+\\gamma V(s')\\right)\\right\\}} Their order depends on the variant of the algorithm; one can also do them for all states at once or state by state, and more often to some states than others. As long as no state is permanently excluded from either of the steps, the algorithm will eventually arrive at the correct solution. [ 7 ] In value iteration ( Bellman 1957 ), which is also called backward induction , the π {\\displaystyle \\pi } function is not used; instead, the value of π ( s ) {\\displaystyle \\pi (s)} is calculated within V ( s ) {\\displaystyle V(s)} whenever it is needed. Substituting the calculation of π ( s ) {\\displaystyle \\pi (s)} into the calculation of V ( s ) {\\displaystyle V(s)} gives the combined step; [ further explanation needed ] V i + 1 ( s ) := max a { ∑ s ′ P a ( s , s ′ ) ( R a ( s , s ′ ) + γ V i ( s ′ ) ) } , {\\displaystyle V_{i+1}(s):=\\max _{a}\\left\\{\\sum _{s'}P_{a}(s,s')\\left(R_{a}(s,s')+\\gamma V_{i}(s')\\right)\\right\\},} where i {\\displaystyle i} is the iteration number. Value iteration starts at i = 0 {\\displaystyle i=0} and V 0 {\\displaystyle V_{0}} as a guess of the value function . It then iterates, repeatedly computing V i + 1 {\\displaystyle V_{i+1}} for all states s {\\displaystyle s} , until V {\\displaystyle V} converges with the left-hand side equal to the right-hand side (which is the \" Bellman equation \" for this problem [ clarification needed ] ). Lloyd Shapley 's 1953 paper on stochastic games included as a special case the value iteration method for MDPs, [ 8 ] but this was recognized only later on. [ 9 ] In policy iteration ( Howard 1960 ) harv error: no target: CITEREFHoward1960 ( help ) , step one is performed once, and then step two is performed once, then both are repeated until policy converges. Then step one is again performed once and so on. (Policy iteration was invented by Howard to optimize Sears catalogue mailing, which he had been optimizing using value iteration. [ 10 ] ) Instead of repeating step two to convergence, it may be formulated and solved as a set of linear equations. These equations are merely obtained by making s = s ′ {\\displaystyle s=s'} in the step two equation. [ clarification needed ] Thus, repeating step two to convergence can be interpreted as solving the linear equations by relaxation . This variant has the advantage that there is a definite stopping condition: when the array π {\\displaystyle \\pi } does not change in the course of applying step 1 to all states, the algorithm is completed. Policy iteration is usually slower than value iteration for a large number of possible states. Modified policy iteration ["
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 3,
    "text": "edit ] In modified policy iteration ( van Nunen 1976 ; Puterman & Shin 1978 ), step one is performed once, and then step two is repeated several times. [ 11 ] [ 12 ] Then step one is again performed once and so on. Prioritized sweeping [ edit ] In this variant, the steps are preferentially applied to states which are in some way important – whether based on the algorithm (there were large changes in V {\\displaystyle V} or π {\\displaystyle \\pi } around those states recently) or based on use (those states are near the starting state, or otherwise of interest to the person or program using the algorithm). Computational complexity [ edit ] Algorithms for finding optimal policies with time complexity polynomial in the size of the problem representation exist for finite MDPs. Thus, decision problems based on MDPs are in computational complexity class P . [ 13 ] However, due to the curse of dimensionality , the size of the problem representation is often exponential in the number of state and action variables, limiting exact solution techniques to problems that have a compact representation. In practice, online planning techniques such as Monte Carlo tree search can find useful solutions in larger problems, and, in theory, it is possible to construct online planning algorithms that can find an arbitrarily near-optimal policy with no computational complexity dependence on the size of the state space. [ 14 ] Extensions and generalizations [ edit ] A Markov decision process is a stochastic game with only one player. Partial observability [ edit ] The solution above assumes that the state s {\\displaystyle s} is known when action is to be taken; otherwise π ( s ) {\\displaystyle \\pi (s)} cannot be calculated. When this assumption is not true, the problem is called a partially observable Markov decision process or POMDP. Constrained Markov decision processes [ edit ] Constrained Markov decision processes (CMDPS) are extensions to Markov decision process (MDPs). There are three fundamental differences between MDPs and CMDPs. [ 15 ] There are multiple costs incurred after applying an action instead of one. CMDPs are solved with linear programs only, and dynamic programming does not work. The final policy depends on the starting state. The method of Lagrange multipliers applies to CMDPs. Many Lagrangian-based algorithms have been developed. Natural policy gradient primal-dual method. [ 16 ] There are a number of applications for CMDPs. It has recently been used in motion planning scenarios in robotics. [ 17 ] Continuous-time Markov decision process [ edit ] In discrete-time Markov Decision Processes, decisions are made at discrete time intervals. However, for continuous-time Markov decision processes , decisions can be made at any time the decision maker chooses. In comparison to discrete-time Markov decision processes, continuous-time Markov decision processes can better model the decision-making process for a system that has continuous dynamics , i.e., the system dynamics is defined by ordinary differential equations (ODEs). These kind of applications raise in queueing systems , epidemic processes, and population processes . Like the discrete-time Markov decision processes, in continuous-time Markov decision processes the agent aims at finding the optimal policy which could maximize the expected cumulated reward. The only difference with the standard case stays in the fact that, due to the continuous nature of the time variable, the sum is replaced by an integral: max E π ⁡ [ ∫ 0 ∞ γ t r ( s ( t ) , π ( s ( t ) ) ) d t | s 0 ] {\\displaystyle \\max \\operatorname {E} _{\\pi }\\left[\\left.\\int _{0}^{\\infty }\\gamma ^{t}r(s(t),\\pi (s(t)))\\,dt\\;\\right|s_{0}\\right]} where 0 ≤ γ < 1. {\\displaystyle 0\\leq \\gamma <1.} Discrete space: Linear programming formulation [ edit ] If the state space and action space are finite, we could use linear programming to find the optimal policy, which was one of the earliest approaches applied. Here we only consider the ergodic model, which means our continuous-time MDP becomes an ergodic continuous-time Markov chain under a stationary policy . Under this assumption, although the decision maker can make a decision at any time in the current state, there is no benefit in taking multiple actions. It is better to take an action only at the time when system is transitioning from the current state to another state. Under some conditions, [ 18 ] if our optimal value function V ∗ {\\displaystyle V^{*}} is independent of state i {\\displaystyle i} , we will have the following inequality: g ≥ R ( i , a ) + ∑ j ∈ S q ( j ∣ i , a ) h ( j ) ∀ i ∈ S and a ∈ A ( i ) {\\displaystyle g\\geq R(i,a)+\\sum _{j\\in S}q(j\\mid i,a)h(j)\\quad \\forall i\\in S{\\text{ and }}a\\in A(i)} If there exists a function h"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 4,
    "text": "{\\displaystyle h} , then V ¯ ∗ {\\displaystyle {\\bar {V}}^{*}} will be the smallest g {\\displaystyle g} satisfying the above equation. In order to find V ¯ ∗ {\\displaystyle {\\bar {V}}^{*}} , we could use the following linear programming model: Primal linear program(P-LP) Minimize g s.t g − ∑ j ∈ S q ( j ∣ i , a ) h ( j ) ≥ R ( i , a ) ∀ i ∈ S , a ∈ A ( i ) {\\displaystyle {\\begin{aligned}{\\text{Minimize}}\\quad &g\\\\{\\text{s.t}}\\quad &g-\\sum _{j\\in S}q(j\\mid i,a)h(j)\\geq R(i,a)\\,\\,\\forall i\\in S,\\,a\\in A(i)\\end{aligned}}} Dual linear program(D-LP) Maximize ∑ i ∈ S ∑ a ∈ A ( i ) R ( i , a ) y ( i , a ) s.t. ∑ i ∈ S ∑ a ∈ A ( i ) q ( j ∣ i , a ) y ( i , a ) = 0 ∀ j ∈ S , ∑ i ∈ S ∑ a ∈ A ( i ) y ( i , a ) = 1 , y ( i , a ) ≥ 0 ∀ a ∈ A ( i ) and ∀ i ∈ S {\\displaystyle {\\begin{aligned}{\\text{Maximize}}&\\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y(i,a)\\\\{\\text{s.t.}}&\\sum _{i\\in S}\\sum _{a\\in A(i)}q(j\\mid i,a)y(i,a)=0\\quad \\forall j\\in S,\\\\&\\sum _{i\\in S}\\sum _{a\\in A(i)}y(i,a)=1,\\\\&y(i,a)\\geq 0\\qquad \\forall a\\in A(i){\\text{ and }}\\forall i\\in S\\end{aligned}}} y ( i , a ) {\\displaystyle y(i,a)} is a feasible solution to the D-LP if y ( i , a ) {\\displaystyle y(i,a)} is nonnative and satisfied the constraints in the D-LP problem. A feasible solution y ∗ ( i , a ) {\\displaystyle y^{*}(i,a)} to the D-LP is said to be an optimal solution if ∑ i ∈ S ∑ a ∈ A ( i ) R ( i , a ) y ∗ ( i , a ) ≥ ∑ i ∈ S ∑ a ∈ A ( i ) R ( i , a ) y ( i , a ) {\\displaystyle {\\begin{aligned}\\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y^{*}(i,a)\\geq \\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y(i,a)\\end{aligned}}} for all feasible solution y ( i , a ) {\\displaystyle y(i,a)} to the D-LP. Once we have found the optimal solution y ∗ ( i , a ) {\\displaystyle y^{*}(i,a)} , we can use it to establish the optimal policies. Continuous space: Hamilton–Jacobi–Bellman equation [ edit ] In continuous-time MDP, if the state space and action space are continuous, the optimal criterion could be found by solving Hamilton–Jacobi–Bellman (HJB) partial differential equation . In order to discuss the HJB equation, we need to reformulate our problem V ( s ( 0 ) , 0 ) = max a ( t ) = π ( s ( t ) ) ∫ 0 T r ( s ( t ) , a ( t ) ) d t + D [ s ( T ) ] s.t. d s ( t ) d t = f [ t , s ( t ) , a ( t ) ] {\\displaystyle {\\begin{aligned}V(s(0),0)={}&\\max _{a(t)=\\pi (s(t))}\\int _{0}^{T}r(s(t),a(t))\\,dt+D[s(T)]\\\\{\\text{s.t.}}\\quad &{\\frac {ds(t)}{dt}}=f[t,s(t),a(t)]\\end{aligned}}} D ( ⋅ ) {\\displaystyle D(\\cdot )} is the terminal reward function, s ( t ) {\\displaystyle s(t)} is the system state vector, a ( t ) {\\displaystyle a(t)} is the system control vector we try to find. f ( ⋅ ) {\\displaystyle f(\\cdot )} shows how the state vector changes over time. The Hamilton–Jacobi–Bellman equation is as follows: 0 = max u ( r ( t , s , a ) + ∂ V ( t , s ) ∂ x f ( t , s , a ) ) {\\displaystyle 0=\\max _{u}(r(t,s,a)+{\\frac {\\partial V(t,s)}{\\partial x}}f(t,s,a))} We could solve the equation to find the optimal control a ( t ) {\\displaystyle a(t)} , which could give us the optimal value function V ∗ {\\displaystyle V^{*}} Reinforcement learning [ edit ] Reinforcement learning is an interdisciplinary area of machine learning and optimal control that has, as main objective, finding an approximately optimal policy for MDPs where transition probabilities and rewards are unknown. [ 19 ] Reinforcement learning can solve Markov-Decision processes without explicit specification of the transition probabilities which are instead needed to perform policy iteration. In this setting, transition probabilities and rewards must be learned from experience, i.e. by letting an agent interact with the MDP for a given number of steps. Both on a theoretical and on a practical level, effort is put in maximizing the sample efficiency, i.e. minimimizing the number of samples needed to learn a policy whose performance is ε − {\\displaystyle \\varepsilon -} close to the optimal one (due to the stochastic nature of the process, learning the optimal policy with a finite number of samples is, in general, impossible). Reinforcement Learning for discrete MDPs [ edit ] For the purpose of this section, it is useful to define a further function, which corresponds"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 5,
    "text": "to taking the action a {\\displaystyle a} and then continuing optimally (or according to whatever policy one currently has): Q ( s , a ) = ∑ s ′ P a ( s , s ′ ) ( R a ( s , s ′ ) + γ V ( s ′ ) ) . {\\displaystyle \\ Q(s,a)=\\sum _{s'}P_{a}(s,s')(R_{a}(s,s')+\\gamma V(s')).\\ } While this function is also unknown, experience during learning is based on ( s , a ) {\\displaystyle (s,a)} pairs (together with the outcome s ′ {\\displaystyle s'} ; that is, \"I was in state s {\\displaystyle s} and I tried doing a {\\displaystyle a} and s ′ {\\displaystyle s'} happened\"). Thus, one has an array Q {\\displaystyle Q} and uses experience to update it directly. This is known as Q-learning . Another application of MDP process in machine learning theory is called learning automata. This is also one type of reinforcement learning if the environment is stochastic. The first detail learning automata paper is surveyed by Narendra and Thathachar (1974), which were originally described explicitly as finite-state automata . [ 20 ] Similar to reinforcement learning, a learning automata algorithm also has the advantage of solving the problem when probability or rewards are unknown. The difference between learning automata and Q-learning is that the former technique omits the memory of Q-values, but updates the action probability directly to find the learning result. Learning automata is a learning scheme with a rigorous proof of convergence. [ 21 ] In learning automata theory, a stochastic automaton consists of: a set x of possible inputs, a set Φ = { Φ 1 , ..., Φ s } of possible internal states, a set α = { α 1 , ..., α r } of possible outputs, or actions, with r ≤ s , an initial state probability vector p (0) = ≪ p 1 (0), ..., p s (0) ≫, a computable function A which after each time step t generates p ( t + 1) from p ( t ), the current input, and the current state, and a function G : Φ → α which generates the output at each time step. The states of such an automaton correspond to the states of a \"discrete-state discrete-parameter Markov process \". At each time step t = 0,1,2,3,..., the automaton reads an input from its environment, updates P( t ) to P( t + 1) by A , randomly chooses a successor state according to the probabilities P( t + 1) and outputs the corresponding action. The automaton's environment, in turn, reads the action and sends the next input to the automaton. [ 21 ] Category theoretic interpretation [ edit ] Other than the rewards, a Markov decision process ( S , A , P ) {\\displaystyle (S,A,P)} can be understood in terms of Category theory . Namely, let A {\\displaystyle {\\mathcal {A}}} denote the free monoid with generating set A . Let Dist denote the Kleisli category of the Giry monad . Then a functor A → D i s t {\\displaystyle {\\mathcal {A}}\\to \\mathbf {Dist} } encodes both the set S of states and the probability function P . In this way, Markov decision processes could be generalized from monoids (categories with one object) to arbitrary categories. One can call the result ( C , F : C → D i s t ) {\\displaystyle ({\\mathcal {C}},F:{\\mathcal {C}}\\to \\mathbf {Dist} )} a context-dependent Markov decision process , because moving from one object to another in C {\\displaystyle {\\mathcal {C}}} changes the set of available actions and the set of possible states. [ citation needed ] Alternative notations [ edit ] The terminology and notation for MDPs are not entirely settled. There are two main streams — one focuses on maximization problems from contexts like economics, using the terms action, reward, value, and calling the discount factor β or γ , while the other focuses on minimization problems from engineering and navigation [ citation needed ] , using the terms control, cost, cost-to-go, and calling the discount factor α . In addition, the notation for the transition probability varies. in this article alternative comment action a control u reward R cost g g is the negative of R value V cost-to-go J J is the negative of V policy π policy μ discounting factor γ discounting factor α transition probability P a ( s , s ′ ) {\\displaystyle P_{a}(s,s')} transition probability p s s ′ ( a ) {\\displaystyle p_{ss'}(a)} In addition, transition probability is sometimes written Pr ( s , a , s ′ ) {\\displaystyle \\Pr(s,a,s')} , Pr ( s ′ ∣ s , a ) {\\displaystyle \\Pr(s'\\mid s,a)} or, rarely, p s ′ s ( a ) . {\\displaystyle p_{s's}(a).} ^ Puterman, Martin L. (1994). Markov"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
    "title": "Markov decision process",
    "chunk": 6,
    "text": "decision processes: discrete stochastic dynamic programming . Wiley series in probability and mathematical statistics. Applied probability and statistics section. New York: Wiley. ISBN 978-0-471-61977-2 . ^ Schneider, S.; Wagner, D. H. (1957-02-26). \"Error detection in redundant systems\" . Papers presented at the February 26-28, 1957, western joint computer conference: Techniques for reliability on - IRE-AIEE-ACM '57 (Western) . New York, NY, USA: Association for Computing Machinery. pp. 115– 121. doi : 10.1145/1455567.1455587 . ISBN 978-1-4503-7861-1 . ^ Bellman, Richard (1958-09-01). \"Dynamic programming and stochastic control processes\" . Information and Control . 1 (3): 228– 239. doi : 10.1016/S0019-9958(58)80003-0 . ISSN 0019-9958 . ^ a b Sutton, Richard S.; Barto, Andrew G. (2018). Reinforcement learning: an introduction . Adaptive computation and machine learning series (2nd ed.). Cambridge, Massachusetts: The MIT Press. ISBN 978-0-262-03924-6 . ^ Kearns, Michael; Mansour, Yishay; Ng, Andrew (2002). \"A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes\" . Machine Learning . 49 ( 193– 208): 193– 208. doi : 10.1023/A:1017932429737 . ^ Wrobel, A. (1984). \"On Markovian decision models with a finite skeleton\". Zeitschrift für Operations Research . 28 (1): 17– 27. doi : 10.1007/bf01919083 . S2CID 2545336 . ^ Reinforcement Learning: Theory and Python Implementation . Beijing: China Machine Press. 2019. p. 44. ISBN 9787111631774 . ^ Shapley, Lloyd (1953). \"Stochastic Games\" . Proceedings of the National Academy of Sciences of the United States of America . 39 (10): 1095– 1100. Bibcode : 1953PNAS...39.1095S . doi : 10.1073/pnas.39.10.1095 . PMC 1063912 . PMID 16589380 . ^ Kallenberg, Lodewijk (2002). \"Finite state and action MDPs\". In Feinberg, Eugene A. ; Shwartz, Adam (eds.). Handbook of Markov decision processes: methods and applications . Springer. ISBN 978-0-7923-7459-6 . ^ Howard 2002, \"Comments on the Origin and Application of Markov Decision Processes\" ^ Puterman, M. L.; Shin, M. C. (1978). \"Modified Policy Iteration Algorithms for Discounted Markov Decision Problems\". Management Science . 24 (11): 1127– 1137. doi : 10.1287/mnsc.24.11.1127 . ^ van Nunen, J.A. E. E (1976). \"A set of successive approximation methods for discounted Markovian decision problems\". Zeitschrift für Operations Research . 20 (5): 203– 208. doi : 10.1007/bf01920264 . S2CID 5167748 . ^ Papadimitriou, Christos ; Tsitsiklis, John (1987). \"The Complexity of Markov Decision Processes\" . Mathematics of Operations Research . 12 (3): 441– 450. doi : 10.1287/moor.12.3.441 . hdl : 1721.1/2893 . Retrieved November 2, 2023 . ^ Kearns, Michael; Mansour, Yishay; Ng, Andrew (November 2002). \"A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes\" . Machine Learning . 49 (2/3): 193– 208. doi : 10.1023/A:1017932429737 . ^ Altman, Eitan (1999). Constrained Markov decision processes . Vol. 7. CRC Press. ^ Ding, Dongsheng; Zhang, Kaiqing; Jovanovic, Mihailo; Basar, Tamer (2020). Natural policy gradient primal-dual method for constrained Markov decision processes . Advances in Neural Information Processing Systems. ^ Feyzabadi, S.; Carpin, S. (18–22 Aug 2014). \"Risk-aware path planning using hierarchical constrained Markov Decision Processes\" . Automation Science and Engineering (CASE) . IEEE International Conference. pp. 297, 303. ^ Continuous-Time Markov Decision Processes . Stochastic Modelling and Applied Probability. Vol. 62. 2009. doi : 10.1007/978-3-642-02547-1 . ISBN 978-3-642-02546-4 . ^ Shoham, Y.; Powers, R.; Grenager, T. (2003). \"Multi-agent reinforcement learning: a critical survey\" (PDF) . Technical Report, Stanford University : 1– 13 . Retrieved 2018-12-12 . ^ Narendra, K. S. ; Thathachar, M. A. L. (1974). \"Learning Automata – A Survey\". IEEE Transactions on Systems, Man, and Cybernetics . SMC-4 (4): 323– 334. CiteSeerX 10.1.1.295.2280 . doi : 10.1109/TSMC.1974.5408453 . ISSN 0018-9472 . ^ a b Narendra, Kumpati S. ; Thathachar, Mandayam A. L. (1989). Learning automata: An introduction . Prentice Hall. ISBN 9780134855585 . Bellman., R. E. (2003) [1957]. Dynamic Programming (Dover paperback ed.). Princeton, NJ: Princeton University Press. ISBN 978-0-486-42809-3 . Bertsekas, D. (1995). Dynamic Programming and Optimal Control . Vol. 2. MA: Athena. Derman, C. (1970). Finite state Markovian decision processes . Academic Press. Feinberg, E.A.; Shwartz, A., eds. (2002). Handbook of Markov Decision Processes . Boston, MA: Kluwer. ISBN 9781461508052 . Guo, X.; Hernández-Lerma, O. (2009). Continuous-Time Markov Decision Processes . Stochastic Modelling and Applied Probability. Springer. ISBN 9783642025464 . Meyn, S. P. (2007). Control Techniques for Complex Networks . Cambridge University Press. ISBN 978-0-521-88441-9 . Archived from the original on 19 June 2010. Appendix contains abridged \"Meyn & Tweedie\" . Archived from the original on 18 December 2012. Puterman., M. L. (1994). Markov Decision Processes . Wiley. Ross, S. M. (1983). Introduction to stochastic dynamic programming (PDF) . Academic press. Sutton, R. S.; Barto, A. G. (2017). Reinforcement Learning: An Introduction . Cambridge, MA: The MIT Press. Tijms., H.C. (2003). A First Course in Stochastic Models . Wiley. ISBN 9780470864289 ."
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 0,
    "text": "[Updated on 2020-09-03: Updated the algorithm of SARSA and Q-learning so that the difference is more pronounced. [Updated on 2021-09-19: Thanks to 爱吃猫的鱼, we have this post in Chinese ]. A couple of exciting news in Artificial Intelligence (AI) has just happened in recent years. AlphaGo defeated the best professional human player in the game of Go. Very soon the extended algorithm AlphaGo Zero beat AlphaGo by 100-0 without supervised learning on human knowledge. Top professional game players lost to the bot developed by OpenAI on DOTA2 1v1 competition. After knowing these, it is pretty hard not to be curious about the magic behind these algorithms — Reinforcement Learning (RL). I’m writing this post to briefly go over the field. We will first introduce several fundamental concepts and then dive into classic approaches to solving RL problems. Hopefully, this post could be a good starting point for newbies, bridging the future study on the cutting-edge research. What is Reinforcement Learning? # Say, we have an agent in an unknown environment and this agent can obtain some rewards by interacting with the environment. The agent ought to take actions so as to maximize cumulative rewards. In reality, the scenario could be a bot playing a game to achieve high scores, or a robot trying to complete physical tasks with physical items; and not just limited to these. An agent interacts with the environment, trying to take smart actions to maximize cumulative rewards. The goal of Reinforcement Learning (RL) is to learn a good strategy for the agent from experimental trials and relative simple feedback received. With the optimal strategy, the agent is capable to actively adapt to the environment to maximize future rewards. Key Concepts # Now Let’s formally define a set of key concepts in RL. The agent is acting in an environment . How the environment reacts to certain actions is defined by a model which we may or may not know. The agent can stay in one of many states ($s \\in \\mathcal{S}$) of the environment, and choose to take one of many actions ($a \\in \\mathcal{A}$) to switch from one state to another. Which state the agent will arrive in is decided by transition probabilities between states ($P$). Once an action is taken, the environment delivers a reward ($r \\in \\mathcal{R}$) as feedback. The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances: Know the model : planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by Dynamic Programming (DP). Do you still remember “longest increasing subsequence” or “traveling salesmen problem” from your Algorithms 101 class? LOL. This is not the focus of this post though. Does not know the model : learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm. Most of the following content serves the scenarios when the model is unknown. The agent’s policy $\\pi(s)$ provides the guideline on what is the optimal action to take in a certain state with the goal to maximize the total rewards . Each state is associated with a value function $V(s)$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy. In other words, the value function quantifies how good a state is. Both policy and value functions are what we try to learn in reinforcement learning. Summary of approaches in RL based on whether we want to model the value, policy, or the environment. (Image source: reproduced from David Silver's RL course lecture 1 .) The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, $t=1, 2, \\dots, T$. During the process, the agent accumulates the knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy. Let’s label the state, action, and reward at time step t as $S_t$, $A_t$, and $R_t$, respectively. Thus the interaction sequence is fully described by one episode (also known as “trial” or “trajectory”) and the sequence ends at the terminal state $S_T$: $$ S_1, A_1, R_2, S_2, A_2, \\dots, S_T $$ Terms you will encounter a lot when diving into different categories of RL algorithms: Model-based : Rely on the model of the environment; either the model is known or the algorithm learns it explicitly. Model-free : No dependency on the model during learning. On-policy : Use the deterministic outcomes or samples from the target policy to train the algorithm. Off-policy : Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 1,
    "text": "policy. Model: Transition and Reward # The model is a descriptor of the environment. With the model, we can learn or infer how the environment would interact with and provide feedback to the agent. The model has two major parts, transition probability function $P$ and reward function $R$. Let’s say when we are in state s, we decide to take action a to arrive in the next state s’ and obtain reward r. This is known as one transition step, represented by a tuple (s, a, s’, r). The transition function P records the probability of transitioning from state s to s’ after taking action a while obtaining reward r. We use $\\mathbb{P}$ as a symbol of “probability”. $$ P(s', r \\vert s, a) = \\mathbb{P} [S_{t+1} = s', R_{t+1} = r \\vert S_t = s, A_t = a] $$ Thus the state-transition function can be defined as a function of $P(s’, r \\vert s, a)$: $$ P_{ss'}^a = P(s' \\vert s, a) = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a] = \\sum_{r \\in \\mathcal{R}} P(s', r \\vert s, a) $$ The reward function R predicts the next reward triggered by one action: $$ R(s, a) = \\mathbb{E} [R_{t+1} \\vert S_t = s, A_t = a] = \\sum_{r\\in\\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} P(s', r \\vert s, a) $$ Policy # Policy, as the agent’s behavior function $\\pi$, tells us which action to take in state s. It is a mapping from state s to action a and can be either deterministic or stochastic: Deterministic: $\\pi(s) = a$. Stochastic: $\\pi(a \\vert s) = \\mathbb{P}_\\pi [A=a \\vert S=s]$. Value Function # Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of future reward. The future reward, also known as return , is a total sum of discounted rewards going forward. Let’s compute the return $G_t$ starting from time t: $$ G_t = R_{t+1} + \\gamma R_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$ The discounting factor $\\gamma \\in [0, 1]$ penalize the rewards in the future, because: The future rewards may have higher uncertainty; i.e. stock market. The future rewards do not provide immediate benefits; i.e. As human beings, we might prefer to have fun today rather than 5 years later ;). Discounting provides mathematical convenience; i.e., we don’t need to track future steps forever to compute return. We don’t need to worry about the infinite loops in the state transition graph. The state-value of a state s is the expected return if we are in this state at time t, $S_t = s$: $$ V_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\vert S_t = s] $$ Similarly, we define the action-value (“Q-value”; Q as “Quality” I believe?) of a state-action pair as: $$ Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a] $$ Additionally, since we follow the target policy $\\pi$, we can make use of the probility distribution over possible actions and the Q-values to recover the state-value: $$ V_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} Q_{\\pi}(s, a) \\pi(a \\vert s) $$ The difference between action-value and state-value is the action advantage function (“A-value”): $$ A_{\\pi}(s, a) = Q_{\\pi}(s, a) - V_{\\pi}(s) $$ Optimal Value and Policy # The optimal value function produces the maximum return: $$ V_{*}(s) = \\max_{\\pi} V_{\\pi}(s), Q_{*}(s, a) = \\max_{\\pi} Q_{\\pi}(s, a) $$ The optimal policy achieves optimal value functions: $$ \\pi_{*} = \\arg\\max_{\\pi} V_{\\pi}(s), \\pi_{*} = \\arg\\max_{\\pi} Q_{\\pi}(s, a) $$ And of course, we have $V_{\\pi_{*}}(s)=V_{*}(s)$ and $Q_{\\pi_{*}}(s, a) = Q_{*}(s, a)$. Markov Decision Processes # In more formal terms, almost all the RL problems can be framed as Markov Decision Processes (MDPs). All states in MDP has “Markov” property, referring to the fact that the future only depends on the current state, not the history: $$ \\mathbb{P}[ S_{t+1} \\vert S_t ] = \\mathbb{P} [S_{t+1} \\vert S_1, \\dots, S_t] $$ Or in other words, the future and the past are conditionally independent given the present, as the current state encapsulates all the statistics we need to decide the future. The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton & Barto (2017).) A Markov deicison process consists of five elements $\\mathcal{M} = \\langle \\mathcal{S}, \\mathcal{A}, P, R, \\gamma \\rangle$, where the symbols carry the same meanings as key concepts in the previous section, well aligned with RL problem settings: $\\mathcal{S}$ - a set of states; $\\mathcal{A}$ - a set of actions; $P$ - transition probability function; $R$ - reward function; $\\gamma$ - discounting factor for future rewards. In an unknown environment, we do not have perfect knowledge about $P$ and $R$. A fun example of Markov decision process: a typical work day. (Image source: randomant.net/reinforcement-learning-concepts ) Bellman Equations # Bellman equations refer to a set of equations that decompose the"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 2,
    "text": "value function into the immediate reward plus the discounted future values. $$ \\begin{aligned} V(s) &= \\mathbb{E}[G_t \\vert S_t = s] \\\\ &= \\mathbb{E} [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\vert S_t = s] \\\\ &= \\mathbb{E} [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\dots) \\vert S_t = s] \\\\ &= \\mathbb{E} [R_{t+1} + \\gamma G_{t+1} \\vert S_t = s] \\\\ &= \\mathbb{E} [R_{t+1} + \\gamma V(S_{t+1}) \\vert S_t = s] \\end{aligned} $$ Similarly for Q-value, $$ \\begin{aligned} Q(s, a) &= \\mathbb{E} [R_{t+1} + \\gamma V(S_{t+1}) \\mid S_t = s, A_t = a] \\\\ &= \\mathbb{E} [R_{t+1} + \\gamma \\mathbb{E}_{a\\sim\\pi} Q(S_{t+1}, a) \\mid S_t = s, A_t = a] \\end{aligned} $$ Bellman Expectation Equations # The recursive update process can be further decomposed to be equations built on both state-value and action-value functions. As we go further in future action steps, we extend V and Q alternatively by following the policy $\\pi$. Illustration of how Bellman expection equations update state-value and action-value functions. $$ \\begin{aligned} V_{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s) Q_{\\pi}(s, a) \\\\ Q_{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_{\\pi} (s') \\\\ V_{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s) \\big( R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_{\\pi} (s') \\big) \\\\ Q_{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\vert s') Q_{\\pi} (s', a') \\end{aligned} $$ Bellman Optimality Equations # If we are only interested in the optimal values, rather than computing the expectation following a policy, we could jump right into the maximum returns during the alternative updates without using a policy. RECAP: the optimal values $V_*$ and $Q_*$ are the best returns we can obtain, defined here . $$ \\begin{aligned} V_*(s) &= \\max_{a \\in \\mathcal{A}} Q_*(s,a)\\\\ Q_*(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_*(s') \\\\ V_*(s) &= \\max_{a \\in \\mathcal{A}} \\big( R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_*(s') \\big) \\\\ Q_*(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a \\max_{a' \\in \\mathcal{A}} Q_*(s', a') \\end{aligned} $$ Unsurprisingly they look very similar to Bellman expectation equations. If we have complete information of the environment, this turns into a planning problem, solvable by DP. Unfortunately, in most scenarios, we do not know $P_{ss’}^a$ or $R(s, a)$, so we cannot solve MDPs by directly applying Bellmen equations, but it lays the theoretical foundation for many RL algorithms. Common Approaches # Now it is the time to go through the major approaches and classic algorithms for solving RL problems. In future posts, I plan to dive into each approach further. Dynamic Programming # When the model is fully known, following Bellman equations, we can use Dynamic Programming (DP) to iteratively evaluate value functions and improve policy. Policy Evaluation # Policy Evaluation is to compute the state-value $V_\\pi$ for a given policy $\\pi$: $$ V_{t+1}(s) = \\mathbb{E}_\\pi [r + \\gamma V_t(s') | S_t = s] = \\sum_a \\pi(a \\vert s) \\sum_{s', r} P(s', r \\vert s, a) (r + \\gamma V_t(s')) $$ Policy Improvement # Based on the value functions, Policy Improvement generates a better policy $\\pi’ \\geq \\pi$ by acting greedily. $$ Q_\\pi(s, a) = \\mathbb{E} [R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\vert S_t=s, A_t=a] = \\sum_{s', r} P(s', r \\vert s, a) (r + \\gamma V_\\pi(s')) $$ Policy Iteration # The Generalized Policy Iteration (GPI) algorithm refers to an iterative procedure to improve the policy when combining policy evaluation and improvement. $$ \\pi_0 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_0} \\xrightarrow[]{\\text{improve}} \\pi_1 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_1} \\xrightarrow[]{\\text{improve}} \\pi_2 \\xrightarrow[]{\\text{evaluation}} \\dots \\xrightarrow[]{\\text{improve}} \\pi_* \\xrightarrow[]{\\text{evaluation}} V_* $$ In GPI, the value function is approximated repeatedly to be closer to the true value of the current policy and in the meantime, the policy is improved repeatedly to approach optimality. This policy iteration process works and always converges to the optimality, but why this is the case? Say, we have a policy $\\pi$ and then generate an improved version $\\pi’$ by greedily taking actions, $\\pi’(s) = \\arg\\max_{a \\in \\mathcal{A}} Q_\\pi(s, a)$. The value of this improved $\\pi’$ is guaranteed to be better because: $$ \\begin{aligned} Q_\\pi(s, \\pi'(s)) &= Q_\\pi(s, \\arg\\max_{a \\in \\mathcal{A}} Q_\\pi(s, a)) \\\\ &= \\max_{a \\in \\mathcal{A}} Q_\\pi(s, a) \\geq Q_\\pi(s, \\pi(s)) = V_\\pi(s) \\end{aligned} $$ Monte-Carlo Methods # First, let’s recall that $V(s) = \\mathbb{E}[ G_t \\vert S_t=s]$. Monte-Carlo (MC) methods uses a simple idea: It learns from episodes of raw experience without modeling the environmental dynamics and computes the observed mean return as an approximation of the expected return. To compute the empirical return $G_t$, MC methods need to learn from complete episodes $S_1, A_1, R_2, \\dots, S_T$ to compute $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$ and all the episodes must eventually terminate. The empirical mean return for state s is: $$ V(s) = \\frac{\\sum_{t=1}^T \\mathbb{1}[S_t = s] G_t}{\\sum_{t=1}^T \\mathbb{1}[S_t = s]} $$"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 3,
    "text": "where $\\mathbb{1}[S_t = s]$ is a binary indicator function. We may count the visit of state s every time so that there could exist multiple visits of one state in one episode (“every-visit”), or only count it the first time we encounter a state in one episode (“first-visit”). This way of approximation can be easily extended to action-value functions by counting (s, a) pair. $$ Q(s, a) = \\frac{\\sum_{t=1}^T \\mathbb{1}[S_t = s, A_t = a] G_t}{\\sum_{t=1}^T \\mathbb{1}[S_t = s, A_t = a]} $$ To learn the optimal policy by MC, we iterate it by following a similar idea to GPI . Improve the policy greedily with respect to the current value function: $\\pi(s) = \\arg\\max_{a \\in \\mathcal{A}} Q(s, a)$. Generate a new episode with the new policy $\\pi$ (i.e. using algorithms like ε-greedy helps us balance between exploitation and exploration.) Estimate Q using the new episode: $q_\\pi(s, a) = \\frac{\\sum_{t=1}^T \\big( \\mathbb{1}[S_t = s, A_t = a] \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1} \\big)}{\\sum_{t=1}^T \\mathbb{1}[S_t = s, A_t = a]}$ Temporal-Difference Learning # Similar to Monte-Carlo methods, Temporal-Difference (TD) Learning is model-free and learns from episodes of experience. However, TD learning can learn from incomplete episodes and hence we don’t need to track the episode up to termination. TD learning is so important that Sutton & Barto (2017) in their RL book describes it as “one idea … central and novel to reinforcement learning”. Bootstrapping # TD learning methods update targets with regard to existing estimates rather than exclusively relying on actual rewards and complete returns as in MC methods. This approach is known as bootstrapping . Value Estimation # The key idea in TD learning is to update the value function $V(S_t)$ towards an estimated return $R_{t+1} + \\gamma V(S_{t+1})$ (known as “ TD target ”). To what extent we want to update the value function is controlled by the learning rate hyperparameter α: $$ \\begin{aligned} V(S_t) &\\leftarrow (1- \\alpha) V(S_t) + \\alpha G_t \\\\ V(S_t) &\\leftarrow V(S_t) + \\alpha (G_t - V(S_t)) \\\\ V(S_t) &\\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)) \\end{aligned} $$ Similarly, for action-value estimation: $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) $$ Next, let’s dig into the fun part on how to learn optimal policy in TD learning (aka “TD control”). Be prepared, you are gonna see many famous names of classic algorithms in this section. SARSA: On-Policy TD control # “SARSA” refers to the procedure of updaing Q-value by following a sequence of $\\dots, S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \\dots$. The idea follows the same route of GPI . Within one episode, it works as follows: Initialize $t=0$. Start with $S_0$ and choose action $A_0 = \\arg\\max_{a \\in \\mathcal{A}} Q(S_0, a)$, where $\\epsilon$-greedy is commonly applied. At time $t$, after applying action $A_t$, we observe reward $R_{t+1}$ and get into the next state $S_{t+1}$. Then pick the next action in the same way as in step 2: $A_{t+1} = \\arg\\max_{a \\in \\mathcal{A}} Q(S_{t+1}, a)$. Update the Q-value function: $ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) $. Set $t = t+1$ and repeat from step 3. In each step of SARSA, we need to choose the next action according to the current policy. Q-Learning: Off-policy TD control # The development of Q-learning ( Watkins & Dayan, 1992 ) is a big breakout in the early days of Reinforcement Learning. Within one episode, it works as follows: Initialize $t=0$. Starts with $S_0$. At time step $t$, we pick the action according to Q values, $A_t = \\arg\\max_{a \\in \\mathcal{A}} Q(S_t, a)$ and $\\epsilon$-greedy is commonly applied. After applying action $A_t$, we observe reward $R_{t+1}$ and get into the next state $S_{t+1}$. Update the Q-value function: $Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t))$. $t = t+1$ and repeat from step 3. The key difference from SARSA is that Q-learning does not follow the current policy to pick the second action $A_{t+1}$. It estimates $Q^*$ out of the best Q values, but which action (denoted as $a^*$) leads to this maximal Q does not matter and in the next step Q-learning may not follow $a^*$. The backup diagrams for Q-learning and SARSA. (Image source: Replotted based on Figure 6.5 in Sutton & Barto (2017)) Deep Q-Network # Theoretically, we can memorize $Q_*(.)$ for all state-action pairs in Q-learning, like in a gigantic table. However, it quickly becomes computationally infeasible when the state and action space are large. Thus people use functions (i.e. a machine learning model) to approximate Q values and this is called function approximation . For example, if we use a function with parameter $\\theta$ to calculate Q values, we can label Q value function as $Q(s,"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 4,
    "text": "a; \\theta)$. Unfortunately Q-learning may suffer from instability and divergence when combined with an nonlinear Q-value function approximation and bootstrapping (See Problems #2 ). Deep Q-Network (“DQN”; Mnih et al. 2015) aims to greatly improve and stabilize the training procedure of Q-learning by two innovative mechanisms: Experience Replay : All the episode steps $e_t = (S_t, A_t, R_t, S_{t+1})$ are stored in one replay memory $D_t = \\{ e_1, \\dots, e_t \\}$. $D_t$ has experience tuples over many episodes. During Q-learning updates, samples are drawn at random from the replay memory and thus one sample could be used multiple times. Experience replay improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution. Periodically Updated Target : Q is optimized towards target values that are only periodically updated. The Q network is cloned and kept frozen as the optimization target every C steps (C is a hyperparameter). This modification makes the training more stable as it overcomes the short-term oscillations. The loss function looks like this: $$ \\mathcal{L}(\\theta) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\Big[ \\big( r + \\gamma \\max_{a'} Q(s', a'; \\theta^{-}) - Q(s, a; \\theta) \\big)^2 \\Big] $$ where $U(D)$ is a uniform distribution over the replay memory D; $\\theta^{-}$ is the parameters of the frozen target Q-network. In addition, it is also found to be helpful to clip the error term to be between [-1, 1]. (I always get mixed feeling with parameter clipping, as many studies have shown that it works empirically but it makes the math much less pretty. :/) Algorithm for DQN with experience replay and occasionally frozen optimization target. The prepossessed sequence is the output of some processes running on the input images of Atari games. Don't worry too much about it; just consider them as input feature vectors. (Image source: Mnih et al. 2015) There are many extensions of DQN to improve the original design, such as DQN with dueling architecture (Wang et al. 2016) which estimates state-value function V(s) and advantage function A(s, a) with shared network parameters. Combining TD and MC Learning # In the previous section on value estimation in TD learning, we only trace one step further down the action chain when calculating the TD target. One can easily extend it to take multiple steps to estimate the return. Let’s label the estimated return following n steps as $G_t^{(n)}, n=1, \\dots, \\infty$, then: $n$ $G_t$ Notes $n=1$ $G_t^{(1)} = R_{t+1} + \\gamma V(S_{t+1})$ TD learning $n=2$ $G_t^{(2)} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})$ … $n=n$ $ G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n}) $ … $n=\\infty$ $G_t^{(\\infty)} = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-t-1} R_T + \\gamma^{T-t} V(S_T) $ MC estimation The generalized n-step TD learning still has the same form for updating the value function: $$ V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^{(n)} - V(S_t)) $$ We are free to pick any $n$ in TD learning as we like. Now the question becomes what is the best $n$? Which $G_t^{(n)}$ gives us the best return approximation? A common yet smart solution is to apply a weighted sum of all possible n-step TD targets rather than to pick a single best n. The weights decay by a factor λ with n, $\\lambda^{n-1}$; the intuition is similar to why we want to discount future rewards when computing the return: the more future we look into the less confident we would be. To make all the weight (n → ∞) sum up to 1, we multiply every weight by (1-λ), because: $$ \\begin{aligned} \\text{let } S &= 1 + \\lambda + \\lambda^2 + \\dots \\\\ S &= 1 + \\lambda(1 + \\lambda + \\lambda^2 + \\dots) \\\\ S &= 1 + \\lambda S \\\\ S &= 1 / (1-\\lambda) \\end{aligned} $$ This weighted sum of many n-step returns is called λ-return $G_t^{\\lambda} = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t^{(n)}$. TD learning that adopts λ-return for value updating is labeled as TD(λ) . The original version we introduced above is equivalent to TD(0) . Comparison of the backup diagrams of Monte-Carlo, Temporal-Difference learning, and Dynamic Programming for state value functions. (Image source: David Silver's RL course lecture 4 : \"Model-Free Prediction\") Policy Gradient # All the methods we have introduced above aim to learn the state/action value function and then to select actions accordingly. Policy Gradient methods instead learn the policy directly with a parameterized function respect to $\\theta$, $\\pi(a \\vert s; \\theta)$. Let’s define the reward function (opposite of loss function) as the expected return and train the algorithm with the goal to maximize the reward function. My next post described why the policy gradient theorem works (proof) and introduced a number of policy gradient algorithms. In discrete space: $$ \\mathcal{J}(\\theta) = V_{\\pi_\\theta}(S_1) = \\mathbb{E}_{\\pi_\\theta}[V_1]"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 5,
    "text": "$$ where $S_1$ is the initial starting state. Or in continuous space: $$ \\mathcal{J}(\\theta) = \\sum_{s \\in \\mathcal{S}} d_{\\pi_\\theta}(s) V_{\\pi_\\theta}(s) = \\sum_{s \\in \\mathcal{S}} \\Big( d_{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s, \\theta) Q_\\pi(s, a) \\Big) $$ where $d_{\\pi_\\theta}(s)$ is stationary distribution of Markov chain for $\\pi_\\theta$. If you are unfamiliar with the definition of a “stationary distribution,” please check this reference . Using gradient ascent we can find the best θ that produces the highest return. It is natural to expect policy-based methods are more useful in continuous space, because there is an infinite number of actions and/or states to estimate the values for in continuous space and hence value-based approaches are computationally much more expensive. Policy Gradient Theorem # Computing the gradient numerically can be done by perturbing θ by a small amount ε in the k-th dimension. It works even when $J(\\theta)$ is not differentiable (nice!), but unsurprisingly very slow. $$ \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial \\theta_k} \\approx \\frac{\\mathcal{J}(\\theta + \\epsilon u_k) - \\mathcal{J}(\\theta)}{\\epsilon} $$ Or analytically , $$ \\mathcal{J}(\\theta) = \\mathbb{E}_{\\pi_\\theta} [r] = \\sum_{s \\in \\mathcal{S}} d_{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) R(s, a) $$ Actually we have nice theoretical support for (replacing $d(.)$ with $d_\\pi(.)$): $$ \\mathcal{J}(\\theta) = \\sum_{s \\in \\mathcal{S}} d_{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) Q_\\pi(s, a) \\propto \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) Q_\\pi(s, a) $$ Check Sec 13.1 in Sutton & Barto (2017) for why this is the case. Then, $$ \\begin{aligned} \\mathcal{J}(\\theta) &= \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) Q_\\pi(s, a) \\\\ \\nabla \\mathcal{J}(\\theta) &= \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\nabla \\pi(a \\vert s; \\theta) Q_\\pi(s, a) \\\\ &= \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) \\frac{\\nabla \\pi(a \\vert s; \\theta)}{\\pi(a \\vert s; \\theta)} Q_\\pi(s, a) \\\\ & = \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) \\nabla \\ln \\pi(a \\vert s; \\theta) Q_\\pi(s, a) \\\\ & = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\ln \\pi(a \\vert s; \\theta) Q_\\pi(s, a)] \\end{aligned} $$ This result is named “Policy Gradient Theorem” which lays the theoretical foundation for various policy gradient algorithms: $$ \\nabla \\mathcal{J}(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\ln \\pi(a \\vert s, \\theta) Q_\\pi(s, a)] $$ REINFORCE # REINFORCE, also known as Monte-Carlo policy gradient, relies on $Q_\\pi(s, a)$, an estimated return by MC methods using episode samples, to update the policy parameter $\\theta$. A commonly used variation of REINFORCE is to subtract a baseline value from the return $G_t$ to reduce the variance of gradient estimation while keeping the bias unchanged. For example, a common baseline is state-value, and if applied, we would use $A(s, a) = Q(s, a) - V(s)$ in the gradient ascent update. Initialize θ at random Generate one episode $S_1, A_1, R_2, S_2, A_2, \\dots, S_T$ For t=1, 2, … , T: Estimate the the return G_t since the time step t. $\\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla \\ln \\pi(A_t \\vert S_t, \\theta)$. Actor-Critic # If the value function is learned in addition to the policy, we would get Actor-Critic algorithm. Critic : updates value function parameters w and depending on the algorithm it could be action-value $Q(a \\vert s; w)$ or state-value $V(s; w)$. Actor : updates policy parameters θ, in the direction suggested by the critic, $\\pi(a \\vert s; \\theta)$. Let’s see how it works in an action-value actor-critic algorithm. Initialize s, θ, w at random; sample $a \\sim \\pi(a \\vert s; \\theta)$. For t = 1… T: Sample reward $r_t \\sim R(s, a)$ and next state $s’ \\sim P(s’ \\vert s, a)$. Then sample the next action $a’ \\sim \\pi(s’, a’; \\theta)$. Update policy parameters: $\\theta \\leftarrow \\theta + \\alpha_\\theta Q(s, a; w) \\nabla_\\theta \\ln \\pi(a \\vert s; \\theta)$. Compute the correction for action-value at time t: $G_{t:t+1} = r_t + \\gamma Q(s’, a’; w) - Q(s, a; w)$ and use it to update value function parameters: $w \\leftarrow w + \\alpha_w G_{t:t+1} \\nabla_w Q(s, a; w) $. Update $a \\leftarrow a’$ and $s \\leftarrow s’$. $\\alpha_\\theta$ and $\\alpha_w$ are two learning rates for policy and value function parameter updates, respectively. A3C # Asynchronous Advantage Actor-Critic (Mnih et al., 2016), short for A3C, is a classic policy gradient method with the special focus on parallel training. In A3C, the critics learn the state-value function, $V(s; w)$, while multiple actors are trained in parallel and get synced with global parameters from time to time. Hence, A3C is good for parallel training by default, i.e. on one machine with multi-core CPU. The loss function for state-value is to minimize the mean squared error, $\\mathcal{J}_v (w) = (G_t - V(s; w))^2$ and we use gradient descent to find the optimal w. This state-value function is used as the baseline in the policy gradient update. Here is the algorithm outline:"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 6,
    "text": "We have global parameters, θ and w; similar thread-specific parameters, θ’ and w'. Initialize the time step t = 1 While T <= T_MAX: Reset gradient: dθ = 0 and dw = 0. Synchronize thread-specific parameters with global ones: θ’ = θ and w’ = w. $t_\\text{start}$ = t and get $s_t$. While ($s_t \\neq \\text{TERMINAL}$) and ($t - t_\\text{start} <= t_\\text{max}$): Pick the action $a_t \\sim \\pi(a_t \\vert s_t; \\theta’)$ and receive a new reward $r_t$ and a new state $s_{t+1}$. Update t = t + 1 and T = T + 1. Initialize the variable that holds the return estimation $$R = \\begin{cases} 0 & \\text{if } s_t \\text{ is TERMINAL} \\ V(s_t; w’) & \\text{otherwise} \\end{cases}$$. For $i = t-1, \\dots, t_\\text{start}$: $R \\leftarrow r_i + \\gamma R$; here R is a MC measure of $G_i$. Accumulate gradients w.r.t. θ’: $d\\theta \\leftarrow d\\theta + \\nabla_{\\theta’} \\log \\pi(a_i \\vert s_i; \\theta’)(R - V(s_i; w’))$; Accumulate gradients w.r.t. w’: $dw \\leftarrow dw + \\nabla_{w’} (R - V(s_i; w’))^2$. Update synchronously θ using dθ, and w using dw. A3C enables the parallelism in multiple agent training. The gradient accumulation step (6.2) can be considered as a reformation of minibatch-based stochastic gradient update: the values of w or θ get corrected by a little bit in the direction of each training thread independently. Evolution Strategies # Evolution Strategies (ES) is a type of model-agnostic optimization approach. It learns the optimal solution by imitating Darwin’s theory of the evolution of species by natural selection. Two prerequisites for applying ES: (1) our solutions can freely interact with the environment and see whether they can solve the problem; (2) we are able to compute a fitness score of how good each solution is. We don’t have to know the environment configuration to solve the problem. Say, we start with a population of random solutions. All of them are capable of interacting with the environment and only candidates with high fitness scores can survive ( only the fittest can survive in a competition for limited resources ). A new generation is then created by recombining the settings ( gene mutation ) of high-fitness survivors. This process is repeated until the new solutions are good enough. Very different from the popular MDP-based approaches as what we have introduced above, ES aims to learn the policy parameter $\\theta$ without value approximation. Let’s assume the distribution over the parameter $\\theta$ is an isotropic multivariate Gaussian with mean $\\mu$ and fixed covariance $\\sigma^2I$. The gradient of $F(\\theta)$ is calculated: $$ \\begin{aligned} & \\nabla_\\theta \\mathbb{E}_{\\theta \\sim N(\\mu, \\sigma^2)} F(\\theta) \\\\ =& \\nabla_\\theta \\int_\\theta F(\\theta) \\Pr(\\theta) && \\text{Pr(.) is the Gaussian density function.} \\\\ =& \\int_\\theta F(\\theta) \\Pr(\\theta) \\frac{\\nabla_\\theta \\Pr(\\theta)}{\\Pr(\\theta)} \\\\ =& \\int_\\theta F(\\theta) \\Pr(\\theta) \\nabla_\\theta \\log \\Pr(\\theta) \\\\ =& \\mathbb{E}_{\\theta \\sim N(\\mu, \\sigma^2)} [F(\\theta) \\nabla_\\theta \\log \\Pr(\\theta)] && \\text{Similar to how we do policy gradient update.} \\\\ =& \\mathbb{E}_{\\theta \\sim N(\\mu, \\sigma^2)} \\Big[ F(\\theta) \\nabla_\\theta \\log \\Big( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(\\theta - \\mu)^2}{2 \\sigma^2 }} \\Big) \\Big] \\\\ =& \\mathbb{E}_{\\theta \\sim N(\\mu, \\sigma^2)} \\Big[ F(\\theta) \\nabla_\\theta \\Big( -\\log \\sqrt{2\\pi\\sigma^2} - \\frac{(\\theta - \\mu)^2}{2 \\sigma^2} \\Big) \\Big] \\\\ =& \\mathbb{E}_{\\theta \\sim N(\\mu, \\sigma^2)} \\Big[ F(\\theta) \\frac{\\theta - \\mu}{\\sigma^2} \\Big] \\end{aligned} $$ We can rewrite this formula in terms of a “mean” parameter $\\theta$ (different from the $\\theta$ above; this $\\theta$ is the base gene for further mutation), $\\epsilon \\sim N(0, I)$ and therefore $\\theta + \\epsilon \\sigma \\sim N(\\theta, \\sigma^2)$. $\\epsilon$ controls how much Gaussian noises should be added to create mutation: $$ \\nabla_\\theta \\mathbb{E}_{\\epsilon \\sim N(0, I)} F(\\theta + \\sigma \\epsilon) = \\frac{1}{\\sigma} \\mathbb{E}_{\\epsilon \\sim N(0, I)} [F(\\theta + \\sigma \\epsilon) \\epsilon] $$ A simple parallel evolution-strategies-based RL algorithm. Parallel workers share the random seeds so that they can reconstruct the Gaussian noises with tiny communication bandwidth. (Image source: Salimans et al. 2017.) ES, as a black-box optimization algorithm, is another approach to RL problems ( In my original writing, I used the phrase “a nice alternative”; Seita pointed me to this discussion and thus I updated my wording. ). It has a couple of good characteristics (Salimans et al., 2017) keeping it fast and easy to train: ES does not need value function approximation; ES does not perform gradient back-propagation; ES is invariant to delayed or long-term rewards; ES is highly parallelizable with very little data communication. Known Problems # Exploration-Exploitation Dilemma # The problem of exploration vs exploitation dilemma has been discussed in my previous post . When the RL problem faces an unknown environment, this issue is especially a key to finding a good solution: without enough exploration, we cannot learn the environment well enough; without enough exploitation, we cannot complete our reward optimization task. Different RL algorithms balance between exploration and exploitation in different ways. In MC methods, Q-learning or many on-policy algorithms, the exploration is commonly implemented"
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 7,
    "text": "by ε-greedy ; In ES , the exploration is captured by the policy parameter perturbation. Please keep this into consideration when developing a new RL algorithm. Deadly Triad Issue # We do seek the efficiency and flexibility of TD methods that involve bootstrapping. However, when off-policy, nonlinear function approximation, and bootstrapping are combined in one RL algorithm, the training could be unstable and hard to converge. This issue is known as the deadly triad (Sutton & Barto, 2017). Many architectures using deep learning models were proposed to resolve the problem, including DQN to stabilize the training with experience replay and occasionally frozen target network. Case Study: AlphaGo Zero # The game of Go has been an extremely hard problem in the field of Artificial Intelligence for decades until recent years. AlphaGo and AlphaGo Zero are two programs developed by a team at DeepMind. Both involve deep Convolutional Neural Networks ( CNN ) and Monte Carlo Tree Search (MCTS) and both have been approved to achieve the level of professional human Go players. Different from AlphaGo that relied on supervised learning from expert human moves, AlphaGo Zero used only reinforcement learning and self-play without human knowledge beyond the basic rules. The board of Go. Two players play black and white stones alternatively on the vacant intersections of a board with 19 x 19 lines. A group of stones must have at least one open point (an intersection, called a \"liberty\") to remain on the board and must have at least two or more enclosed liberties (called \"eyes\") to stay \"alive\". No stone shall repeat a previous position. With all the knowledge of RL above, let’s take a look at how AlphaGo Zero works. The main component is a deep CNN over the game board configuration (precisely, a ResNet with batch normalization and ReLU). This network outputs two values: $$ (p, v) = f_\\theta(s) $$ $s$: the game board configuration, 19 x 19 x 17 stacked feature planes; 17 features for each position, 8 past configurations (including current) for the current player + 8 past configurations for the opponent + 1 feature indicating the color (1=black, 0=white). We need to code the color specifically because the network is playing with itself and the colors of current player and opponents are switching between steps. $p$: the probability of selecting a move over 19^2 + 1 candidates (19^2 positions on the board, in addition to passing). $v$: the winning probability given the current setting. During self-play, MCTS further improves the action probability distribution $\\pi \\sim p(.)$ and then the action $a_t$ is sampled from this improved policy. The reward $z_t$ is a binary value indicating whether the current player eventually wins the game. Each move generates an episode tuple $(s_t, \\pi_t, z_t)$ and it is saved into the replay memory. The details on MCTS are skipped for the sake of space in this post; please read the original paper if you are interested. AlphaGo Zero is trained by self-play while MCTS improves the output policy further in every step. (Image source: Figure 1a in Silver et al., 2017). The network is trained with the samples in the replay memory to minimize the loss: $$ \\mathcal{L} = (z - v)^2 - \\pi^\\top \\log p + c \\| \\theta \\|^2 $$ where $c$ is a hyperparameter controlling the intensity of L2 penalty to avoid overfitting. AlphaGo Zero simplified AlphaGo by removing supervised learning and merging separated policy and value networks into one. It turns out that AlphaGo Zero achieved largely improved performance with a much shorter training time! I strongly recommend reading these two papers side by side and compare the difference, super fun. I know this is a long read, but hopefully worth it. If you notice mistakes and errors in this post, don’t hesitate to contact me at [lilian dot wengweng at gmail dot com]. See you in the next post! :) Cited as: @article{weng2018bandit, title = \"A (Long) Peek into Reinforcement Learning\", author = \"Weng, Lilian\", journal = \"lilianweng.github.io\", year = \"2018\", url = \"https://lilianweng.github.io/posts/2018-02-19-rl-overview/\" } References # [1] Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274. 2017. [2] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition . 2017. [3] Volodymyr Mnih, et al. Asynchronous methods for deep reinforcement learning. ICML. 2016. [4] Tim Salimans, et al. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864 (2017). [5] David Silver, et al. Mastering the game of go without human knowledge . Nature 550.7676 (2017): 354. [6] David Silver, et al. Mastering the game of Go with deep neural networks and tree search. Nature 529.7587 (2016): 484-489. [7] Volodymyr Mnih, et al. Human-level control through deep reinforcement learning. Nature 518.7540 (2015): 529. [8] Ziyu Wang, et al. Dueling network architectures for deep reinforcement learning."
  },
  {
    "type": "doc",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "title": "A (Long) Peek into Reinforcement Learning",
    "chunk": 8,
    "text": "ICML. 2016. [9] Reinforcement Learning lectures by David Silver on YouTube. [10] OpenAI Blog: Evolution Strategies as a Scalable Alternative to Reinforcement Learning [11] Frank Sehnke, et al. Parameter-exploring policy gradients. Neural Networks 23.4 (2010): 551-559. [12] Csaba Szepesvári. Algorithms for reinforcement learning. 1st Edition. Synthesis lectures on artificial intelligence and machine learning 4.1 (2010): 1-103. If you notice mistakes and errors in this post, please don’t hesitate to contact me at [lilian dot wengweng at gmail dot com] and I would be super happy to correct them right away!"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 0,
    "text": "Intelligence of machines Artificial intelligence ( AI ) is the capability of computational systems to perform tasks typically associated with human intelligence , such as learning , reasoning , problem-solving , perception , and decision-making . It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search ); recommendation systems (used by YouTube , Amazon , and Netflix ); virtual assistants (e.g., Google Assistant , Siri , and Alexa ); autonomous vehicles (e.g., Waymo ); generative and creative tools (e.g., language models and AI art ); and superhuman play and analysis in strategy games (e.g., chess and Go ). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore .\" [ 2 ] [ 3 ] Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning , knowledge representation , planning , natural language processing , perception , and support for robotics . [ a ] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization , formal logic , artificial neural networks , and methods based on statistics , operations research , and economics . [ b ] AI also draws upon psychology , linguistics , philosophy , neuroscience , and other fields. [ 4 ] Some companies, such as OpenAI , Google DeepMind and Meta , [ 5 ] aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, [ 6 ] and the field went through multiple cycles of optimism throughout its history , [ 7 ] [ 8 ] followed by periods of disappointment and loss of funding, known as AI winters . [ 9 ] [ 10 ] Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. [ 11 ] This growth accelerated further after 2017 with the transformer architecture . In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom . Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks , prompting discussions about regulatory policies to ensure the safety and benefits of the technology. Goals The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. [ a ] Reasoning and problem-solving Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions . [ 13 ] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics . [ 14 ] Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. [ 15 ] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. [ 16 ] Accurate and efficient reasoning is an unsolved problem. Knowledge representation An ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts. Knowledge representation and knowledge engineering [ 17 ] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases ), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; [ 24 ] situations, events, states, and time; [ 25 ] causes and effects; [ 26 ] knowledge about knowledge (what we know about what other people know); [ 27 ] default reasoning (things that"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 1,
    "text": "humans assume are true until they are told differently and will remain true even when other facts are changing); [ 28 ] and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); [ 29 ] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). [ 16 ] There is also the difficulty of knowledge acquisition , the problem of obtaining knowledge for AI applications. [ c ] Planning and decision-making An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. [ d ] In automated planning , the agent has a specific goal. [ 33 ] In automated decision-making , the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \" utility \") that measures how much the agent prefers it. For each possible action, it can calculate the \" expected utility \": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility. [ 34 ] In classical planning , the agent knows exactly what the effect of any action will be. [ 35 ] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. [ 36 ] In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning ), or the agent can seek information to improve its preferences. [ 37 ] Information value theory can be used to weigh the value of exploratory or experimental actions. [ 38 ] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration ), be heuristic , or it can be learned. [ 39 ] Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. [ 40 ] Learning Machine learning is the study of programs that can improve their performance on a given task automatically. [ 41 ] It has been a part of AI from the beginning. [ e ] In supervised learning , the training data is labelled with the expected answers, while in unsupervised learning , the model identifies patterns or structures in unlabelled data. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. [ 44 ] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). [ 45 ] In reinforcement learning , the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". [ 46 ] Transfer learning is when the knowledge gained from one problem is applied to a new problem. [ 47 ] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. [ 48 ] Computational learning theory can assess learners by computational complexity , by sample complexity (how much data is required), or by other notions of optimization . [ 49 ] Natural language processing Natural language processing (NLP) allows programs to read, write and communicate in human languages. [ 50 ] Specific problems include speech recognition , speech synthesis , machine translation , information extraction , information retrieval and question answering . [ 51 ] Early work, based on Noam"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 2,
    "text": "Chomsky 's generative grammar and semantic networks , had difficulty with word-sense disambiguation [ f ] unless restricted to small domains called \" micro-worlds \" (due to the common sense knowledge problem [ 29 ] ). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. [ 54 ] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam , SAT test, GRE test, and many other real-world applications. Perception Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar , sonar, radar, and tactile sensors ) to deduce aspects of the world. Computer vision is the ability to analyze visual input. [ 58 ] The field includes speech recognition , image classification , facial recognition , object recognition , object tracking , [ 62 ] and robotic perception . Social intelligence Kismet , a robot head which was made in the 1990s; it is a machine that can recognize and simulate emotions. Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood . [ 65 ] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction . However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis , wherein AI classifies the effects displayed by a videotaped subject. General intelligence A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence . [ 68 ] Techniques AI research uses a wide variety of techniques to accomplish the goals above. [ b ] Search and optimization AI can solve many problems by intelligently searching through many possible solutions. [ 69 ] There are two very different kinds of search used in AI: state space search and local search . State space search State space search searches through a tree of possible states to try to find a goal state. [ 70 ] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis . Simple exhaustive searches [ 72 ] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers . The result is a search that is too slow or never completes. [ 15 ] \" Heuristics \" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal. [ 73 ] Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position. [ 74 ] Local search Illustration of gradient descent for 3 different starting points; two parameters (represented by the plan coordinates) are adjusted in order to minimize the loss function (the height) Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally. [ 75 ] Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function . Variants of gradient descent are commonly used to train neural networks , [ 76 ] through the backpropagation algorithm. Another type of local search is evolutionary computation , which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation. [ 77 ] Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking ) and ant colony optimization (inspired by ant trails ). Logic Formal logic is used for reasoning and knowledge representation . [ 79 ] Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") [ 80 ] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \" Every X is a Y \" and \"There are some X"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 3,
    "text": "s that are Y s\"). [ 81 ] Deductive reasoning in logic is the process of proving a new statement ( conclusion ) from other statements that are given and assumed to be true (the premises ). [ 82 ] Proofs can be structured as proof trees , in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules . Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms . In the case of Horn clauses , problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. [ 83 ] In the more general case of the clausal form of first-order logic , resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved. [ 84 ] Inference in both Horn clause logic and first-order logic is undecidable , and therefore intractable . However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog , is Turing complete . Moreover, its efficiency is competitive with computation in other symbolic programming languages. [ 85 ] Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true. [ 86 ] Non-monotonic logics , including logic programming with negation as failure , are designed to handle default reasoning . [ 28 ] Other specialized versions of logic have been developed to describe many complex domains. Probabilistic methods for uncertain reasoning A simple Bayesian network , with the associated conditional probability tables Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. [ 87 ] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory , decision analysis , [ 88 ] and information value theory . [ 89 ] These tools include models such as Markov decision processes , [ 90 ] dynamic decision networks , [ 91 ] game theory and mechanism design . [ 92 ] Bayesian networks [ 93 ] are a tool that can be used for reasoning (using the Bayesian inference algorithm), [ g ] [ 95 ] learning (using the expectation–maximization algorithm ), [ h ] [ 97 ] planning (using decision networks ) [ 98 ] and perception (using dynamic Bayesian networks ). [ 91 ] Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters ). [ 91 ] Expectation–maximization clustering of Old Faithful eruption data starts from a random guess but then successfully converges on an accurate clustering of the two physically distinct modes of eruption. Classifiers and statistical learning methods The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers [ 99 ] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning . Each pattern (also called an \" observation \") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set . When a new observation is received, that observation is classified based on previous experience. [ 45 ] There are many kinds of classifiers in use. [ 100 ] The decision tree is the simplest and most widely used symbolic machine learning algorithm. [ 101 ] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s. [ 102 ] The naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability. [ 104 ] Neural networks are also used as classifiers. [ 105 ] Artificial neural networks A neural network is an interconnected group of nodes, akin to the vast network of neurons in the human brain . An artificial neural network is based on a collection of nodes also known as artificial neurons , which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 4,
    "text": "an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers. [ 105 ] Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. [ 106 ] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function. [ 107 ] In feedforward neural networks the signal passes in only one direction. [ 108 ] The term perceptron typically refers to a single-layer neural network. [ 109 ] In contrast, deep learning uses many layers. [ 110 ] Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem . [ 111 ] Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing , where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects. [ 112 ] Deep learning Deep learning is a subset of machine learning , which is itself a subset of artificial intelligence. [ 113 ] Deep learning uses several layers of neurons between the network's inputs and outputs. [ 110 ] The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing , lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces. Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision , speech recognition , natural language processing , image classification , and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) [ i ] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs ) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet . [ j ] GPT Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \" hallucinations \". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. [ 124 ] Such systems are used in chatbots , which allow people to ask a question or request a task in simple text. [ 126 ] Current models and services include ChatGPT , Claude , Gemini , Copilot , and Meta AI . [ 127 ] Multimodal GPT models can process different types of data ( modalities ) such as images, videos, sound, and text. Hardware and software In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, [ 130 ] but general-purpose programming languages like Python have become predominant. [ 131 ] The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law , named after the Intel co-founder Gordon Moore , who first identified it. Improvements in GPUs have been even faster, [ 132 ] a trend sometimes called Huang's law , [ 133 ] named after Nvidia co-founder and CEO Jensen"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 5,
    "text": "Huang . Applications AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search ), targeting online advertisements , recommendation systems (offered by Netflix , YouTube or Amazon ), driving internet traffic , targeted advertising ( AdSense , Facebook ), virtual assistants (such as Siri or Alexa ), autonomous vehicles (including drones , ADAS and self-driving cars ), automatic language translation ( Microsoft Translator , Google Translate ), facial recognition ( Apple 's FaceID or Microsoft 's DeepFace and Google 's FaceNet ) and image labeling (used by Facebook , Apple's Photos and TikTok ). The deployment of AI may be overseen by a chief automation officer (CAO). Health and medicine The application of AI in medicine and medical research has the potential to increase patient care and quality of life. [ 134 ] Through the lens of the Hippocratic Oath , medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients. [ 135 ] [ 136 ] For medical research, AI is an important tool for processing and integrating big data . This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. [ 137 ] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. [ 137 ] [ 138 ] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein . [ 139 ] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. [ 140 ] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold. [ 141 ] [ 142 ] Games Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. [ 143 ] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov , on 11 May 1997. [ 144 ] In 2011, in a Jeopardy! quiz show exhibition match, IBM 's question answering system , Watson , defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings , by a significant margin. [ 145 ] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol , becoming the first computer Go -playing system to beat a professional Go player without handicaps . Then, in 2017, it defeated Ke Jie , who was the best Go player in the world. [ 146 ] Other programs handle imperfect-information games, such as the poker -playing program Pluribus . [ 147 ] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero , which could be trained to play chess, Go, or Atari games. [ 148 ] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II , a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. [ 149 ] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. [ 150 ] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions. [ 151 ] Mathematics Large language models, such as GPT-4 , Gemini , Claude , Llama or Mistral , are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations . They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning [ 152 ] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. [ 153 ] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. [ 154 ] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. [ 155 ] The Alibaba Group developed a version of its Qwen models called Qwen2-Math , that achieved state-of-the-art performance"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 6,
    "text": "on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. [ 156 ] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems. [ 157 ] Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor , AlphaGeometry , AlphaProof and AlphaEvolve [ 158 ] all from Google DeepMind , [ 159 ] Llemma from EleutherAI [ 160 ] or Julius . [ 161 ] When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025. [ 162 ] Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics. [ 163 ] Topological deep learning integrates various topological approaches. Finance Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years. [ 164 ] According to Nicolas Firzli, director of the World Pensions & Investments Forum , it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\" [ 165 ] Military Various countries are deploying AI military applications. [ 166 ] The main applications enhance command and control , communications, sensors, integration and interoperability. [ 167 ] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles . [ 166 ] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition , coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous . [ 167 ] AI has been used in military operations in Iraq, Syria, Israel and Ukraine. [ 166 ] [ 168 ] [ 169 ] [ 170 ] Generative AI Vincent van Gogh in watercolour created by generative AI software Generative artificial intelligence (Generative AI, GenAI, [ 171 ] or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. [ 172 ] [ 173 ] [ 174 ] These models learn the underlying patterns and structures of their training data and use them to produce new data [ 175 ] [ 176 ] based on the input, which often comes in the form of natural language prompts . [ 177 ] [ 178 ] Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer -based deep neural networks , particularly large language models (LLMs). Major tools include chatbots such as ChatGPT , Copilot , Gemini , Claude , Grok , and DeepSeek ; text-to-image models such as Stable Diffusion , Midjourney , and DALL-E ; and text-to-video models such as Veo , LTXV and Sora . [ 179 ] [ 180 ] [ 181 ] [ 182 ] [ 183 ] Technology companies developing generative AI include OpenAI , xAI , Anthropic , Meta AI , Microsoft , Google , DeepSeek , and Baidu . [ 177 ] [ 184 ] [ 185 ] Generative AI has raised many ethical questions and governance challenges as it can be used for cybercrime , or to deceive or manipulate people through fake news or deepfakes . [ 186 ] [ 187 ] Even if used ethically, it may lead to mass replacement of human jobs . [ 188 ] The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works. [ 189 ] Agents AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants , chatbots , autonomous vehicles , game-playing systems , and industrial robotics . AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 7,
    "text": "scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks. [ 190 ] [ 191 ] [ 192 ] Sexuality Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, [ 193 ] AI-integrated sex toys (e.g., teledildonics ), [ 194 ] AI-generated sexual education content, [ 195 ] and AI agents that simulate sexual and romantic partners (e.g., Replika ). [ 196 ] AI is also used for the production of non-consensual deepfake pornography , raising significant ethical and legal concerns. [ 197 ] AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors. [ 198 ] [ 199 ] Other industry-specific tasks There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. [ 200 ] A few examples are energy storage , medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy , or supply chain management. AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions. [ 201 ] [ 202 ] [ 203 ] In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics , classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water. Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation. During the 2024 Indian elections , US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages. [ 204 ] Ethics Street art in Tel Aviv [ 205 ] [ 206 ] AI has potential benefits and potential risks. [ 207 ] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. [ 210 ] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning. Risks and harm Privacy and copyright Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy , surveillance and copyright . AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency. Sensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy . AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation , de-identification and differential privacy . Since 2016, some privacy experts, such as Cynthia Dwork , have begun to view privacy in terms of fairness . Brian Christian wrote that experts"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 8,
    "text": "have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\" Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \" fair use \". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". [ 218 ] Website owners who do not wish to have their content scraped can indicate it in a \" robots.txt \" file. [ 219 ] In 2023, leading authors (including John Grisham and Jonathan Franzen ) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors. [ 222 ] Dominance by tech giants The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc. , Amazon , Apple Inc. , Meta Platforms , and Microsoft . [ 223 ] [ 224 ] [ 225 ] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers , allowing them to entrench further in the marketplace. [ 226 ] [ 227 ] Power needs and environmental impacts In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026 , forecasting electric power use. [ 228 ] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation. [ 229 ] Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms. [ 230 ] A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge , found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. [ 231 ] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all. [ 232 ] In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. [ 233 ] Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers. [ 234 ] In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission . If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act . [ 235 ] The US government and the state of Michigan are investing almost US$2 billion"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 9,
    "text": "to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation. [ 236 ] After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. [ 237 ] Taiwan aims to phase out nuclear power by 2025. [ 237 ] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. [ 237 ] Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident , according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI. [ 238 ] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI. [ 238 ] On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. [ 239 ] According to the Commission Chairman Willie L. Phillips , it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors. [ 239 ] In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it. [ 240 ] Misinformation YouTube , Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation , conspiracy theories , and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. [ 242 ] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem. [ 243 ] In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, [ 244 ] while realistic AI-generated videos became feasible in the mid-2020s. [ 245 ] [ 246 ] [ 247 ] It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda . [ 249 ] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks. AI researchers at Microsoft , OpenAI , universities and other organisations have suggested using \" personhood credentials \" as a way to overcome online deception enabled by AI models. [ 251 ] Algorithmic bias and fairness Machine learning applications will be biased [ k ] if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine , finance , recruitment , housing or policing ) then the algorithm may cause discrimination . [ 256 ] The field of fairness studies how to prevent harms from algorithmic biases. On June 28, 2015, Google Photos 's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 10,
    "text": "system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon. COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist . In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers [ l ] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data. [ 262 ] A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". [ 263 ] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\" [ 264 ] Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations , some of these \"recommendations\" will likely be racist. [ 265 ] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive. [ m ] Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women. There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness , which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws . [ 252 ] At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery , in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed. [ dubious – discuss ] Lack of transparency Many AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks , in which there are many non- linear relationships between inputs and outputs. But some popular explainability techniques exist. [ 269 ] It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 11,
    "text": "misleading. People who have been harmed by an algorithm's decision have a right to an explanation. [ 272 ] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. [ n ] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used. DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems. Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution , DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers , Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts. [ 279 ] Bad actors and weaponized AI Artificial intelligence provides a number of tools that are useful to bad actors , such as authoritarian governments , terrorists , criminals or rogue states . A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. [ o ] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction . Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person . In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations ' Convention on Certain Conventional Weapons , however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots. [ 283 ] AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance . Machine learning , operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets . It lowers the cost and difficulty of digital warfare and advanced spyware . All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China. [ 285 ] [ 286 ] There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours. Technological unemployment Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment. [ 288 ] In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. [ 289 ] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment , but they generally agree that it could be a net benefit if productivity gains are redistributed . Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". [ p ] [ 292 ] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. [ 288 ] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence. [ 293 ] [ 294 ] Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 12,
    "text": "care-related professions ranging from personal healthcare to the clergy. [ 296 ] From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum , about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement. [ 297 ] Existential risk It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \" spell the end of the human race \". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. [ q ] These sci-fi scenarios are misleading in several ways. First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer ). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\". [ 302 ] Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies , law , government , money and the economy are built on language ; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking , Bill Gates , and Elon Musk , [ 305 ] as well as AI pioneers such as Yoshua Bengio , Stuart Russell , Demis Hassabis , and Sam Altman , have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". [ 306 ] He notably mentioned risks of an AI takeover , [ 307 ] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. [ 308 ] In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" [ 310 ] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" [ 311 ] [ 312 ] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" [ 313 ] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" [ 314 ] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. [ 315 ] However, after 2016, the study of current and future risks and possible solutions became a serious area of research. Ethical machines and alignment Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky , who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk. Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas. [ 318 ] The field of machine ethics is also called computational morality, [ 318 ] and was founded at"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 13,
    "text": "an AAAI symposium in 2005. Other approaches include Wendell Wallach 's \"artificial moral agents\" and Stuart J. Russell 's three principles for developing provably beneficial machines. Open source Active organizations in the AI open-source community include Hugging Face , [ 322 ] Google , [ 323 ] EleutherAI and Meta . [ 324 ] Various AI models, such as Llama 2 , Mistral or Stable Diffusion , have been made open-weight, [ 325 ] [ 326 ] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned , which allows companies to specialize them with their own data and for their own use-case. [ 327 ] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism ) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. [ 328 ] Frameworks Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows: [ 329 ] [ 330 ] Respect the dignity of individual people Connect with other people sincerely, openly, and inclusively Care for the wellbeing of everyone Protect social values, justice, and the public interest Other developments in ethical frameworks include those decided upon during the Asilomar Conference , the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; [ 331 ] however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks. [ 332 ] Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers. [ 333 ] The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities. [ 334 ] Regulation The first global AI Safety Summit was held in the United Kingdom in November 2023 with a declaration calling for international cooperation. The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. [ 335 ] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford , the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger , Eric Schmidt , and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. [ 342 ] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \" Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law \". It was adopted by the European Union, the United States, the United Kingdom, and other signatories. [ 343 ] In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters /Ipsos poll"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 14,
    "text": "found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\". In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. [ 347 ] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. [ 348 ] [ 349 ] In May 2024 at the AI Seoul Summit , 16 global AI tech companies agreed to safety commitments on the development of AI. [ 350 ] [ 351 ] History In 2024, AI patents in China and the US numbered more than three-fourths of AI patents worldwide. [ 352 ] Though China had more AI patents, the US had 35% more patents per AI patent-applicant company than China. [ 352 ] The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing 's theory of computation , which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. [ 354 ] This, along with concurrent discoveries in cybernetics , information theory and neurobiology , led researchers to consider the possibility of building an \"electronic brain\". [ r ] They developed several areas of research that would become part of AI, [ 356 ] such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper ' Computing Machinery and Intelligence ', which introduced the Turing test and showed that \"machine intelligence\" was plausible. [ 357 ] [ 354 ] The field of AI research was founded at a workshop at Dartmouth College in 1956. [ s ] [ 6 ] The attendees became the leaders of AI research in the 1960s. [ t ] They and their students produced programs that the press described as \"astonishing\": [ u ] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. [ v ] [ 7 ] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s. [ 354 ] Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". [ 362 ] In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". [ 363 ] They had, however, underestimated the difficulty of the problem. [ w ] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects . Minsky and Papert 's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \" AI winter \", a period when obtaining funding for AI projects was difficult, followed. [ 9 ] In the early 1980s, AI research was revived by the commercial success of expert systems , [ 368 ] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research . [ 8 ] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began. [ 10 ] Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception , robotics , learning and pattern recognition , and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. [ x ] Judea Pearl , Lotfi Zadeh , and others developed methods that handled"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 15,
    "text": "incomplete and uncertain information by making reasonable guesses rather than precise logic. [ 87 ] But the most important development was the revival of \" connectionism \", including neural network research, by Geoffrey Hinton and others. [ 376 ] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks. AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \" narrow \" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics , economics and mathematics ). [ 378 ] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect ). [ 379 ] However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s. [ 68 ] Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field. [ 11 ] For many specific tasks, other methods were abandoned. [ y ] Deep learning's success was based on both hardware improvements ( faster computers , [ 381 ] graphics processing units , cloud computing ) and access to large amounts of data [ 383 ] (including curated datasets, such as ImageNet ). Deep learning's success led to an enormous increase in interest and funding in AI. [ z ] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. The number of Google searches for the term \"AI\" accelerated in 2022. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo , developed by DeepMind , beat the world champion Go player . The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. [ 384 ] ChatGPT , launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. [ 385 ] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. [ 386 ] These programs, and others, inspired an aggressive AI boom , where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. [ 389 ] Philosophy Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. [ 390 ] Another major focus has been whether machines can be conscious, and the associated ethical implications. [ 391 ] Many other topics in philosophy are relevant to AI, such as epistemology and free will . [ 392 ] Rapid advancements have intensified public discussions on the philosophy and ethics of AI . [ 391 ] Defining artificial intelligence Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test , which measures the ability of a machine to simulate human conversation. [ 357 ] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\" The Turing test can provide some evidence of intelligence, but it penalizes non-human intelligent behavior. [ 395 ] Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \" Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 16,
    "text": "that fly so exactly like pigeons that they can fool other pigeons. ' \" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky , similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible. Another definition has been adopted by Google, [ 400 ] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, [ 401 ] with many companies during the early 2020s AI boom using the term as a marketing buzzword , often even if they did \"not actually use AI in a material way\". [ 402 ] There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text . [ 403 ] Evaluating approaches to AI No established unifying theory or paradigm has guided AI research for most of its history. [ aa ] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic , soft and narrow . Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AI and its limits Symbolic AI (or \" GOFAI \") [ 405 ] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis : \"A physical symbol system has the necessary and sufficient means of general intelligent action.\" [ 406 ] However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning . Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. [ 407 ] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. [ 408 ] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him. [ ab ] [ 16 ] The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias . Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI : it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches. Neat vs. scruffy \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic , optimization , or neural networks ). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, [ 412 ] but eventually was seen as irrelevant. Modern AI has elements of both. Soft vs. hard computing Finding a provably correct or optimal solution is intractable for many important problems. [ 15 ] Soft computing is a set of techniques, including genetic algorithms , fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 17,
    "text": "many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. Machine consciousness, sentience, and mind There is no settled consensus in philosophy of mind on whether a machine can have a mind , consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction . Consciousness David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like . Computationalism and functionalism Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem . This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam . Philosopher John Searle characterized this position as \" strong AI \": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" [ ac ] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind. [ 422 ] AI welfare and rights It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. [ 423 ] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. [ 424 ] [ 425 ] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness ) may provide another moral basis for AI rights. [ 424 ] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society. [ 426 ] In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. [ 427 ] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights , and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own. [ 428 ] [ 429 ] Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming , which could lead to large-scale suffering if sentient AI is created and carelessly exploited. [ 425 ] [ 424 ] Future Superintelligence and the singularity A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself . The improved software would be even better at improving itself, leading to what I."
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 18,
    "text": "J. Good called an \" intelligence explosion \" and Vernor Vinge called a \" singularity \". [ 430 ] However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve , slowing when they reach the physical limits of what the technology can do. Transhumanism Robot designer Hans Moravec , cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger . [ 432 ] Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler 's \" Darwin among the Machines \" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence . [ 433 ] In fiction The word \"robot\" itself was coined by Karel Čapek in his 1921 play R.U.R. , the title standing for \"Rossum's Universal Robots\". Thought-capable artificial beings have appeared as storytelling devices since antiquity, [ 434 ] and have been a persistent theme in science fiction . A common trope in these works began with Mary Shelley 's Frankenstein , where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000 , the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture. Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \" Multivac \" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; [ 437 ] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity. Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel , and thus to suffer. This appears in Karel Čapek 's R.U.R. , the films A.I. Artificial Intelligence and Ex Machina , as well as the novel Do Androids Dream of Electric Sheep? , by Philip K. Dick . Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. See also Explanatory notes ^ a b This list of intelligent traits is based on the topics covered by the major AI textbooks, including: Russell & Norvig (2021) , Luger & Stubblefield (2004) , Poole, Mackworth & Goebel (1998) and Nilsson (1998) ^ a b This list of tools is based on the topics covered by the major AI textbooks, including: Russell & Norvig (2021) , Luger & Stubblefield (2004) , Poole, Mackworth & Goebel (1998) and Nilsson (1998) ^ It is among the reasons that expert systems proved to be inefficient for capturing knowledge. ^ \"Rational agent\" is general term used in economics , philosophy and theoretical artificial intelligence. It can refer to anything that directs its behavior to accomplish goals, such as a person, an animal, a corporation, a nation, or in the case of AI, a computer program. ^ Alan Turing discussed the centrality of learning as early as 1950, in his classic paper \" Computing Machinery and Intelligence \". In 1956, at the original Dartmouth AI summer conference, Ray Solomonoff wrote a report on unsupervised probabilistic machine learning: \"An Inductive Inference Machine\". ^ See AI winter § Machine translation and the ALPAC report of 1966 ^ Compared with symbolic logic, formal Bayesian inference is computationally expensive. For inference to be tractable, most observations must be conditionally independent of one another. AdSense uses a Bayesian network with over 300 million edges to learn which ads to serve. ^ Expectation–maximization, one of the most popular algorithms in machine learning, allows clustering in the presence of unknown latent variables . ^ Some form of deep neural networks (without a specific learning algorithm) were described by: Warren S. McCulloch and Walter Pitts (1943) Alan Turing (1948); Karl Steinbuch and Roger David Joseph (1961). Deep or recurrent networks that learned (or used gradient descent) were developed by: Frank Rosenblatt (1957); Oliver Selfridge (1959); Alexey Ivakhnenko and Valentin Lapa (1965); Kaoru Nakano (1971); Shun-Ichi Amari (1972); John Joseph Hopfield (1982). Precursors to backpropagation were developed by: Henry J. Kelley (1960); Arthur E. Bryson (1962); Stuart Dreyfus (1962); Arthur E. Bryson and Yu-Chi Ho (1969); Backpropagation was independently developed by: Seppo Linnainmaa (1970); Paul"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 19,
    "text": "Werbos (1974). ^ Geoffrey Hinton said, of his work on neural networks in the 1990s, \"our labeled datasets were thousands of times too small. [And] our computers were millions of times too slow.\" [ 123 ] ^ In statistics, a bias is a systematic error or deviation from the correct value. But in the context of fairness , it refers to a tendency in favor or against a certain group or individual characteristic, usually in a way that is considered unfair or harmful. A statistically unbiased AI system that produces disparate outcomes for different demographic groups may thus be viewed as biased in the ethical sense. [ 252 ] ^ Including Jon Kleinberg ( Cornell University ), Sendhil Mullainathan ( University of Chicago ), Cynthia Chouldechova ( Carnegie Mellon ) and Sam Corbett-Davis ( Stanford ) ^ Moritz Hardt (a director at the Max Planck Institute for Intelligent Systems ) argues that machine learning \"is fundamentally the wrong tool for a lot of domains, where you're trying to design interventions and mechanisms that change the world.\" [ 266 ] ^ When the law was passed in 2018, it still contained a form of this provision. ^ This is the United Nations ' definition, and includes things like land mines as well. ^ See table 4; 9% is both the OECD average and the U.S. average. ^ Sometimes called a \" robopocalypse \" ^ \"Electronic brain\" was the term used by the press around this time. [ 355 ] ^ Daniel Crevier wrote, \"the conference is generally recognized as the official birthdate of the new science.\" Russell and Norvig called the conference \"the inception of artificial intelligence.\" ^ Russell and Norvig wrote \"for the next 20 years the field would be dominated by these people and their students.\" ^ Russell and Norvig wrote, \"it was astonishing whenever a computer did anything kind of smartish\". ^ The programs described are Arthur Samuel 's checkers program for the IBM 701 , Daniel Bobrow 's STUDENT , Newell and Simon 's Logic Theorist and Terry Winograd 's SHRDLU . ^ Russell and Norvig write: \"in almost all cases, these early systems failed on more difficult problems\" ^ Embodied approaches to AI were championed by Hans Moravec and Rodney Brooks and went by many names: Nouvelle AI . Developmental robotics . [ 374 ] ^ Matteo Wong wrote in The Atlantic : \"Whereas for decades, computer-science fields such as natural-language processing, computer vision, and robotics used extremely different methods, now they all use a programming method called \"deep learning\". As a result, their code and approaches have become more similar, and their models are easier to integrate into one another.\" ^ Jack Clark wrote in Bloomberg : \"After a half-decade of quiet breakthroughs in artificial intelligence, 2015 has been a landmark year. Computers are smarter and learning faster than ever\", and noted that the number of software projects that use machine learning at Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects in 2015. ^ Nils Nilsson wrote in 1983: \"Simply put, there is wide disagreement in the field about what AI is all about.\" ^ Daniel Crevier wrote that \"time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.\" ^ Searle presented this definition of \"Strong AI\" in 1999. Searle's original formulation was \"The appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states.\" Strong AI is defined similarly by Russell and Norvig : \"Stong AI – the assertion that machines that do so are actually thinking (as opposed to simulating thinking).\" References ^ AI set to exceed human brain power Archived 2008-02-19 at the Wayback Machine CNN.com (July 26, 2006) ^ Kaplan, Andreas; Haenlein, Michael (2019). \"Siri, Siri, in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence\". Business Horizons . 62 : 15– 25. doi : 10.1016/j.bushor.2018.08.004 . ISSN 0007-6813 . S2CID 158433736 . ^ Russell & Norvig (2021 , §1.2). ^ \"Tech companies want to build artificial general intelligence. But who decides when AGI is attained?\" . AP News . 4 April 2024 . Retrieved 20 May 2025 . ^ a b Dartmouth workshop : Russell & Norvig (2021 , p. 18), McCorduck (2004 , pp. 111–136), NRC (1999 , pp. 200–201) The proposal: McCarthy et al. (1955) ^ a b Successful programs of the 1960s: McCorduck (2004 , pp. 243–252), Crevier (1993 , pp. 52–107), Moravec (1988 , p. 9), Russell & Norvig (2021 , pp. 19–21) ^ a b Funding initiatives in the early 1980s: Fifth Generation Project (Japan), Alvey (UK), Microelectronics"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 20,
    "text": "and Computer Technology Corporation (US), Strategic Computing Initiative (US): McCorduck (2004 , pp. 426–441), Crevier (1993 , pp. 161–162, 197–203, 211, 240), Russell & Norvig (2021 , p. 23), NRC (1999 , pp. 210–211), Newquist (1994 , pp. 235–248) ^ a b First AI Winter , Lighthill report , Mansfield Amendment : Crevier (1993 , pp. 115–117), Russell & Norvig (2021 , pp. 21–22), NRC (1999 , pp. 212–213), Howe (1994) , Newquist (1994 , pp. 189–201) ^ a b Second AI Winter : Russell & Norvig (2021 , p. 24), McCorduck (2004 , pp. 430–435), Crevier (1993 , pp. 209–210), NRC (1999 , pp. 214–216), Newquist (1994 , pp. 301–318) ^ a b Deep learning revolution, AlexNet : Goldman (2022) , Russell & Norvig (2021 , p. 26), McKinsey (2018) ^ Problem-solving, puzzle solving, game playing, and deduction: Russell & Norvig (2021 , chpt. 3–5), Russell & Norvig (2021 , chpt. 6) ( constraint satisfaction ), Poole, Mackworth & Goebel (1998 , chpt. 2, 3, 7, 9), Luger & Stubblefield (2004 , chpt. 3, 4, 6, 8), Nilsson (1998 , chpt. 7–12) ^ Uncertain reasoning: Russell & Norvig (2021 , chpt. 12–18), Poole, Mackworth & Goebel (1998 , pp. 345–395), Luger & Stubblefield (2004 , pp. 333–381), Nilsson (1998 , chpt. 7–12) ^ a b c Intractability and efficiency and the combinatorial explosion : Russell & Norvig (2021 , p. 21) ^ a b c Psychological evidence of the prevalence of sub-symbolic reasoning and knowledge: Kahneman (2011) , Dreyfus & Dreyfus (1986) , Wason & Shapiro (1966) , Kahneman, Slovic & Tversky (1982) ^ Knowledge representation and knowledge engineering : Russell & Norvig (2021 , chpt. 10), Poole, Mackworth & Goebel (1998 , pp. 23–46, 69–81, 169–233, 235–277, 281–298, 319–345), Luger & Stubblefield (2004 , pp. 227–243), Nilsson (1998 , chpt. 17.1–17.4, 18) ^ Representing categories and relations: Semantic networks , description logics , inheritance (including frames , and scripts ): Russell & Norvig (2021 , §10.2 & 10.5), Poole, Mackworth & Goebel (1998 , pp. 174–177), Luger & Stubblefield (2004 , pp. 248–258), Nilsson (1998 , chpt. 18.3) ^ Representing events and time: Situation calculus , event calculus , fluent calculus (including solving the frame problem ): Russell & Norvig (2021 , §10.3), Poole, Mackworth & Goebel (1998 , pp. 281–298), Nilsson (1998 , chpt. 18.2) ^ Causal calculus : Poole, Mackworth & Goebel (1998 , pp. 335–337) ^ Representing knowledge about knowledge: Belief calculus, modal logics : Russell & Norvig (2021 , §10.4), Poole, Mackworth & Goebel (1998 , pp. 275–277) ^ a b Default reasoning , Frame problem , default logic , non-monotonic logics , circumscription , closed world assumption , abduction : Russell & Norvig (2021 , §10.6), Poole, Mackworth & Goebel (1998 , pp. 248–256, 323–335), Luger & Stubblefield (2004 , pp. 335–363), Nilsson (1998 , ~18.3.3) (Poole et al. places abduction under \"default reasoning\". Luger et al. places this under \"uncertain reasoning\"). ^ a b Breadth of commonsense knowledge: Lenat & Guha (1989 , Introduction), Crevier (1993 , pp. 113–114), Moravec (1988 , p. 13), Russell & Norvig (2021 , pp. 241, 385, 982) ( qualification problem ) ^ Automated planning : Russell & Norvig (2021 , chpt. 11). ^ Automated decision making , Decision theory : Russell & Norvig (2021 , chpt. 16–18). ^ Classical planning : Russell & Norvig (2021 , Section 11.2). ^ Sensorless or \"conformant\" planning, contingent planning, replanning (a.k.a. online planning): Russell & Norvig (2021 , Section 11.5). ^ Uncertain preferences: Russell & Norvig (2021 , Section 16.7) Inverse reinforcement learning : Russell & Norvig (2021 , Section 22.6) ^ Information value theory : Russell & Norvig (2021 , Section 16.6). ^ Markov decision process : Russell & Norvig (2021 , chpt. 17). ^ Game theory and multi-agent decision theory: Russell & Norvig (2021 , chpt. 18). ^ Learning : Russell & Norvig (2021 , chpt. 19–22), Poole, Mackworth & Goebel (1998 , pp. 397–438), Luger & Stubblefield (2004 , pp. 385–542), Nilsson (1998 , chpt. 3.3, 10.3, 17.5, 20) ^ Unsupervised learning : Russell & Norvig (2021 , pp. 653) (definition), Russell & Norvig (2021 , pp. 738–740) ( cluster analysis ), Russell & Norvig (2021 , pp. 846–860) ( word embedding ) ^ a b Supervised learning : Russell & Norvig (2021 , §19.2) (Definition), Russell & Norvig (2021 , Chpt. 19–20) (Techniques) ^ Reinforcement learning : Russell & Norvig (2021 , chpt. 22), Luger & Stubblefield (2004 , pp. 442–449) ^ Transfer learning : Russell & Norvig (2021 , pp. 281), The Economist (2016) ^ \"Artificial Intelligence (AI): What Is AI and How Does It Work? | Built In\" . builtin.com . Retrieved 30 October 2023 . ^ Computational learning theory : Russell & Norvig (2021 , pp. 672–674), Jordan & Mitchell (2015) ^ Natural"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 21,
    "text": "language processing (NLP): Russell & Norvig (2021 , chpt. 23–24), Poole, Mackworth & Goebel (1998 , pp. 91–104), Luger & Stubblefield (2004 , pp. 591–632) ^ Subproblems of NLP : Russell & Norvig (2021 , pp. 849–850) ^ Modern statistical and deep learning approaches to NLP : Russell & Norvig (2021 , chpt. 24), Cambria & White (2014) ^ Computer vision : Russell & Norvig (2021 , chpt. 25), Nilsson (1998 , chpt. 6) ^ Challa et al. (2011) . ^ Affective computing : Thro (1993) , Edelson (1991) , Tao & Tan (2005) , Scassellati (2002) ^ a b Artificial general intelligence : Russell & Norvig (2021 , pp. 32–33, 1020–1021) Proposal for the modern version: Pennachin & Goertzel (2007) Warnings of overspecialization in AI from leading researchers: Nilsson (1995) , McCarthy (2007) , Beal & Winston (2009) ^ Search algorithms : Russell & Norvig (2021 , chpts. 3–5), Poole, Mackworth & Goebel (1998 , pp. 113–163), Luger & Stubblefield (2004 , pp. 79–164, 193–219), Nilsson (1998 , chpts. 7–12) ^ State space search : Russell & Norvig (2021 , chpt. 3) ^ Uninformed searches ( breadth first search , depth-first search and general state space search ): Russell & Norvig (2021 , sect. 3.4), Poole, Mackworth & Goebel (1998 , pp. 113–132), Luger & Stubblefield (2004 , pp. 79–121), Nilsson (1998 , chpt. 8) ^ Heuristic or informed searches (e.g., greedy best first and A* ): Russell & Norvig (2021 , sect. 3.5), Poole, Mackworth & Goebel (1998 , pp. 132–147), Poole & Mackworth (2017 , sect. 3.6), Luger & Stubblefield (2004 , pp. 133–150) ^ Adversarial search : Russell & Norvig (2021 , chpt. 5) ^ Local or \" optimization \" search: Russell & Norvig (2021 , chpt. 4) ^ Singh Chauhan, Nagesh (18 December 2020). \"Optimization Algorithms in Neural Networks\" . KDnuggets . Retrieved 13 January 2024 . ^ Evolutionary computation : Russell & Norvig (2021 , sect. 4.1.2) ^ Logic : Russell & Norvig (2021 , chpts. 6–9), Luger & Stubblefield (2004 , pp. 35–77), Nilsson (1998 , chpt. 13–16) ^ Propositional logic : Russell & Norvig (2021 , chpt. 6), Luger & Stubblefield (2004 , pp. 45–50), Nilsson (1998 , chpt. 13) ^ First-order logic and features such as equality : Russell & Norvig (2021 , chpt. 7), Poole, Mackworth & Goebel (1998 , pp. 268–275), Luger & Stubblefield (2004 , pp. 50–62), Nilsson (1998 , chpt. 15) ^ Logical inference : Russell & Norvig (2021 , chpt. 10) ^ logical deduction as search: Russell & Norvig (2021 , sects. 9.3, 9.4), Poole, Mackworth & Goebel (1998 , pp. ~46–52), Luger & Stubblefield (2004 , pp. 62–73), Nilsson (1998 , chpt. 4.2, 7.2) ^ Resolution and unification : Russell & Norvig (2021 , sections 7.5.2, 9.2, 9.5) ^ Warren, D.H.; Pereira, L.M.; Pereira, F. (1977). \"Prolog-the language and its implementation compared with Lisp\". ACM SIGPLAN Notices . 12 (8): 109– 115. doi : 10.1145/872734.806939 . ^ Fuzzy logic: Russell & Norvig (2021 , pp. 214, 255, 459), Scientific American (1999) ^ a b Stochastic methods for uncertain reasoning: Russell & Norvig (2021 , chpt. 12–18, 20), Poole, Mackworth & Goebel (1998 , pp. 345–395), Luger & Stubblefield (2004 , pp. 165–191, 333–381), Nilsson (1998 , chpt. 19) ^ decision theory and decision analysis : Russell & Norvig (2021 , chpt. 16–18), Poole, Mackworth & Goebel (1998 , pp. 381–394) ^ Information value theory : Russell & Norvig (2021 , sect. 16.6) ^ Markov decision processes and dynamic decision networks : Russell & Norvig (2021 , chpt. 17) ^ a b c Stochastic temporal models: Russell & Norvig (2021 , chpt. 14) Hidden Markov model : Russell & Norvig (2021 , sect. 14.3) Kalman filters : Russell & Norvig (2021 , sect. 14.4) Dynamic Bayesian networks : Russell & Norvig (2021 , sect. 14.5) ^ Game theory and mechanism design : Russell & Norvig (2021 , chpt. 18) ^ Bayesian networks : Russell & Norvig (2021 , sects. 12.5–12.6, 13.4–13.5, 14.3–14.5, 16.5, 20.2–20.3), Poole, Mackworth & Goebel (1998 , pp. 361–381), Luger & Stubblefield (2004 , pp. ~182–190, ≈363–379), Nilsson (1998 , chpt. 19.3–19.4) ^ Bayesian inference algorithm: Russell & Norvig (2021 , sect. 13.3–13.5), Poole, Mackworth & Goebel (1998 , pp. 361–381), Luger & Stubblefield (2004 , pp. ~363–379), Nilsson (1998 , chpt. 19.4 & 7) ^ Bayesian learning and the expectation–maximization algorithm : Russell & Norvig (2021 , chpt. 20), Poole, Mackworth & Goebel (1998 , pp. 424–433), Nilsson (1998 , chpt. 20), Domingos (2015 , p. 210) ^ Bayesian decision theory and Bayesian decision networks : Russell & Norvig (2021 , sect. 16.5) ^ Statistical learning methods and classifiers : Russell & Norvig (2021 , chpt. 20), ^ Ciaramella, Alberto ; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 22,
    "text": "generative AI . Intellisemantic Editions. ISBN 978-8-8947-8760-3 . ^ Decision trees : Russell & Norvig (2021 , sect. 19.3), Domingos (2015 , p. 88) ^ Non-parameteric learning models such as K-nearest neighbor and support vector machines : Russell & Norvig (2021 , sect. 19.7), Domingos (2015 , p. 187) (k-nearest neighbor) ^ Naive Bayes classifier : Russell & Norvig (2021 , sect. 12.6), Domingos (2015 , p. 152) ^ a b Neural networks: Russell & Norvig (2021 , chpt. 21), Domingos (2015 , Chapter 4) ^ Gradient calculation in computational graphs, backpropagation , automatic differentiation : Russell & Norvig (2021 , sect. 21.2), Luger & Stubblefield (2004 , pp. 467–474), Nilsson (1998 , chpt. 3.3) ^ Universal approximation theorem : Russell & Norvig (2021 , p. 752) The theorem: Cybenko (1988) , Hornik, Stinchcombe & White (1989) ^ Feedforward neural networks : Russell & Norvig (2021 , sect. 21.1) ^ Perceptrons : Russell & Norvig (2021 , pp. 21, 22, 683, 22) ^ a b Deep learning : Russell & Norvig (2021 , chpt. 21), Goodfellow, Bengio & Courville (2016) , Hinton et al. (2016) , Schmidhuber (2015) ^ Recurrent neural networks : Russell & Norvig (2021 , sect. 21.6) ^ Convolutional neural networks : Russell & Norvig (2021 , sect. 21.3) ^ Sindhu V, Nivedha S, Prakash M (February 2020). \"An Empirical Science Research on Bioinformatics in Machine Learning\" . Journal of Mechanics of Continua and Mathematical Sciences (7). doi : 10.26782/jmcms.spl.7/2020.02.00006 . ^ Quoted in Christian (2020 , p. 22) ^ Metz, Cade; Weise, Karen (5 May 2025). \"A.I. Hallucinations Are Getting Worse, Even as New Systems Become More Powerful\" . The New York Times . ISSN 0362-4331 . Retrieved 6 May 2025 . ^ \"Explained: Generative AI\" . 9 November 2023. ^ \"AI Writing and Content Creation Tools\" . MIT Sloan Teaching & Learning Technologies. Archived from the original on 25 December 2023 . Retrieved 25 December 2023 . ^ Thomason, James (21 May 2024). \"Mojo Rising: The resurgence of AI-first programming languages\" . VentureBeat . Archived from the original on 27 June 2024 . Retrieved 26 May 2024 . ^ Wodecki, Ben (5 May 2023). \"7 AI Programming Languages You Need to Know\" . AI Business . Archived from the original on 25 July 2024 . Retrieved 5 October 2024 . ^ Plumb, Taryn (18 September 2024). \"Why Jensen Huang and Marc Benioff see 'gigantic' opportunity for agentic AI\" . VentureBeat . Archived from the original on 5 October 2024 . Retrieved 4 October 2024 . ^ Mims, Christopher (19 September 2020). \"Huang's Law Is the New Moore's Law, and Explains Why Nvidia Wants Arm\" . Wall Street Journal . ISSN 0099-9660 . Archived from the original on 2 October 2023 . Retrieved 19 January 2025 . ^ Davenport, T; Kalakota, R (June 2019). \"The potential for artificial intelligence in healthcare\" . Future Healthc J . 6 (2): 94– 98. doi : 10.7861/futurehosp.6-2-94 . PMC 6616181 . PMID 31363513 . ^ Lyakhova, U.A.; Lyakhov, P.A. (2024). \"Systematic review of approaches to detection and classification of skin cancer using artificial intelligence: Development and prospects\" . Computers in Biology and Medicine . 178 : 108742. doi : 10.1016/j.compbiomed.2024.108742 . PMID 38875908 . Archived from the original on 3 December 2024 . Retrieved 10 October 2024 . ^ Alqudaihi, Kawther S.; Aslam, Nida; Khan, Irfan Ullah; Almuhaideb, Abdullah M.; Alsunaidi, Shikah J.; Ibrahim, Nehad M. Abdel Rahman; Alhaidari, Fahd A.; Shaikh, Fatema S.; Alsenbel, Yasmine M.; Alalharith, Dima M.; Alharthi, Hajar M.; Alghamdi, Wejdan M.; Alshahrani, Mohammed S. (2021). \"Cough Sound Detection and Diagnosis Using Artificial Intelligence Techniques: Challenges and Opportunities\" . IEEE Access . 9 : 102327– 102344. Bibcode : 2021IEEEA...9j2327A . doi : 10.1109/ACCESS.2021.3097559 . ISSN 2169-3536 . PMC 8545201 . PMID 34786317 . ^ a b Bax, Monique; Thorpe, Jordan; Romanov, Valentin (December 2023). \"The future of personalized cardiovascular medicine demands 3D and 4D printing, stem cells, and artificial intelligence\" . Frontiers in Sensors . 4 . doi : 10.3389/fsens.2023.1294721 . ISSN 2673-5067 . ^ Dankwa-Mullan, Irene (2024). \"Health Equity and Ethical Considerations in Using Artificial Intelligence in Public Health and Medicine\" . Preventing Chronic Disease . 21 : E64. doi : 10.5888/pcd21.240245 . ISSN 1545-1151 . PMC 11364282 . PMID 39173183 . ^ Jumper, J; Evans, R; Pritzel, A (2021). \"Highly accurate protein structure prediction with AlphaFold\" . Nature . 596 (7873): 583– 589. Bibcode : 2021Natur.596..583J . doi : 10.1038/s41586-021-03819-2 . PMC 8371605 . PMID 34265844 . ^ \"AI discovers new class of antibiotics to kill drug-resistant bacteria\" . 20 December 2023. Archived from the original on 16 September 2024 . Retrieved 5 October 2024 . ^ \"AI speeds up drug design for Parkinson's ten-fold\" . Cambridge University. 17 April 2024. Archived from the original on 5 October 2024 . Retrieved 5 October 2024"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 23,
    "text": ". ^ Horne, Robert I.; Andrzejewska, Ewa A.; Alam, Parvez; Brotzakis, Z. Faidon; Srivastava, Ankit; Aubert, Alice; Nowinska, Magdalena; Gregory, Rebecca C.; Staats, Roxine; Possenti, Andrea; Chia, Sean; Sormanni, Pietro; Ghetti, Bernardino; Caughey, Byron; Knowles, Tuomas P. J.; Vendruscolo, Michele (17 April 2024). \"Discovery of potent inhibitors of α-synuclein aggregation using structure-based iterative learning\" . Nature Chemical Biology . 20 (5). Nature: 634– 645. doi : 10.1038/s41589-024-01580-x . PMC 11062903 . PMID 38632492 . ^ Grant, Eugene F.; Lardner, Rex (25 July 1952). \"The Talk of the Town – It\" . The New Yorker . ISSN 0028-792X . Archived from the original on 16 February 2020 . Retrieved 28 January 2024 . ^ Anderson, Mark Robert (11 May 2017). \"Twenty years on from Deep Blue vs Kasparov: how a chess match started the big data revolution\" . The Conversation . Archived from the original on 17 September 2024 . Retrieved 28 January 2024 . ^ Markoff, John (16 February 2011). \"Computer Wins on 'Jeopardy!': Trivial, It's Not\" . The New York Times . ISSN 0362-4331 . Archived from the original on 22 October 2014 . Retrieved 28 January 2024 . ^ Byford, Sam (27 May 2017). \"AlphaGo retires from competitive Go after defeating world number one 3–0\" . The Verge . Archived from the original on 7 June 2017 . Retrieved 28 January 2024 . ^ Brown, Noam; Sandholm, Tuomas (30 August 2019). \"Superhuman AI for multiplayer poker\" . Science . 365 (6456): 885– 890. Bibcode : 2019Sci...365..885B . doi : 10.1126/science.aay2400 . ISSN 0036-8075 . PMID 31296650 . ^ \"MuZero: Mastering Go, chess, shogi and Atari without rules\" . Google DeepMind . 23 December 2020 . Retrieved 28 January 2024 . ^ Sample, Ian (30 October 2019). \"AI becomes grandmaster in 'fiendishly complex' StarCraft II\" . The Guardian . ISSN 0261-3077 . Archived from the original on 29 December 2020 . Retrieved 28 January 2024 . ^ Wurman, P. R.; Barrett, S.; Kawamoto, K. (2022). \"Outracing champion Gran Turismo drivers with deep reinforcement learning\" (PDF) . Nature . 602 (7896): 223– 228. Bibcode : 2022Natur.602..223W . doi : 10.1038/s41586-021-04357-7 . PMID 35140384 . ^ Wilkins, Alex (13 March 2024). \"Google AI learns to play open-world video games by watching them\" . New Scientist . Archived from the original on 26 July 2024 . Retrieved 21 July 2024 . ^ Wu, Zhengxuan; Arora, Aryaman; Wang, Zheng; Geiger, Atticus; Jurafsky, Dan; Manning, Christopher D.; Potts, Christopher (2024). \"ReFT: Representation Finetuning for Language Models\". NeurIPS . arXiv : 2404.03592 . ^ \"Improving mathematical reasoning with process supervision\" . OpenAI . 31 May 2023 . Retrieved 26 January 2025 . ^ Srivastava, Saurabh (29 February 2024). \"Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap\". arXiv : 2402.19450 [ cs.AI ]. ^ Lightman, Hunter; Kosaraju, Vineet; Burda, Yura; Edwards, Harri; Baker, Bowen; Lee, Teddy; Leike, Jan; Schulman, John; Sutskever, Ilya; Cobbe, Karl (2023). \"Let's Verify Step by Step\". arXiv : 2305.20050v1 [ cs.LG ]. ^ Franzen, Carl (8 August 2024). \"Alibaba claims no. 1 spot in AI math models with Qwen2-Math\" . VentureBeat . Retrieved 16 February 2025 . ^ Franzen, Carl (9 January 2025). \"Microsoft's new rStar-Math technique upgrades small models to outperform OpenAI's o1-preview at math problems\" . VentureBeat . Retrieved 26 January 2025 . ^ Gina Genkina: New AI Model Advances the “Kissing Problem” and More. AlphaEvolve made several mathematical discoveries and practical optimizations IEEE Spectrum 2025-05-14. Retrieved 2025-06-07 ^ Roberts, Siobhan (25 July 2024). \"AI achieves silver-medal standard solving International Mathematical Olympiad problems\" . The New York Times . Archived from the original on 26 September 2024 . Retrieved 7 August 2024 . ^ Azerbayev, Zhangir; Schoelkopf, Hailey; Paster, Keiran; Santos, Marco Dos; McAleer', Stephen; Jiang, Albert Q.; Deng, Jia; Biderman, Stella; Welleck, Sean (16 October 2023). \"Llemma: An Open Language Model For Mathematics\" . EleutherAI Blog . Retrieved 26 January 2025 . ^ \"Julius AI\" . julius.ai . ^ Metz, Cade (21 July 2025). \"Google A.I. System Wins Gold Medal in International Math Olympiad\" . The New York Times . ISSN 0362-4331 . Retrieved 24 July 2025 . ^ McFarland, Alex (12 July 2024). \"8 Best AI for Math Tools (January 2025)\" . Unite.AI . Retrieved 26 January 2025 . ^ Matthew Finio & Amanda Downie: IBM Think 2024 Primer, \"What is Artificial Intelligence (AI) in Finance?\" 8 Dec. 2023 ^ M. Nicolas, J. Firzli: Pensions Age / European Pensions magazine, \"Artificial Intelligence: Ask the Industry\", May–June 2024. https://videovoice.org/ai-in-finance-innovation-entrepreneurship-vs-over-regulation-with-the-eus-artificial-intelligence-act-wont-work-as-intended/ Archived 11 September 2024 at the Wayback Machine . ^ a b c Congressional Research Service (2019). Artificial Intelligence and National Security (PDF) . Washington, DC: Congressional Research Service. Archived (PDF) from the original on 8 May 2020 . Retrieved 25 February 2024 . PD-notice ^ a b Slyusar, Vadym (2019). Artificial intelligence as the basis"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 24,
    "text": "of future control networks (Preprint). doi : 10.13140/RG.2.2.30247.50087 . ^ Iraqi, Amjad (3 April 2024). \" 'Lavender': The AI machine directing Israel's bombing spree in Gaza\" . +972 Magazine . Archived from the original on 10 October 2024 . Retrieved 6 April 2024 . ^ Davies, Harry; McKernan, Bethan; Sabbagh, Dan (1 December 2023). \" 'The Gospel': how Israel uses AI to select bombing targets in Gaza\" . The Guardian . Archived from the original on 6 December 2023 . Retrieved 4 December 2023 . ^ Marti, J Werner (10 August 2024). \"Drohnen haben den Krieg in der Ukraine revolutioniert, doch sie sind empfindlich auf Störsender – deshalb sollen sie jetzt autonom operieren\" . Neue Zürcher Zeitung (in German). Archived from the original on 10 August 2024 . Retrieved 10 August 2024 . ^ Newsom, Gavin; Weber, Shirley N. (5 September 2023). \"Executive Order N-12-23\" (PDF) . Executive Department, State of California. Archived (PDF) from the original on 21 February 2024 . Retrieved 7 September 2023 . ^ ^ \"What is ChatGPT, DALL-E, and generative AI?\" . McKinsey . Archived from the original on 23 April 2023 . Retrieved 14 December 2024 . ^ \"What is generative AI?\" . IBM . 22 March 2024. Archived from the original on 13 December 2024 . Retrieved 13 December 2024 . ^ Pasick, Adam (27 March 2023). \"Artificial Intelligence Glossary: Neural Networks and Other Terms Explained\" . The New York Times . ISSN 0362-4331 . Archived from the original on 1 September 2023 . Retrieved 22 April 2023 . ^ Karpathy, Andrej; Abbeel, Pieter; Brockman, Greg; Chen, Peter; Cheung, Vicki; Duan, Yan; Goodfellow, Ian; Kingma, Durk; Ho, Jonathan; Rein Houthooft; Tim Salimans; John Schulman; Ilya Sutskever; Wojciech Zaremba (16 June 2016). \"Generative models\" . OpenAI. Archived from the original on 17 November 2023 . Retrieved 15 March 2023 . ^ a b Griffith, Erin; Metz, Cade (27 January 2023). \"Anthropic Said to Be Closing In on $300 Million in New A.I. Funding\" . The New York Times . Archived from the original on 9 December 2023 . Retrieved 14 March 2023 . ^ Lanxon, Nate; Bass, Dina; Davalos, Jackie (10 March 2023). \"A Cheat Sheet to AI Buzzwords and Their Meanings\" . Bloomberg News . Archived from the original on 17 November 2023 . Retrieved 14 March 2023 . ^ Metz, Cade (14 March 2023). \"OpenAI Plans to Up the Ante in Tech's A.I. Race\" . The New York Times . ISSN 0362-4331 . Archived from the original on 31 March 2023 . Retrieved 31 March 2023 . ^ Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer, Noam; Kulshreshtha, Apoorv (20 January 2022). \"LaMDA: Language Models for Dialog Applications\". arXiv : 2201.08239 [ cs.CL ]. ^ Roose, Kevin (21 October 2022). \"A Coming-Out Party for Generative A.I., Silicon Valley's New Craze\" . The New York Times . Archived from the original on 15 February 2023 . Retrieved 14 March 2023 . ^ Metz, Cade (15 February 2024). \"OpenAI Unveils A.I. That Instantly Generates Eye-Popping Videos\" . The New York Times . ISSN 0362-4331 . Archived from the original on 15 February 2024 . Retrieved 16 February 2024 . ^ Fink, Charlie. \"LTX Video Breaks The 60-Second Barrier, Redefining AI Video As A Longform Medium\" . Forbes . Retrieved 24 July 2025 . ^ \"The race of the AI labs heats up\" . The Economist . 30 January 2023. Archived from the original on 17 November 2023 . Retrieved 14 March 2023 . ^ Yang, June; Gokturk, Burak (14 March 2023). \"Google Cloud brings generative AI to developers, businesses, and governments\" . Archived from the original on 17 November 2023 . Retrieved 15 March 2023 . ^ Taeihagh, Araz (4 April 2025). \"Governance of Generative AI\" . Policy and Society . 44 (1): 1– 22. doi : 10.1093/polsoc/puaf001 . ISSN 1449-4035 . ^ Simon, Felix M.; Altay, Sacha; Mercier, Hugo (18 October 2023). \"Misinformation reloaded? Fears about the impact of generative AI on misinformation are overblown\" (PDF) . Harvard Kennedy School Misinformation Review . doi : 10.37016/mr-2020-127 . S2CID 264113883 . Retrieved 16 November 2023 . ^ Hendrix, Justin (16 May 2023). \"Transcript: Senate Judiciary Subcommittee Hearing on Oversight of AI\" . techpolicy.press . Archived from the original on 17 November 2023 . Retrieved 19 May 2023 . ^ \"New AI systems collide with copyright law\" . BBC News . 1 August 2023 . Retrieved 28 September 2024 . ^ Poole, David; Mackworth, Alan (2023). Artificial Intelligence, Foundations of Computational Agents (3rd ed.). Cambridge University Press. doi : 10.1017/9781009258227 . ISBN 978-1-0092-5819-7 . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ Russell, Stuart; Norvig, Peter (2020). Artificial Intelligence: A Modern Approach (4th ed.). Pearson. ISBN 978-0-1346-1099-3 . ^ \"Why agents are the next frontier of generative AI\""
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 25,
    "text": ". McKinsey Digital . 24 July 2024. Archived from the original on 3 October 2024 . Retrieved 10 August 2024 . ^ Figueiredo, Mayara Costa; Ankrah, Elizabeth; Powell, Jacquelyn E.; Epstein, Daniel A.; Chen, Yunan (12 January 2024). \"Powered by AI: Examining How AI Descriptions Influence Perceptions of Fertility Tracking Applications\" . Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies . 7 (4): 1– 24. doi : 10.1145/3631414 . ^ Power, Jennifer; Pym, Tinonee; James, Alexandra; Waling, Andrea (5 July 2024). \"Smart Sex Toys: A Narrative Review of Recent Research on Cultural, Health and Safety Considerations\" . Current Sexual Health Reports . 16 (3): 199– 215. doi : 10.1007/s11930-024-00392-3 . ISSN 1548-3592 . ^ Marcantonio, Tiffany L.; Avery, Gracie; Thrash, Anna; Leone, Ruschelle M. (10 September 2024). \"Large Language Models in an App: Conducting a Qualitative Synthetic Data Analysis of How Snapchat's \"My AI\" Responds to Questions About Sexual Consent, Sexual Refusals, Sexual Assault, and Sexting\" . The Journal of Sex Research : 1– 15. doi : 10.1080/00224499.2024.2396457 . ISSN 0022-4499 . PMC 11891083. PMID 39254628 . Archived from the original on 9 December 2024 . Retrieved 9 December 2024 . ^ Hanson, Kenneth R.; Bolthouse, Hannah (2024). \" \"Replika Removing Erotic Role-Play Is Like Grand Theft Auto Removing Guns or Cars\": Reddit Discourse on Artificial Intelligence Chatbots and Sexual Technologies\" . Socius: Sociological Research for a Dynamic World . 10 . doi : 10.1177/23780231241259627 . ISSN 2378-0231 . ^ Mania, Karolina (1 January 2024). \"Legal Protection of Revenge and Deepfake Porn Victims in the European Union: Findings From a Comparative Legal Study\" . Trauma, Violence, & Abuse . 25 (1): 117– 129. doi : 10.1177/15248380221143772 . ISSN 1524-8380 . PMID 36565267 . ^ Singh, Suyesha; Nambiar, Vaishnavi (2024). \"Role of Artificial Intelligence in the Prevention of Online Child Sexual Abuse: A Systematic Review of Literature\" . Journal of Applied Security Research . 19 (4): 586– 627. doi : 10.1080/19361610.2024.2331885 . ISSN 1936-1610 . Archived from the original on 9 December 2024 . Retrieved 9 December 2024 . ^ Razi, Afsaneh; Kim, Seunghyun; Alsoubai, Ashwaq; Stringhini, Gianluca; Solorio, Thamar; De Choudhury, Munmun ; Wisniewski, Pamela J. (13 October 2021). \"A Human-Centered Systematic Literature Review of the Computational Approaches for Online Sexual Risk Detection\" . Proceedings of the ACM on Human-Computer Interaction . 5 (CSCW2): 1– 38. doi : 10.1145/3479609 . ISSN 2573-0142 . Archived from the original on 9 December 2024 . Retrieved 9 December 2024 . ^ Ransbotham, Sam; Kiron, David; Gerbert, Philipp; Reeves, Martin (6 September 2017). \"Reshaping Business With Artificial Intelligence\" . MIT Sloan Management Review . Archived from the original on 13 February 2024. ^ Sun, Yuran; Zhao, Xilei; Lovreglio, Ruggiero; Kuligowski, Erica (1 January 2024), Naser, M. Z. (ed.), \"8 – AI for large-scale evacuation modeling: promises and challenges\" , Interpretable Machine Learning for the Analysis, Design, Assessment, and Informed Decision Making for Civil Infrastructure , Woodhead Publishing Series in Civil and Structural Engineering, Woodhead Publishing, pp. 185– 204, ISBN 978-0-1282-4073-1 , archived from the original on 19 May 2024 , retrieved 28 June 2024 . ^ Gomaa, Islam; Adelzadeh, Masoud; Gwynne, Steven; Spencer, Bruce; Ko, Yoon; Bénichou, Noureddine; Ma, Chunyun; Elsagan, Nour; Duong, Dana; Zalok, Ehab; Kinateder, Max (1 November 2021). \"A Framework for Intelligent Fire Detection and Evacuation System\" . Fire Technology . 57 (6): 3179– 3185. doi : 10.1007/s10694-021-01157-3 . ISSN 1572-8099 . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ Zhao, Xilei; Lovreglio, Ruggiero; Nilsson, Daniel (1 May 2020). \"Modelling and interpreting pre-evacuation decision-making using machine learning\" . Automation in Construction . 113 : 103140. doi : 10.1016/j.autcon.2020.103140 . hdl : 10179/17315 . ISSN 0926-5805 . Archived from the original on 19 May 2024 . Retrieved 5 October 2024 . ^ \"India's latest election embraced AI technology. Here are some ways it was used constructively\" . PBS News . 12 June 2024. Archived from the original on 17 September 2024 . Retrieved 28 October 2024 . ^ \"Экономист Дарон Асемоглу написал книгу об угрозах искусственного интеллекта — и о том, как правильное управление может обратить его на пользу человечеству Спецкор \"Медузы\" Маргарита Лютова узнала у ученого, как скоро мир сможет приблизиться к этой утопии\" . Meduza (in Russian). Archived from the original on 20 June 2023 . Retrieved 21 June 2023 . ^ \"Learning, thinking, artistic collaboration and other such human endeavours in the age of AI\" . The Hindu . 2 June 2023. Archived from the original on 21 June 2023 . Retrieved 21 June 2023 . ^ Müller, Vincent C. (30 April 2020). \"Ethics of Artificial Intelligence and Robotics\" . Stanford Encyclopedia of Philosophy Archive . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ \"Assessing potential future artificial intelligence risks, benefits and"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 26,
    "text": "policy imperatives\" . OECD . 14 November 2024 . Retrieved 1 August 2025 . ^ Kopel, Matthew. \"Copyright Services: Fair Use\" . Cornell University Library . Archived from the original on 26 September 2024 . Retrieved 26 April 2024 . ^ Burgess, Matt. \"How to Stop Your Data From Being Used to Train AI\" . Wired . ISSN 1059-1028 . Archived from the original on 3 October 2024 . Retrieved 26 April 2024 . ^ \"Getting the Innovation Ecosystem Ready for AI. An IP policy toolkit\" (PDF) . WIPO . ^ Hammond, George (27 December 2023). \"Big Tech is spending more than VC firms on AI startups\" . Ars Technica . Archived from the original on 10 January 2024. ^ Wong, Matteo (24 October 2023). \"The Future of AI Is GOMA\" . The Atlantic . Archived from the original on 5 January 2024. ^ \"Big tech and the pursuit of AI dominance\" . The Economist . 26 March 2023. Archived from the original on 29 December 2023. ^ Fung, Brian (19 December 2023). \"Where the battle to dominate AI may be won\" . CNN Business . Archived from the original on 13 January 2024. ^ Metz, Cade (5 July 2023). \"In the Age of A.I., Tech's Little Guys Need Big Friends\" . The New York Times . Archived from the original on 8 July 2024 . Retrieved 5 October 2024 . ^ \"Electricity 2024 – Analysis\" . IEA . 24 January 2024 . Retrieved 13 July 2024 . ^ Calvert, Brian (28 March 2024). \"AI already uses as much energy as a small country. It's only the beginning\" . Vox . New York, New York. Archived from the original on 3 July 2024 . Retrieved 5 October 2024 . ^ Halper, Evan; O'Donovan, Caroline (21 June 2024). \"AI is exhausting the power grid. Tech firms are seeking a miracle solution\" . Washington Post . ^ Davenport, Carly. \"AI Data Centers and the Coming YS Power Demand Surge\" (PDF) . Goldman Sachs . Archived from the original (PDF) on 26 July 2024 . Retrieved 5 October 2024 . ^ Ryan, Carol (12 April 2024). \"Energy-Guzzling AI Is Also the Future of Energy Savings\" . Wall Street Journal . Dow Jones. ^ Hiller, Jennifer (1 July 2024). \"Tech Industry Wants to Lock Up Nuclear Power for AI\" . Wall Street Journal . Dow Jones. Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ Kendall, Tyler (28 September 2024). \"Nvidia's Huang Says Nuclear Power an Option to Feed Data Centers\" . Bloomberg . ^ Halper, Evan (20 September 2024). \"Microsoft deal would reopen Three Mile Island nuclear plant to power AI\" . Washington Post . ^ Hiller, Jennifer (20 September 2024). \"Three Mile Island's Nuclear Plant to Reopen, Help Power Microsoft's AI Centers\" . Wall Street Journal . Dow Jones. Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ a b c Niva Yadav (19 August 2024). \"Taiwan to stop large data centers in the North, cites insufficient power\" . DatacenterDynamics. Archived from the original on 8 November 2024 . Retrieved 7 November 2024 . ^ a b Mochizuki, Takashi; Oda, Shoko (18 October 2024). \"エヌビディア出資の日本企業、原発近くでＡＩデータセンター新設検討\" . Bloomberg (in Japanese). Archived from the original on 8 November 2024 . Retrieved 7 November 2024 . ^ a b Naureen S Malik and Will Wade (5 November 2024). \"Nuclear-Hungry AI Campuses Need New Plan to Find Power Fast\" . Bloomberg. ^ \"Energy and AI Executive summary\" . International Energy Agency . Retrieved 10 April 2025 . ^ Rainie, Lee; Keeter, Scott; Perrin, Andrew (22 July 2019). \"Trust and Distrust in America\" . Pew Research Center . Archived from the original on 22 February 2024. ^ Kosoff, Maya (8 February 2018). \"YouTube Struggles to Contain Its Conspiracy Problem\" . Vanity Fair . Retrieved 10 April 2025 . ^ Berry, David M. (19 March 2025). \"Synthetic media and computational capitalism: towards a critical theory of artificial intelligence\" . AI & Society . doi : 10.1007/s00146-025-02265-2 . ISSN 1435-5655 . ^ \"Unreal: A quantum leap in AI video\" . The Week . 17 June 2025 . Retrieved 20 June 2025 . ^ Snow, Jackie. \"AI video is getting real. Beware what comes next\" . Quartz . Retrieved 20 June 2025 . ^ Chow, Andrew R.; Perrigo, Billy (3 June 2025). \"Google's New AI Tool Generates Convincing Deepfakes of Riots, Conflict, and Election Fraud\" . Time . Retrieved 20 June 2025 . ^ Olanipekun, Samson Olufemi (2025). \"Computational propaganda and misinformation: AI technologies as tools of media manipulation\" . World Journal of Advanced Research and Reviews . 25 (1): 911– 923. doi : 10.30574/wjarr.2025.25.1.0131 . ISSN 2581-9615 . ^ \"To fight AI, we need 'personhood credentials,' say AI firms\" . Archived from the original on 24 April"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 27,
    "text": "2025 . Retrieved 9 May 2025 . ^ a b Samuel, Sigal (19 April 2022). \"Why it's so damn hard to make AI fair and unbiased\" . Vox . Archived from the original on 5 October 2024 . Retrieved 24 July 2024 . ^ Berdahl et al. (2023) ; Goffrey (2008 , p. 17); Rose (2023) ; Russell & Norvig (2021 , p. 995) ^ Christian (2020 , pp. 67–70); Russell & Norvig (2021 , pp. 993–994) ^ Russell & Norvig (2021 , p. 995); Lipartito (2011 , p. 36); Goodman & Flaxman (2017 , p. 6); Christian (2020 , pp. 39–40, 65) ^ Quoted in Christian (2020 , p. 65). ^ Russell & Norvig (2021 , p. 994); Christian (2020 , pp. 40, 80–81) ^ Quoted in Christian (2020 , p. 80) ^ \"Black Box AI\" . 16 June 2023. Archived from the original on 15 June 2024 . Retrieved 5 October 2024 . ^ Christian (2020 , p. 83); Russell & Norvig (2021 , p. 997) ^ Ropek, Lucas (21 May 2024). \"New Anthropic Research Sheds Light on AI's 'Black Box' \" . Gizmodo . Archived from the original on 5 October 2024 . Retrieved 23 May 2024 . ^ Robitzski (2018) ; Sainato (2015) ^ Buckley, Chris; Mozur, Paul (22 May 2019). \"How China Uses High-Tech Surveillance to Subdue Minorities\" . The New York Times . Archived from the original on 25 November 2019 . Retrieved 2 July 2019 . ^ \"Security lapse exposed a Chinese smart city surveillance system\" . 3 May 2019. Archived from the original on 7 March 2021 . Retrieved 14 September 2020 . ^ a b E. McGaughey, 'Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy' (2022), 51(3) Industrial Law Journal 511–559 . Archived 27 May 2023 at the Wayback Machine . ^ Ford & Colvin (2015) ; McGaughey (2022) ^ Lohr (2017) ; Frey & Osborne (2017) ; Arntz, Gregory & Zierahn (2016 , p. 33) ^ Zhou, Viola (11 April 2023). \"AI is already taking video game illustrators' jobs in China\" . Rest of World . Archived from the original on 21 February 2024 . Retrieved 17 August 2023 . ^ Carter, Justin (11 April 2023). \"China's game art industry reportedly decimated by growing AI use\" . Game Developer . Archived from the original on 17 August 2023 . Retrieved 17 August 2023 . ^ Mahdawi (2017) ; Thompson (2014) ^ Tarnoff, Ben (4 August 2023). \"Lessons from Eliza\". The Guardian Weekly . pp. 34– 39. ^ Bostrom (2014) ; Müller & Bostrom (2014) ; Bostrom (2015) . ^ Leaders' concerns about the existential risks of AI around 2015: Rawlinson (2015) , Holley (2015) , Gibbs (2014) , Sainato (2015) ^ \" \"Godfather of artificial intelligence\" talks impact and potential of new AI\" . CBS News . 25 March 2023. Archived from the original on 28 March 2023 . Retrieved 28 March 2023 . ^ Pittis, Don (4 May 2023). \"Canadian artificial intelligence leader Geoffrey Hinton piles on fears of computer takeover\" . CBC . Archived from the original on 7 July 2024 . Retrieved 5 October 2024 . ^ \" '50–50 chance' that AI outsmarts humanity, Geoffrey Hinton says\" . Bloomberg BNN . 14 June 2024. Archived from the original on 14 June 2024 . Retrieved 6 July 2024 . ^ Taylor, Josh (7 May 2023). \"Rise of artificial intelligence is inevitable but should not be feared, 'father of AI' says\" . The Guardian . Archived from the original on 23 October 2023 . Retrieved 26 May 2023 . ^ Colton, Emma (7 May 2023). \" 'Father of AI' says tech fears misplaced: 'You cannot stop it' \" . Fox News . Archived from the original on 26 May 2023 . Retrieved 26 May 2023 . ^ Jones, Hessie (23 May 2023). \"Juergen Schmidhuber, Renowned 'Father Of Modern AI,' Says His Life's Work Won't Lead To Dystopia\" . Forbes . Archived from the original on 26 May 2023 . Retrieved 26 May 2023 . ^ McMorrow, Ryan (19 December 2023). \"Andrew Ng: 'Do we think the world is better off with more or less intelligence?' \" . Financial Times . Archived from the original on 25 January 2024 . Retrieved 30 December 2023 . ^ Levy, Steven (22 December 2023). \"How Not to Be Stupid About AI, With Yann LeCun\" . Wired . Archived from the original on 28 December 2023 . Retrieved 30 December 2023 . ^ Arguments that AI is not an imminent risk: Brooks (2014) , Geist (2015) , Madrigal (2015) , Lee (2014) ^ a b Anderson & Anderson (2011) . ^ Stewart, Ashley; Melton, Monica. \"Hugging Face CEO says he's focused on building a 'sustainable model' for the $4.5 billion open-source-AI startup\" . Business Insider . Archived from the"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 28,
    "text": "original on 25 September 2024 . Retrieved 14 April 2024 . ^ Wiggers, Kyle (9 April 2024). \"Google open sources tools to support AI model development\" . TechCrunch . Archived from the original on 10 September 2024 . Retrieved 14 April 2024 . ^ Heaven, Will Douglas (12 May 2023). \"The open-source AI boom is built on Big Tech's handouts. How long will it last?\" . MIT Technology Review . Retrieved 14 April 2024 . ^ Brodsky, Sascha (19 December 2023). \"Mistral AI's New Language Model Aims for Open Source Supremacy\" . AI Business . Archived from the original on 5 September 2024 . Retrieved 5 October 2024 . ^ Edwards, Benj (22 February 2024). \"Stability announces Stable Diffusion 3, a next-gen AI image generator\" . Ars Technica . Archived from the original on 5 October 2024 . Retrieved 14 April 2024 . ^ Marshall, Matt (29 January 2024). \"How enterprises are using open source LLMs: 16 examples\" . VentureBeat . Archived from the original on 26 September 2024 . Retrieved 5 October 2024 . ^ Piper, Kelsey (2 February 2024). \"Should we make our most powerful AI models open source to all?\" . Vox . Archived from the original on 5 October 2024 . Retrieved 14 April 2024 . ^ Alan Turing Institute (2019). \"Understanding artificial intelligence ethics and safety\" (PDF) . Archived (PDF) from the original on 11 September 2024 . Retrieved 5 October 2024 . ^ Alan Turing Institute (2023). \"AI Ethics and Governance in Practice\" (PDF) . Archived (PDF) from the original on 11 September 2024 . Retrieved 5 October 2024 . ^ Floridi, Luciano; Cowls, Josh (23 June 2019). \"A Unified Framework of Five Principles for AI in Society\" . Harvard Data Science Review . 1 (1). doi : 10.1162/99608f92.8cd550d1 . S2CID 198775713 . Archived from the original on 7 August 2019 . Retrieved 5 December 2023 . ^ Buruk, Banu; Ekmekci, Perihan Elif; Arda, Berna (1 September 2020). \"A critical perspective on guidelines for responsible and trustworthy artificial intelligence\" . Medicine, Health Care and Philosophy . 23 (3): 387– 399. doi : 10.1007/s11019-020-09948-1 . ISSN 1572-8633 . PMID 32236794 . S2CID 214766800 . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ Kamila, Manoj Kumar; Jasrotia, Sahil Singh (1 January 2023). \"Ethical issues in the development of artificial intelligence: recognizing the risks\" . International Journal of Ethics and Systems . 41 (ahead-of-print): 45– 63. doi : 10.1108/IJOES-05-2023-0107 . ISSN 2514-9369 . S2CID 259614124 . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ \"AI Safety Institute releases new AI safety evaluations platform\" . UK Government. 10 May 2024. Archived from the original on 5 October 2024 . Retrieved 14 May 2024 . ^ Regulation of AI to mitigate risks: Berryhill et al. (2019) , Barfield & Pagallo (2018) , Iphofen & Kritikos (2019) , Wirtz, Weyerer & Geyer (2018) , Buiten (2019) ^ VOA News (25 October 2023). \"UN Announces Advisory Body on Artificial Intelligence\" . Archived from the original on 18 September 2024 . Retrieved 5 October 2024 . ^ \"Council of Europe opens first ever global treaty on AI for signature\" . Council of Europe . 5 September 2024. Archived from the original on 17 September 2024 . Retrieved 17 September 2024 . ^ Milmo, Dan (3 November 2023). \"Hope or Horror? The great AI debate dividing its pioneers\". The Guardian Weekly . pp. 10– 12. ^ \"The Bletchley Declaration by Countries Attending the AI Safety Summit, 1–2 November 2023\" . GOV.UK . 1 November 2023. Archived from the original on 1 November 2023 . Retrieved 2 November 2023 . ^ \"Countries agree to safe and responsible development of frontier AI in landmark Bletchley Declaration\" . GOV.UK (Press release). Archived from the original on 1 November 2023 . Retrieved 1 November 2023 . ^ \"Second global AI summit secures safety commitments from companies\" . Reuters. 21 May 2024 . Retrieved 23 May 2024 . ^ \"Frontier AI Safety Commitments, AI Seoul Summit 2024\" . gov.uk. 21 May 2024. Archived from the original on 23 May 2024 . Retrieved 23 May 2024 . ^ a b Buntz, Brian (3 November 2024). \"Quality vs. quantity: US and China chart different paths in global AI patent race in 2024 / Geographical breakdown of AI patents in 2024\" . R&D World. Archived from the original on 9 December 2024. ^ a b c Copeland, J., ed. (2004). The Essential Turing: the ideas that gave birth to the computer age . Oxford, England: Clarendon Press. ISBN 0-1982-5079-7 . ^ \"Google books ngram\" . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . ^ AI's immediate precursors: McCorduck (2004 , pp. 51–107), Crevier (1993 , pp. 27–32), Russell & Norvig (2021"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 29,
    "text": ", pp. 8–17), Moravec (1988 , p. 3) ^ a b Turing's original publication of the Turing test in \" Computing machinery and intelligence \": Turing (1950) Historical influence and philosophical implications: Haugeland (1985 , pp. 6–9), Crevier (1993 , p. 24), McCorduck (2004 , pp. 70–71), Russell & Norvig (2021 , pp. 2, 984) ^ Simon (1965 , p. 96) quoted in Crevier (1993 , p. 109) ^ Minsky (1967 , p. 2) quoted in Crevier (1993 , p. 109) ^ Expert systems : Russell & Norvig (2021 , pp. 23, 292), Luger & Stubblefield (2004 , pp. 227–331), Nilsson (1998 , chpt. 17.4), McCorduck (2004 , pp. 327–335, 434–435), Crevier (1993 , pp. 145–162, 197–203), Newquist (1994 , pp. 155–183) ^ Developmental robotics : Weng et al. (2001) , Lungarella et al. (2003) , Asada et al. (2009) , Oudeyer (2010) ^ Crevier (1993 , pp. 214–215), Russell & Norvig (2021 , pp. 24, 26) ^ Formal and narrow methods adopted in the 1990s: Russell & Norvig (2021 , pp. 24–26), McCorduck (2004 , pp. 486–487) ^ AI widely used in the late 1990s: Kurzweil (2005 , p. 265), NRC (1999 , pp. 216–222), Newquist (1994 , pp. 189–201) ^ Moore's Law and AI: Russell & Norvig (2021 , pp. 14, 27) ^ Big data : Russell & Norvig (2021 , p. 26) ^ Sagar, Ram (3 June 2020). \"OpenAI Releases GPT-3, The Largest Model So Far\" . Analytics India Magazine . Archived from the original on 4 August 2020 . Retrieved 15 March 2023 . ^ Milmo, Dan (2 February 2023). \"ChatGPT reaches 100 million users two months after launch\" . The Guardian . ISSN 0261-3077 . Archived from the original on 3 February 2023 . Retrieved 31 December 2024 . ^ Gorichanaz, Tim (29 November 2023). \"ChatGPT turns 1: AI chatbot's success says as much about humans as technology\" . The Conversation . Archived from the original on 31 December 2024 . Retrieved 31 December 2024 . ^ \"Nearly 1 in 4 new startups is an AI company\" . PitchBook . 24 December 2024 . Retrieved 3 January 2025 . ^ Grayling, Anthony; Ball, Brian (1 August 2024). \"Philosophy is crucial in the age of AI\" . The Conversation . Archived from the original on 5 October 2024 . Retrieved 4 October 2024 . ^ a b Jarow, Oshan (15 June 2024). \"Will AI ever become conscious? It depends on how you think about biology\" . Vox . Archived from the original on 21 September 2024 . Retrieved 4 October 2024 . ^ McCarthy, John. \"The Philosophy of AI and the AI of Philosophy\" . jmc.stanford.edu . Archived from the original on 23 October 2018 . Retrieved 3 October 2024 . ^ Kirk-Giannini, Cameron Domenico; Goldstein, Simon (16 October 2023). \"AI is closer than ever to passing the Turing test for 'intelligence'. What happens when it does?\" . The Conversation . Archived from the original on 25 September 2024 . Retrieved 17 August 2024 . ^ \"What Is Artificial Intelligence (AI)?\" . Google Cloud Platform . Archived from the original on 31 July 2023 . Retrieved 16 October 2023 . ^ \"One of the Biggest Problems in Regulating AI Is Agreeing on a Definition\" . Carnegie Endowment for International Peace . Retrieved 31 July 2024 . ^ \"AI or BS? How to tell if a marketing tool really uses artificial intelligence\" . The Drum . Retrieved 31 July 2024 . ^ Musser, George (1 September 2023). \"How AI Knows Things No One Told It\" . Scientific American . Retrieved 17 July 2025 . ^ Haugeland (1985) , pp. 112–117. ^ Physical symbol system hypothesis: Newell & Simon (1976 , p. 116) Historical significance: McCorduck (2004 , p. 153), Russell & Norvig (2021 , p. 19) ^ Moravec's paradox : Moravec (1988 , pp. 15–16), Minsky (1986 , p. 29), Pinker (2007 , pp. 190–191) ^ Dreyfus' critique of AI : Dreyfus (1972) , Dreyfus & Dreyfus (1986) Historical significance and philosophical implications: Crevier (1993 , pp. 120–132), McCorduck (2004 , pp. 211–239), Russell & Norvig (2021 , pp. 981–982), Fearn (2007 , chpt. 3) ^ Neats vs. scruffies , the historic debate: McCorduck (2004 , pp. 421–424, 486–489), Crevier (1993 , p. 168), Nilsson (1983 , pp. 10–11), Russell & Norvig (2021 , p. 24) A classic example of the \"scruffy\" approach to intelligence: Minsky (1986) A modern example of neat AI and its aspirations in the 21st century: Domingos (2015) ^ Searle's Chinese room argument: Searle (1980) . Searle's original presentation of the thought experiment., Searle (1999) . Discussion: Russell & Norvig (2021 , pp. 985), McCorduck (2004 , pp. 443–445), Crevier (1993 , pp. 269–271) ^ Leith, Sam (7 July 2022). \"Nick Bostrom: How can we be certain a machine isn't conscious?\" . The"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 30,
    "text": "Spectator . Archived from the original on 26 September 2024 . Retrieved 23 February 2024 . ^ a b c Thomson, Jonny (31 October 2022). \"Why don't robots have rights?\" . Big Think . Archived from the original on 13 September 2024 . Retrieved 23 February 2024 . ^ a b Kateman, Brian (24 July 2023). \"AI Should Be Terrified of Humans\" . Time . Archived from the original on 25 September 2024 . Retrieved 23 February 2024 . ^ Wong, Jeff (10 July 2023). \"What leaders need to know about robot rights\" . Fast Company . ^ Hern, Alex (12 January 2017). \"Give robots 'personhood' status, EU committee argues\" . The Guardian . ISSN 0261-3077 . Archived from the original on 5 October 2024 . Retrieved 23 February 2024 . ^ Dovey, Dana (14 April 2018). \"Experts Don't Think Robots Should Have Rights\" . Newsweek . Archived from the original on 5 October 2024 . Retrieved 23 February 2024 . ^ Cuddy, Alice (13 April 2018). \"Robot rights violate human rights, experts warn EU\" . euronews . Archived from the original on 19 September 2024 . Retrieved 23 February 2024 . ^ The Intelligence explosion and technological singularity : Russell & Norvig (2021 , pp. 1004–1005), Omohundro (2008) , Kurzweil (2005) I. J. Good 's \"intelligence explosion\": Good (1965) Vernor Vinge 's \"singularity\": Vinge (1993) ^ Transhumanism : Moravec (1988) , Kurzweil (2005) , Russell & Norvig (2021 , p. 1005) ^ AI as evolution: Edward Fredkin is quoted in McCorduck (2004 , p. 401), Butler (1863) , Dyson (1998) ^ AI in myth: McCorduck (2004 , pp. 4–5) ^ Anderson (2008) . AI textbooks The two most widely used textbooks in 2023 (see the Open Syllabus ): The four most widely used AI textbooks in 2008: Luger, George ; Stubblefield, William (2004). Artificial Intelligence: Structures and Strategies for Complex Problem Solving (5th ed.). Benjamin/Cummings. ISBN 978-0-8053-4780-7 . Archived from the original on 26 July 2020 . Retrieved 17 December 2019 . Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis . Morgan Kaufmann. ISBN 978-1-5586-0467-4 . Archived from the original on 26 July 2020 . Retrieved 18 November 2019 . Russell, Stuart J. ; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2 . Poole, David ; Mackworth, Alan ; Goebel, Randy (1998). Computational Intelligence: A Logical Approach . New York: Oxford University Press. ISBN 978-0-1951-0270-3 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Later edition: Poole, David; Mackworth, Alan (2017). Artificial Intelligence: Foundations of Computational Agents (2nd ed.). Cambridge University Press. ISBN 978-1-1071-9539-4 . Archived from the original on 7 December 2017 . Retrieved 6 December 2017 . Other textbooks: History of AI Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence . New York, NY: BasicBooks. ISBN 0-465-02997-3 . McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, Massachusetts: A. K. Peters, ISBN 1-5688-1205-1 Newquist, H. P. (1994). The Brain Makers: Genius, Ego, And Greed In The Quest For Machines That Think . New York: Macmillan/SAMS. ISBN 978-0-6723-0412-5 . Harmon, Paul; Sawyer, Brian (1990). Creating Expert Systems for Business and Industry . New York: John Wiley & Sons. ISBN 0471614963 . Other sources AI & ML in Fusion AI & ML in Fusion, video lecture Archived 2 July 2023 at the Wayback Machine Alter, Alexandra; Harris, Elizabeth A. (20 September 2023), \"Franzen, Grisham and Other Prominent Authors Sue OpenAI\" , The New York Times , archived from the original on 14 September 2024 , retrieved 5 October 2024 Altman, Sam ; Brockman, Greg ; Sutskever, Ilya (22 May 2023). \"Governance of Superintelligence\" . openai.com . Archived from the original on 27 May 2023 . Retrieved 27 May 2023 . Anderson, Susan Leigh (2008). \"Asimov's \"three laws of robotics\" and machine metaethics\". AI & Society . 22 (4): 477– 493. doi : 10.1007/s00146-007-0094-5 . S2CID 1809459 . Anderson, Michael; Anderson, Susan Leigh (2011). Machine Ethics . Cambridge University Press. Arntz, Melanie; Gregory, Terry; Zierahn, Ulrich (2016), \"The risk of automation for jobs in OECD countries: A comparative analysis\", OECD Social, Employment, and Migration Working Papers 189 Asada, M.; Hosoda, K.; Kuniyoshi, Y.; Ishiguro, H.; Inui, T.; Yoshikawa, Y.; Ogino, M.; Yoshida, C. (2009). \"Cognitive developmental robotics: a survey\". IEEE Transactions on Autonomous Mental Development . 1 (1): 12– 34. doi : 10.1109/tamd.2009.2021702 . S2CID 10168773 . \"Ask the AI experts: What's driving today's progress in AI?\" . McKinsey & Company . Archived from the original on 13 April 2018 . Retrieved 13 April 2018 . Barfield, Woodrow; Pagallo, Ugo (2018). Research handbook on the law of artificial intelligence . Cheltenham, UK: Edward Elgar Publishing. ISBN 978-1-7864-3904-8 . OCLC 1039480085 . Beal, J.; Winston, Patrick (2009), \"The New Frontier of Human-Level"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 31,
    "text": "Artificial Intelligence\", IEEE Intelligent Systems , vol. 24, pp. 21– 24, doi : 10.1109/MIS.2009.75 , hdl : 1721.1/52357 , S2CID 32437713 Berdahl, Carl Thomas; Baker, Lawrence; Mann, Sean; Osoba, Osonde; Girosi, Federico (7 February 2023). \"Strategies to Improve the Impact of Artificial Intelligence on Health Equity: Scoping Review\" . JMIR AI . 2 : e42936. doi : 10.2196/42936 . ISSN 2817-1705 . PMC 11041459 . PMID 38875587 . S2CID 256681439 . Berryhill, Jamie; Heang, Kévin Kok; Clogher, Rob; McBride, Keegan (2019). Hello, World: Artificial Intelligence and its Use in the Public Sector (PDF) . Paris: OECD Observatory of Public Sector Innovation. Archived (PDF) from the original on 20 December 2019 . Retrieved 9 August 2020 . Bertini, M; Del Bimbo, A; Torniai, C (2006). \"Automatic annotation and semantic retrieval of video sequences using multimedia ontologies\". MM '06 Proceedings of the 14th ACM international conference on Multimedia . 14th ACM international conference on Multimedia. Santa Barbara: ACM. pp. 679– 682. Bostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies . Oxford University Press. Bostrom, Nick (2015). \"What happens when our computers get smarter than we are?\" . TED (conference) . Archived from the original on 25 July 2020 . Retrieved 30 January 2020 . Brooks, Rodney (10 November 2014). \"artificial intelligence is a tool, not a threat\" . Archived from the original on 12 November 2014. Brooks, Rodney (1990). \"Elephants Don't Play Chess\" (PDF) . Robotics and Autonomous Systems . 6 ( 1– 2): 3– 15. CiteSeerX 10.1.1.588.7539 . doi : 10.1016/S0921-8890(05)80025-9 . Archived (PDF) from the original on 9 August 2007. Buiten, Miriam C (2019). \"Towards Intelligent Regulation of Artificial Intelligence\" . European Journal of Risk Regulation . 10 (1): 41– 59. doi : 10.1017/err.2019.8 . ISSN 1867-299X . Bushwick, Sophie (16 March 2023), \"What the New GPT-4 AI Can Do\" , Scientific American , archived from the original on 22 August 2023 , retrieved 5 October 2024 Butler, Samuel (13 June 1863). \"Darwin among the Machines\" . Letters to the Editor. The Press . Christchurch, New Zealand. Archived from the original on 19 September 2008 . Retrieved 16 October 2014 – via Victoria University of Wellington. Buttazzo, G. (July 2001). \"Artificial consciousness: Utopia or real possibility?\". Computer . 34 (7): 24– 30. doi : 10.1109/2.933500 . Cambria, Erik; White, Bebo (May 2014). \"Jumping NLP Curves: A Review of Natural Language Processing Research [Review Article]\". IEEE Computational Intelligence Magazine . 9 (2): 48– 57. doi : 10.1109/MCI.2014.2307227 . S2CID 206451986 . Cellan-Jones, Rory (2 December 2014). \"Stephen Hawking warns artificial intelligence could end mankind\" . BBC News . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Chalmers, David (1995). \"Facing up to the problem of consciousness\" . Journal of Consciousness Studies . 2 (3): 200– 219. CiteSeerX 10.1.1.103.8362 . Archived from the original on 8 March 2005 . Retrieved 11 October 2018 . Challa, Subhash; Moreland, Mark R.; Mušicki, Darko; Evans, Robin J. (2011). Fundamentals of Object Tracking . Cambridge University Press. doi : 10.1017/CBO9780511975837 . ISBN 978-0-5218-7628-5 . Christian, Brian (2020). The Alignment Problem : Machine learning and human values . W. W. Norton & Company. ISBN 978-0-3938-6833-3 . OCLC 1233266753 . Ciresan, D.; Meier, U.; Schmidhuber, J. (2012). \"Multi-column deep neural networks for image classification\". 2012 IEEE Conference on Computer Vision and Pattern Recognition . pp. 3642– 3649. arXiv : 1202.2745 . doi : 10.1109/cvpr.2012.6248110 . ISBN 978-1-4673-1228-8 . S2CID 2161592 . Clark, Jack (2015b). \"Why 2015 Was a Breakthrough Year in Artificial Intelligence\" . Bloomberg.com . Archived from the original on 23 November 2016 . Retrieved 23 November 2016 . CNA (12 January 2019). \"Commentary: Bad news. Artificial intelligence is biased\" . CNA . Archived from the original on 12 January 2019 . Retrieved 19 June 2020 . Cybenko, G. (1988). Continuous valued neural networks with two hidden layers are sufficient (Report). Department of Computer Science, Tufts University. Deng, L.; Yu, D. (2014). \"Deep Learning: Methods and Applications\" (PDF) . Foundations and Trends in Signal Processing . 7 ( 3– 4): 197– 387. doi : 10.1561/2000000039 . Archived (PDF) from the original on 14 March 2016 . Retrieved 18 October 2014 . Dennett, Daniel (1991). Consciousness Explained . The Penguin Press. ISBN 978-0-7139-9037-9 . DiFeliciantonio, Chase (3 April 2023). \"AI has already changed the world. This report shows how\" . San Francisco Chronicle . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Dickson, Ben (2 May 2022). \"Machine learning: What is the transformer architecture?\" . TechTalks . Archived from the original on 22 November 2023 . Retrieved 22 November 2023 . Dockrill, Peter (27 June 2022), \"Robots With Flawed AI Make Sexist And Racist Decisions, Experiment Shows\" , Science Alert , archived from the original on 27 June 2022 Domingos, Pedro (2015). The Master Algorithm: How the"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 32,
    "text": "Quest for the Ultimate Learning Machine Will Remake Our World . Basic Books . ISBN 978-0-4650-6570-7 . Dreyfus, Hubert (1972). What Computers Can't Do . New York: MIT Press. ISBN 978-0-0601-1082-6 . Dreyfus, Hubert ; Dreyfus, Stuart (1986). Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer . Oxford: Blackwell. ISBN 978-0-0290-8060-3 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Dyson, George (1998). Darwin among the Machines . Allan Lane Science. ISBN 978-0-7382-0030-9 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Edelson, Edward (1991). The Nervous System . New York: Chelsea House. ISBN 978-0-7910-0464-7 . Archived from the original on 26 July 2020 . Retrieved 18 November 2019 . Edwards, Benj (17 May 2023). \"Poll: AI poses risk to humanity, according to majority of Americans\" . Ars Technica . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Fearn, Nicholas (2007). The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World's Greatest Thinkers . New York: Grove Press. ISBN 978-0-8021-1839-4 . Ford, Martin; Colvin, Geoff (6 September 2015). \"Will robots create more jobs than they destroy?\" . The Guardian . Archived from the original on 16 June 2018 . Retrieved 13 January 2018 . Fox News (2023). \"Fox News Poll\" (PDF) . Fox News. Archived (PDF) from the original on 12 May 2023 . Retrieved 19 June 2023 . Frey, Carl Benedikt; Osborne, Michael A (1 January 2017). \"The future of employment: How susceptible are jobs to computerisation?\". Technological Forecasting and Social Change . 114 : 254– 280. CiteSeerX 10.1.1.395.416 . doi : 10.1016/j.techfore.2016.08.019 . ISSN 0040-1625 . \"From not working to neural networking\" . The Economist . 2016. Archived from the original on 31 December 2016 . Retrieved 26 April 2018 . Galvan, Jill (1 January 1997). \"Entering the Posthuman Collective in Philip K. Dick's \"Do Androids Dream of Electric Sheep?\" \". Science Fiction Studies . 24 (3): 413– 429. doi : 10.1525/sfs.24.3.0413 . JSTOR 4240644 . Geist, Edward Moore (9 August 2015). \"Is artificial intelligence really an existential threat to humanity?\" . Bulletin of the Atomic Scientists . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Gibbs, Samuel (27 October 2014). \"Elon Musk: artificial intelligence is our biggest existential threat\" . The Guardian . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Goffrey, Andrew (2008). \"Algorithm\". In Fuller, Matthew (ed.). Software studies: a lexicon . Cambridge, Mass.: MIT Press. pp. 15 –20. ISBN 978-1-4356-4787-9 . Goldman, Sharon (14 September 2022). \"10 years later, deep learning 'revolution' rages on, say AI pioneers Hinton, LeCun and Li\" . VentureBeat . Archived from the original on 5 October 2024 . Retrieved 8 December 2023 . Good, I. J. (1965), Speculations Concerning the First Ultraintelligent Machine , archived from the original on 10 July 2023 , retrieved 5 October 2024 Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016), Deep Learning , MIT Press., archived from the original on 16 April 2016 , retrieved 12 November 2017 Goodman, Bryce; Flaxman, Seth (2017). \"EU regulations on algorithmic decision-making and a 'right to explanation' \". AI Magazine . 38 (3): 50. arXiv : 1606.08813 . doi : 10.1609/aimag.v38i3.2741 . S2CID 7373959 . Government Accountability Office (13 September 2022). Consumer Data: Increasing Use Poses Risks to Privacy . gao.gov (Report). Archived from the original on 13 September 2024 . Retrieved 5 October 2024 . Grant, Nico; Hill, Kashmir (22 May 2023). \"Google's Photo App Still Can't Find Gorillas. And Neither Can Apple's\" . The New York Times . Archived from the original on 14 September 2024 . Retrieved 5 October 2024 . Goswami, Rohan (5 April 2023). \"Here's where the A.I. jobs are\" . CNBC . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Harari, Yuval Noah (October 2018). \"Why Technology Favors Tyranny\" . The Atlantic . Archived from the original on 25 September 2021 . Retrieved 23 September 2021 . Harari, Yuval Noah (2023). \"AI and the future of humanity\" . YouTube . Archived from the original on 30 September 2024 . Retrieved 5 October 2024 . Haugeland, John (1985). Artificial Intelligence: The Very Idea . Cambridge, Mass.: MIT Press. ISBN 978-0-2620-8153-5 . Hinton, G.; Deng, L.; Yu, D.; Dahl, G.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T. ; Kingsbury, B. (2012). \"Deep Neural Networks for Acoustic Modeling in Speech Recognition – The shared views of four research groups\". IEEE Signal Processing Magazine . 29 (6): 82– 97. Bibcode : 2012ISPM...29...82H . doi : 10.1109/msp.2012.2205597 . S2CID 206485943 . Holley, Peter (28 January 2015). \"Bill Gates on dangers of artificial intelligence: 'I don't understand"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 33,
    "text": "why some people are not concerned' \" . The Washington Post . ISSN 0190-8286 . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Hornik, Kurt; Stinchcombe, Maxwell; White, Halbert (1989). Multilayer Feedforward Networks are Universal Approximators (PDF) . Neural Networks . Vol. 2. Pergamon Press. pp. 359– 366. Archived (PDF) from the original on 21 April 2023 . Retrieved 5 October 2024 . Horst, Steven (2005). \"The Computational Theory of Mind\" . The Stanford Encyclopedia of Philosophy . Archived from the original on 6 March 2016 . Retrieved 7 March 2016 . Howe, J. (November 1994). \"Artificial Intelligence at Edinburgh University: a Perspective\" . Archived from the original on 15 May 2007 . Retrieved 30 August 2007 . IGM Chicago (30 June 2017). \"Robots and Artificial Intelligence\" . igmchicago.org . Archived from the original on 1 May 2019 . Retrieved 3 July 2019 . Iphofen, Ron; Kritikos, Mihalis (3 January 2019). \"Regulating artificial intelligence and robotics: ethics by design in a digital society\". Contemporary Social Science . 16 (2): 170– 184. doi : 10.1080/21582041.2018.1563803 . ISSN 2158-2041 . S2CID 59298502 . Jordan, M. I.; Mitchell, T. M. (16 July 2015). \"Machine learning: Trends, perspectives, and prospects\". Science . 349 (6245): 255– 260. Bibcode : 2015Sci...349..255J . doi : 10.1126/science.aaa8415 . PMID 26185243 . S2CID 677218 . Kahneman, Daniel (2011). Thinking, Fast and Slow . Macmillan. ISBN 978-1-4299-6935-2 . Archived from the original on 15 March 2023 . Retrieved 8 April 2012 . Kahneman, Daniel ; Slovic, D.; Tversky, Amos (1982). \"Judgment under uncertainty: Heuristics and biases\". Science . 185 (4157). New York: Cambridge University Press: 1124– 1131. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . ISBN 978-0-5212-8414-1 . PMID 17835457 . S2CID 143452957 . Kasperowicz, Peter (1 May 2023). \"Regulate AI? GOP much more skeptical than Dems that government can do it right: poll\" . Fox News . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Katz, Yarden (1 November 2012). \"Noam Chomsky on Where Artificial Intelligence Went Wrong\" . The Atlantic . Archived from the original on 28 February 2019 . Retrieved 26 October 2014 . \"Kismet\" . MIT Artificial Intelligence Laboratory, Humanoid Robotics Group. Archived from the original on 17 October 2014 . Retrieved 25 October 2014 . Kissinger, Henry (1 November 2021). \"The Challenge of Being Human in the Age of AI\" . The Wall Street Journal . Archived from the original on 4 November 2021 . Retrieved 4 November 2021 . Kobielus, James (27 November 2019). \"GPUs Continue to Dominate the AI Accelerator Market for Now\" . InformationWeek . Archived from the original on 19 October 2021 . Retrieved 11 June 2020 . Kuperman, G. J.; Reichley, R. M.; Bailey, T. C. (1 July 2006). \"Using Commercial Knowledge Bases for Clinical Decision Support: Opportunities, Hurdles, and Recommendations\" . Journal of the American Medical Informatics Association . 13 (4): 369– 371. doi : 10.1197/jamia.M2055 . PMC 1513681 . PMID 16622160 . Kurzweil, Ray (2005). The Singularity is Near . Penguin Books. ISBN 978-0-6700-3384-3 . Langley, Pat (2011). \"The changing science of machine learning\" . Machine Learning . 82 (3): 275– 279. doi : 10.1007/s10994-011-5242-y . Larson, Jeff; Angwin, Julia (23 May 2016). \"How We Analyzed the COMPAS Recidivism Algorithm\" . ProPublica . Archived from the original on 29 April 2019 . Retrieved 19 June 2020 . Laskowski, Nicole (November 2023). \"What is Artificial Intelligence and How Does AI Work? TechTarget\" . Enterprise AI . Archived from the original on 5 October 2024 . Retrieved 30 October 2023 . Law Library of Congress (U.S.). Global Legal Research Directorate, issuing body. (2019). Regulation of artificial intelligence in selected jurisdictions . LCCN 2019668143 . OCLC 1110727808 . Lee, Timothy B. (22 August 2014). \"Will artificial intelligence destroy humanity? Here are 5 reasons not to worry\" . Vox . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Lenat, Douglas ; Guha, R. V. (1989). Building Large Knowledge-Based Systems . Addison-Wesley. ISBN 978-0-2015-1752-1 . Lighthill, James (1973). \"Artificial Intelligence: A General Survey\". Artificial Intelligence: a paper symposium . Science Research Council. Lipartito, Kenneth (6 January 2011), The Narrative and the Algorithm: Genres of Credit Reporting from the Nineteenth Century to Today (PDF) (Unpublished manuscript), doi : 10.2139/ssrn.1736283 , S2CID 166742927 , archived (PDF) from the original on 9 October 2022 Lohr, Steve (2017). \"Robots Will Take Jobs, but Not as Fast as Some Fear, New Report Says\" . The New York Times . Archived from the original on 14 January 2018 . Retrieved 13 January 2018 . Lungarella, M.; Metta, G.; Pfeifer, R.; Sandini, G. (2003). \"Developmental robotics: a survey\". Connection Science . 15 (4): 151– 190. Bibcode : 2003ConSc..15..151L . CiteSeerX 10.1.1.83.7615 . doi : 10.1080/09540090310001655110 . S2CID 1452734 . \"Machine Ethics\" ."
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 34,
    "text": "aaai.org . Archived from the original on 29 November 2014. Madrigal, Alexis C. (27 February 2015). \"The case against killer robots, from a guy actually working on artificial intelligence\" . Fusion.net . Archived from the original on 4 February 2016 . Retrieved 31 January 2016 . Mahdawi, Arwa (26 June 2017). \"What jobs will still be around in 20 years? Read this to prepare your future\" . The Guardian . Archived from the original on 14 January 2018 . Retrieved 13 January 2018 . Maker, Meg Houston (2006), AI@50: AI Past, Present, Future , Dartmouth College, archived from the original on 8 October 2008 , retrieved 16 October 2008 Marmouyet, Françoise (15 December 2023). \"Google's Gemini: is the new AI model really better than ChatGPT?\" . The Conversation . Archived from the original on 4 March 2024 . Retrieved 25 December 2023 . Minsky, Marvin (1986), The Society of Mind , Simon and Schuster McCarthy, John ; Minsky, Marvin ; Rochester, Nathan ; Shannon, Claude (1955). \"A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence\" . Archived from the original on 26 August 2007 . Retrieved 30 August 2007 . McCarthy, John (2007), \"From Here to Human-Level AI\", Artificial Intelligence , p. 171 McCarthy, John (1999), What is AI? , archived from the original on 4 December 2022 , retrieved 4 December 2022 McCauley, Lee (2007). \"AI armageddon and the three laws of robotics\". Ethics and Information Technology . 9 (2): 153– 164. CiteSeerX 10.1.1.85.8904 . doi : 10.1007/s10676-007-9138-2 . S2CID 37272949 . McGarry, Ken (1 December 2005). \"A survey of interestingness measures for knowledge discovery\". The Knowledge Engineering Review . 20 (1): 39– 61. doi : 10.1017/S0269888905000408 . S2CID 14987656 . McGaughey, E (2022), Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy , p. 51(3) Industrial Law Journal 511–559, doi : 10.2139/ssrn.3044448 , S2CID 219336439 , SSRN 3044448 , archived from the original on 31 January 2021 , retrieved 27 May 2023 Merkle, Daniel; Middendorf, Martin (2013). \"Swarm Intelligence\". In Burke, Edmund K.; Kendall, Graham (eds.). Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques . Springer Science & Business Media. ISBN 978-1-4614-6940-7 . Minsky, Marvin (1967), Computation: Finite and Infinite Machines , Englewood Cliffs, N.J.: Prentice-Hall Moravec, Hans (1988). Mind Children . Harvard University Press. ISBN 978-0-6745-7616-2 . Archived from the original on 26 July 2020 . Retrieved 18 November 2019 . Morgenstern, Michael (9 May 2015). \"Automation and anxiety\" . The Economist . Archived from the original on 12 January 2018 . Retrieved 13 January 2018 . Müller, Vincent C.; Bostrom, Nick (2014). \"Future Progress in Artificial Intelligence: A Poll Among Experts\" (PDF) . AI Matters . 1 (1): 9– 11. doi : 10.1145/2639475.2639478 . S2CID 8510016 . Archived (PDF) from the original on 15 January 2016. Neumann, Bernd; Möller, Ralf (January 2008). \"On scene interpretation with description logics\". Image and Vision Computing . 26 (1): 82– 101. doi : 10.1016/j.imavis.2007.08.013 . S2CID 10767011 . Nilsson, Nils (1995), \"Eyes on the Prize\", AI Magazine , vol. 16, pp. 9– 17 Newell, Allen ; Simon, H. A. (1976). \"Computer Science as Empirical Inquiry: Symbols and Search\" . Communications of the ACM . 19 (3): 113– 126. doi : 10.1145/360018.360022 . Nicas, Jack (7 February 2018). \"How YouTube Drives People to the Internet's Darkest Corners\" . The Wall Street Journal . ISSN 0099-9660 . Archived from the original on 5 October 2024 . Retrieved 16 June 2018 . Nilsson, Nils (1983). \"Artificial Intelligence Prepares for 2001\" (PDF) . AI Magazine . 1 (1). Archived (PDF) from the original on 17 August 2020 . Retrieved 22 August 2020 . Presidential Address to the Association for the Advancement of Artificial Intelligence . NRC (United States National Research Council) (1999). \"Developments in Artificial Intelligence\". Funding a Revolution: Government Support for Computing Research . National Academy Press. Omohundro, Steve (2008). The Nature of Self-Improving Artificial Intelligence . presented and distributed at the 2007 Singularity Summit, San Francisco, CA. Oudeyer, P-Y. (2010). \"On the impact of robotics in behavioral and cognitive sciences: from insect navigation to human cognitive development\" (PDF) . IEEE Transactions on Autonomous Mental Development . 2 (1): 2– 16. doi : 10.1109/tamd.2009.2039057 . S2CID 6362217 . Archived (PDF) from the original on 3 October 2018 . Retrieved 4 June 2013 . Pennachin, C.; Goertzel, B. (2007). \"Contemporary Approaches to Artificial General Intelligence\". Artificial General Intelligence . Cognitive Technologies. Berlin, Heidelberg: Springer. pp. 1– 30. doi : 10.1007/978-3-540-68677-4_1 . ISBN 978-3-5402-3733-4 . Pinker, Steven (2007) [1994], The Language Instinct , Perennial Modern Classics, Harper, ISBN 978-0-0613-3646-1 Poria, Soujanya; Cambria, Erik; Bajpai, Rajiv; Hussain, Amir (September 2017). \"A review of affective computing: From unimodal analysis to multimodal fusion\" . Information Fusion . 37 : 98– 125. doi : 10.1016/j.inffus.2017.02.003 . hdl : 1893/25490 . S2CID"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 35,
    "text": "205433041 . Archived from the original on 23 March 2023 . Retrieved 27 April 2021 . Rawlinson, Kevin (29 January 2015). \"Microsoft's Bill Gates insists AI is a threat\" . BBC News . Archived from the original on 29 January 2015 . Retrieved 30 January 2015 . Reisner, Alex (19 August 2023), \"Revealed: The Authors Whose Pirated Books are Powering Generative AI\" , The Atlantic , archived from the original on 3 October 2024 , retrieved 5 October 2024 Roberts, Jacob (2016). \"Thinking Machines: The Search for Artificial Intelligence\" . Distillations . Vol. 2, no. 2. pp. 14– 23. Archived from the original on 19 August 2018 . Retrieved 20 March 2018 . Robitzski, Dan (5 September 2018). \"Five experts share what scares them the most about AI\" . Archived from the original on 8 December 2019 . Retrieved 8 December 2019 . Rose, Steve (11 July 2023). \"AI Utopia or dystopia?\". The Guardian Weekly . pp. 42– 43. Russell, Stuart (2019). Human Compatible: Artificial Intelligence and the Problem of Control . United States: Viking. ISBN 978-0-5255-5861-3 . OCLC 1083694322 . Sainato, Michael (19 August 2015). \"Stephen Hawking, Elon Musk, and Bill Gates Warn About Artificial Intelligence\" . Observer . Archived from the original on 30 October 2015 . Retrieved 30 October 2015 . Sample, Ian (5 November 2017). \"Computer says no: why making AIs fair, accountable and transparent is crucial\" . The Guardian . Archived from the original on 10 October 2022 . Retrieved 30 January 2018 . Rothman, Denis (7 October 2020). \"Exploring LIME Explanations and the Mathematics Behind It\" . Codemotion . Archived from the original on 25 November 2023 . Retrieved 25 November 2023 . Scassellati, Brian (2002). \"Theory of mind for a humanoid robot\". Autonomous Robots . 12 (1): 13– 24. doi : 10.1023/A:1013298507114 . S2CID 1979315 . Schmidhuber, J. (2015). \"Deep Learning in Neural Networks: An Overview\". Neural Networks . 61 : 85– 117. arXiv : 1404.7828 . doi : 10.1016/j.neunet.2014.09.003 . PMID 25462637 . S2CID 11715509 . Schmidhuber, Jürgen (2022). \"Annotated History of Modern AI and Deep Learning\" . Archived from the original on 7 August 2023 . Retrieved 5 October 2024 . Searle, John (1980). \"Minds, Brains and Programs\" (PDF) . Behavioral and Brain Sciences . 3 (3): 417– 457. doi : 10.1017/S0140525X00005756 . S2CID 55303721 . Archived (PDF) from the original on 17 March 2019 . Retrieved 22 August 2020 . Searle, John (1999). Mind, language and society . New York: Basic Books. ISBN 978-0-4650-4521-1 . OCLC 231867665 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Simon, H. A. (1965), The Shape of Automation for Men and Management , New York: Harper & Row Simonite, Tom (31 March 2016). \"How Google Plans to Solve Artificial Intelligence\" . MIT Technology Review . Archived from the original on 16 September 2024 . Retrieved 5 October 2024 . Smith, Craig S. (15 March 2023). \"ChatGPT-4 Creator Ilya Sutskever on AI Hallucinations and AI Democracy\" . Forbes . Archived from the original on 18 September 2024 . Retrieved 25 December 2023 . Smoliar, Stephen W.; Zhang, HongJiang (1994). \"Content based video indexing and retrieval\". IEEE MultiMedia . 1 (2): 62– 72. doi : 10.1109/93.311653 . S2CID 32710913 . Solomonoff, Ray (1956). An Inductive Inference Machine (PDF) . Dartmouth Summer Research Conference on Artificial Intelligence. Archived (PDF) from the original on 26 April 2011 . Retrieved 22 March 2011 – via std.com, pdf scanned copy of the original. Later published as Solomonoff, Ray (1957). \"An Inductive Inference Machine\". IRE Convention Record . Vol. Section on Information Theory, part 2. pp. 56– 62. Stanford University (2023). \"Artificial Intelligence Index Report 2023/Chapter 6: Policy and Governance\" (PDF) . AI Index. Archived (PDF) from the original on 19 June 2023 . Retrieved 19 June 2023 . Tao, Jianhua; Tan, Tieniu (2005). Affective Computing and Intelligent Interaction . Affective Computing: A Review. Lecture Notes in Computer Science. Vol. 3784. Springer. pp. 981– 995. doi : 10.1007/11573548 . ISBN 978-3-5402-9621-8 . Taylor, Josh; Hern, Alex (2 May 2023). \" 'Godfather of AI' Geoffrey Hinton quits Google and warns over dangers of misinformation\" . The Guardian . Archived from the original on 5 October 2024 . Retrieved 5 October 2024 . Thompson, Derek (23 January 2014). \"What Jobs Will the Robots Take?\" . The Atlantic . Archived from the original on 24 April 2018 . Retrieved 24 April 2018 . Thro, Ellen (1993). Robotics: The Marriage of Computers and Machines . New York: Facts on File. ISBN 978-0-8160-2628-9 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Toews, Rob (3 September 2023). \"Transformers Revolutionized AI. What Will Replace Them?\" . Forbes . Archived from the original on 8 December 2023 . Retrieved 8 December 2023 . Turing, Alan (October"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 36,
    "text": "1950). \"Computing Machinery and Intelligence\" . Mind . 59 (236): 433– 460. doi : 10.1093/mind/LIX.236.433 . ISSN 1460-2113 . JSTOR 2251299 . S2CID 14636783 . UNESCO Science Report: the Race Against Time for Smarter Development . Paris: UNESCO. 2021. ISBN 978-9-2310-0450-6 . Archived from the original on 18 June 2022 . Retrieved 18 September 2021 . Urbina, Fabio; Lentzos, Filippa; Invernizzi, Cédric; Ekins, Sean (7 March 2022). \"Dual use of artificial-intelligence-powered drug discovery\" . Nature Machine Intelligence . 4 (3): 189– 191. doi : 10.1038/s42256-022-00465-9 . PMC 9544280 . PMID 36211133 . S2CID 247302391 . Valance, Christ (30 May 2023). \"Artificial intelligence could lead to extinction, experts warn\" . BBC News . Archived from the original on 17 June 2023 . Retrieved 18 June 2023 . Valinsky, Jordan (11 April 2019), \"Amazon reportedly employs thousands of people to listen to your Alexa conversations\" , CNN.com , archived from the original on 26 January 2024 , retrieved 5 October 2024 Verma, Yugesh (25 December 2021). \"A Complete Guide to SHAP – SHAPley Additive exPlanations for Practitioners\" . Analytics India Magazine . Archived from the original on 25 November 2023 . Retrieved 25 November 2023 . Vincent, James (7 November 2019). \"OpenAI has published the text-generating AI it said was too dangerous to share\" . The Verge . Archived from the original on 11 June 2020 . Retrieved 11 June 2020 . Vincent, James (15 November 2022). \"The scary truth about AI copyright is nobody knows what will happen next\" . The Verge . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Vincent, James (3 April 2023). \"AI is entering an era of corporate control\" . The Verge . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Vinge, Vernor (1993). \"The Coming Technological Singularity: How to Survive in the Post-Human Era\" . Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace : 11. Bibcode : 1993vise.nasa...11V . Archived from the original on 1 January 2007 . Retrieved 14 November 2011 . Waddell, Kaveh (2018). \"Chatbots Have Entered the Uncanny Valley\" . The Atlantic . Archived from the original on 24 April 2018 . Retrieved 24 April 2018 . Wallach, Wendell (2010). Moral Machines . Oxford University Press. Wason, P. C. ; Shapiro, D. (1966). \"Reasoning\" . In Foss, B. M. (ed.). New horizons in psychology . Harmondsworth: Penguin. Archived from the original on 26 July 2020 . Retrieved 18 November 2019 . Weng, J.; McClelland; Pentland, A.; Sporns, O.; Stockman, I.; Sur, M.; Thelen, E. (2001). \"Autonomous mental development by robots and animals\" (PDF) . Science . 291 (5504): 599– 600. doi : 10.1126/science.291.5504.599 . PMID 11229402 . S2CID 54131797 . Archived (PDF) from the original on 4 September 2013 . Retrieved 4 June 2013 – via msu.edu. \"What is 'fuzzy logic'? Are there computers that are inherently fuzzy and do not apply the usual binary logic?\" . Scientific American . 21 October 1999. Archived from the original on 6 May 2018 . Retrieved 5 May 2018 . Williams, Rhiannon (28 June 2023), \"Humans may be more likely to believe disinformation generated by AI\" , MIT Technology Review , archived from the original on 16 September 2024 , retrieved 5 October 2024 Wirtz, Bernd W.; Weyerer, Jan C.; Geyer, Carolin (24 July 2018). \"Artificial Intelligence and the Public Sector – Applications and Challenges\" . International Journal of Public Administration . 42 (7): 596– 615. doi : 10.1080/01900692.2018.1498103 . ISSN 0190-0692 . S2CID 158829602 . Archived from the original on 18 August 2020 . Retrieved 22 August 2020 . Wong, Matteo (19 May 2023), \"ChatGPT Is Already Obsolete\" , The Atlantic , archived from the original on 18 September 2024 , retrieved 5 October 2024 Yudkowsky, E (2008), \"Artificial Intelligence as a Positive and Negative Factor in Global Risk\" (PDF) , Global Catastrophic Risks , Oxford University Press, 2008, Bibcode : 2008gcr..book..303Y , archived (PDF) from the original on 19 October 2013 , retrieved 24 September 2021 Further reading Autor, David H. , \"Why Are There Still So Many Jobs? The History and Future of Workplace Automation\" (2015) 29(3) Journal of Economic Perspectives 3. Boyle, James, The Line: AI and the Future of Personhood , MIT Press , 2024. Cukier, Kenneth , \"Ready for Robots? How to Think about the Future of AI\", Foreign Affairs , vol. 98, no. 4 (July/August 2019), pp. 192–198. George Dyson , historian of computing, writes (in what might be called \"Dyson's Law\") that \"Any system simple enough to be understandable will not be complicated enough to behave intelligently, while any system complicated enough to behave intelligently will be too complicated to understand.\" (p. 197.) Computer scientist Alex Pentland writes: \"Current AI machine-learning algorithms are, at their core, dead simple stupid. They work, but"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 37,
    "text": "they work by brute force.\" (p. 198.) Evans, Woody (2015). \"Posthuman Rights: Dimensions of Transhuman Worlds\" . Teknokultura . 12 (2). doi : 10.5209/rev_TK.2015.v12.n2.49072 . S2CID 147612763 . Frank, Michael (22 September 2023). \"US Leadership in Artificial Intelligence Can Shape the 21st Century Global Order\" . The Diplomat . Archived from the original on 16 September 2024 . Retrieved 8 December 2023 . Instead, the United States has developed a new area of dominance that the rest of the world views with a mixture of awe, envy, and resentment: artificial intelligence... From AI models and research to cloud computing and venture capital, U.S. companies, universities, and research labs – and their affiliates in allied countries – appear to have an enormous lead in both developing cutting-edge AI and commercializing it. The value of U.S. venture capital investments in AI start-ups exceeds that of the rest of the world combined. Gertner, Jon. (2023) \"Wikipedia's Moment of Truth: Can the online encyclopedia help teach A.I. chatbots to get their facts right — without destroying itself in the process?\" New York Times Magazine (July 18, 2023) online Archived 20 July 2023 at the Wayback Machine Gleick, James , \"The Fate of Free Will\" (review of Kevin J. Mitchell, Free Agents: How Evolution Gave Us Free Will , Princeton University Press, 2023, 333 pp.), The New York Review of Books , vol. LXXI, no. 1 (18 January 2024), pp. 27–28, 30. \" Agency is what distinguishes us from machines. For biological creatures, reason and purpose come from acting in the world and experiencing the consequences. Artificial intelligences – disembodied, strangers to blood, sweat, and tears – have no occasion for that.\" (p. 30.) Gleick, James , \" The Parrot in the Machine \" (review of Emily M. Bender and Alex Hanna, The AI Con: How to Fight Big Tech's Hype and Create the Future We Want , Harper, 274 pp.; and James Boyle , The Line: AI and the Future of Personhood , MIT Press, 326 pp.), The New York Review of Books , vol. LXXII, no. 12 (24 July 2025), pp. 43–46. \"[C]hatbox 'writing' has a bland, regurgitated quality. Textures are flattened, sharp edges are sanded. No chatbox could ever have said that April is the cruelest month or that fog comes on little cat feet (though they might now, because one of their chief skills is plagiarism ). And when synthetically extruded text turns out wrong, it can be comically wrong. When a movie fan asked Google whether a certain actor was in Heat , he received this 'AI Overview': 'No, Angelina Jolie is not in heat.'\" (p. 44.) Halpern, Sue, \"The Coming Tech Autocracy\" (review of Verity Harding , AI Needs You: How We Can Change AI's Future and Save Our Own , Princeton University Press, 274 pp.; Gary Marcus , Taming Silicon Valley: How We Can Ensure That AI Works for Us , MIT Press, 235 pp.; Daniela Rus and Gregory Mone , The Mind's Mirror: Risk and Reward in the Age of AI , Norton, 280 pp.; Madhumita Murgia , Code Dependent: Living in the Shadow of AI , Henry Holt, 311 pp.), The New York Review of Books , vol. LXXI, no. 17 (7 November 2024), pp. 44–46. \"'We can't realistically expect that those who hope to get rich from AI are going to have the interests of the rest of us close at heart,' ... writes [Gary Marcus]. 'We can't count on governments driven by campaign finance contributions [from tech companies] to push back.'... Marcus details the demands that citizens should make of their governments and the tech companies . They include transparency on how AI systems work; compensation for individuals if their data [are] used to train LLMs ( large language model )s and the right to consent to this use; and the ability to hold tech companies liable for the harms they cause by eliminating Section 230 , imposing cash penalties, and passing stricter product liability laws... Marcus also suggests... that a new, AI-specific federal agency, akin to the FDA , the FCC , or the FTC , might provide the most robust oversight.... [T]he Fordham law professor Chinmayi Sharma ... suggests... establish[ing] a professional licensing regime for engineers that would function in a similar way to medical licenses , malpractice suits, and the Hippocratic oath in medicine. 'What if, like doctors,' she asks..., 'AI engineers also vowed to do no harm ?'\" (p. 46.) Henderson, Mark (24 April 2007). \"Human rights for robots? We're getting carried away\" . The Times Online . London. Archived from the original on 31 May 2014 . Retrieved 31 May 2014 . Hughes-Castleberry, Kenna, \"A Murder Mystery Puzzle: The literary puzzle Cain's Jawbone , which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\", Scientific American ,"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 38,
    "text": "vol. 329, no. 4 (November 2023), pp. 81–82. \"This murder mystery competition has revealed that although NLP ( natural-language processing ) models are capable of incredible feats, their abilities are very much limited by the amount of context they receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyze ancient languages . In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose.\" (p. 82.) Immerwahr, Daniel , \"Your Lying Eyes: People now use A.I. to generate fake videos indistinguishable from real ones. How much does it matter?\", The New Yorker , 20 November 2023, pp. 54–59. \"If by ' deepfakes ' we mean realistic videos produced using artificial intelligence that actually deceive people, then they barely exist. The fakes aren't deep, and the deeps aren't fake. [...] A.I.-generated videos are not, in general, operating in our media as counterfeited evidence. Their role better resembles that of cartoons , especially smutty ones.\" (p. 59.) Johnston, John (2008) The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI , MIT Press. Jumper, John; Evans, Richard; Pritzel, Alexander; et al. (26 August 2021). \"Highly accurate protein structure prediction with AlphaFold\" . Nature . 596 (7873): 583– 589. Bibcode : 2021Natur.596..583J . doi : 10.1038/s41586-021-03819-2 . PMC 8371605 . PMID 34265844 . S2CID 235959867 . LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (28 May 2015). \"Deep learning\" . Nature . 521 (7553): 436– 444. Bibcode : 2015Natur.521..436L . doi : 10.1038/nature14539 . PMID 26017442 . S2CID 3074096 . Archived from the original on 5 June 2023 . Retrieved 19 June 2023 . Leffer, Lauren, \"The Risks of Trusting AI: We must avoid humanizing machine-learning models used in scientific research\", Scientific American , vol. 330, no. 6 (June 2024), pp. 80–81. Lepore, Jill , \"The Chit-Chatbot: Is talking with a machine a conversation?\", The New Yorker , 7 October 2024, pp. 12–16. Maschafilm (2010). \"Content: Plug & Pray Film – Artificial Intelligence – Robots\" . plugandpray-film.de . Archived from the original on 12 February 2016. Marcus, Gary , \"Artificial Confidence: Even the newest, buzziest systems of artificial general intelligence are stymmied by the same old problems\", Scientific American , vol. 327, no. 4 (October 2022), pp. 42–45. Mitchell, Melanie (2019). Artificial intelligence: a guide for thinking humans . New York: Farrar, Straus and Giroux. ISBN 978-0-3742-5783-5 . Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; et al. (26 February 2015). \"Human-level control through deep reinforcement learning\" . Nature . 518 (7540): 529– 533. Bibcode : 2015Natur.518..529M . doi : 10.1038/nature14236 . PMID 25719670 . S2CID 205242740 . Archived from the original on 19 June 2023 . Retrieved 19 June 2023 . Introduced DQN , which produced human-level performance on some Atari games. Press, Eyal , \"In Front of Their Faces: Does facial-recognition technology lead police to ignore contradictory evidence?\", The New Yorker , 20 November 2023, pp. 20–26. \"Robots could demand legal rights\" . BBC News . 21 December 2006. Archived from the original on 15 October 2019 . Retrieved 3 February 2011 . Roivainen, Eka, \"AI's IQ: ChatGPT aced a [standard intelligence] test but showed that intelligence cannot be measured by IQ alone\", Scientific American , vol. 329, no. 1 (July/August 2023), p. 7. \"Despite its high IQ, ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world.... ChatGPT seemed unable to reason logically and tried to rely on its vast database of... facts derived from online texts.\" Scharre, Paul, \"Killer Apps: The Real Dangers of an AI Arms Race\", Foreign Affairs , vol. 98, no. 3 (May/June 2019), pp. 135–144. \"Today's AI technologies are powerful but unreliable. Rules-based systems cannot deal with circumstances their programmers did not anticipate. Learning systems are limited by the data on which they were trained. AI failures have already led to tragedy. Advanced autopilot features in cars, although they perform well in some circumstances, have driven cars without warning into trucks, concrete barriers, and parked cars. In the wrong situation, AI systems go from supersmart to superdumb in an instant. When an enemy is trying to manipulate and hack an AI system, the risks are even greater.\" (p. 140.) Schulz, Hannes; Behnke, Sven (1 November 2012). \"Deep Learning\" . KI – Künstliche Intelligenz . 26 (4): 357– 363. doi : 10.1007/s13218-012-0198-z . ISSN 1610-1987 . S2CID 220523562 . Serenko, Alexander; Michael Dohan (2011). \"Comparing the expert survey and citation impact journal ranking methods: Example from the field of Artificial Intelligence\" (PDF) . Journal of Informetrics . 5 (4): 629– 649. doi : 10.1016/j.joi.2011.06.002 . Archived (PDF) from the original on 4 October 2013 . Retrieved 12 September 2013 . Silver, David; Huang, Aja; Maddison, Chris J.; et al. (28 January 2016). \"Mastering"
  },
  {
    "type": "doc",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence",
    "chunk": 39,
    "text": "the game of Go with deep neural networks and tree search\" . Nature . 529 (7587): 484– 489. Bibcode : 2016Natur.529..484S . doi : 10.1038/nature16961 . PMID 26819042 . S2CID 515925 . Archived from the original on 18 June 2023 . Retrieved 19 June 2023 . Tarnoff, Ben , \"The Labor Theory of AI\" (review of Matteo Pasquinelli , The Eye of the Master: A Social History of Artificial Intelligence , Verso, 2024, 264 pp.), The New York Review of Books , vol. LXXII, no. 5 (27 March 2025), pp. 30–32. The reviewer, Ben Tarnoff, writes: \"The strangeness at the heart of the generative AI boom is that nobody really knows how the technology works. We know how the large language models within ChatGPT and its counterparts are trained, even if we don't always know which data they're being trained on: they are asked to predict the next string of characters in a sequence. But exactly how they arrive at any given prediction is a mystery. The computations that occur inside the model are simply too intricate for any human to comprehend.\" (p. 32.) Vaswani, Ashish , Noam Shazeer, Niki Parmar et al. \" Attention is all you need .\" Advances in neural information processing systems 30 (2017). Seminal paper on transformers . Vincent, James, \"Horny Robot Baby Voice: James Vincent on AI chatbots\", London Review of Books , vol. 46, no. 19 (10 October 2024), pp. 29–32. \"[AI chatbot] programs are made possible by new technologies but rely on the timelelss human tendency to anthropomorphise .\" (p. 29.) White Paper: On Artificial Intelligence – A European approach to excellence and trust (PDF) . Brussels: European Commission. 2020. Archived (PDF) from the original on 20 February 2020 . Retrieved 20 February 2020 . External links Articles related to artificial intelligence"
  },
  {
    "type": "doc",
    "url": "https://www.coindesk.com/markets/2025/08/10/here-are-3-bullish-reasons-why-jpmorgan-sees-s-and-p-500-rallying-much-higher",
    "title": "Here Are 3 Bullish Reasons Why JPMorgan Sees S&P 500 Rallying Much Higher",
    "chunk": 0,
    "text": "JPMorgan remains bullish on U.S. stocks even as some observers warn that the economy is beginning to pay the price for President Donald Trump's tariffs. The investment banking giant forecasts that the S&P 500, Wall Street's benchmark index, will yield a \"high single-digit return over the next 12 months,\" driven by three key factors. One of the main reasons for optimism is that markets don't care about signs of an economic slowdown. Instead, traders are focused on resilient corporate earnings and the subsequent economic recovery. Since President Trump fired the first tariff salvo on April 2, economists have downgraded full-year U.S. growth forecasts from 2.3% to 1.5%. Still, the S&P 500 has gained over 28% in the four months. The index has held steady despite recent economic data revealing softness in the labour market and consumption , as well as stickiness in manufacturing and service sector inflation . While the macro analysts' warning is concerning and likely playing out in the background, corporate earnings in the U.S. are ignoring the slowdown risks, at least in the short term, making it the second catalyst for JPMorgan's bullish thesis. Over 80% of S&P 500 companies have recently reported their Q2 earnings, with 82% surpassing earnings expectations and 79% beating revenue forecasts—the strongest performance since the second quarter of 2021. The winners and losers According to JPMorgan, while Wall Street analysts initially projected earnings growth below 5%, the index is now on pace for an impressive 11% growth rate. This robust showing supports the ongoing bullish trend in the stock market. \"The full-year earnings expectations for both this year and next have already started to turn higher,\" analysts at JPMorgan's wealth management said in a market note on Friday, adding that the market is increasingly differentiating between the winners and losers of the Trump trade war. Additionally, the market is now figuring out and pricing in which companies are getting hit most by U.S. tariffs. So far, it looks like mega corporations will be just fine. This could bolster the case for further positive sentiment in the markets. JPMorgan analysts explained that consumer-facing and smaller companies with restrained bargaining power against their trading partners and rigid supply chains are facing a stagnant earnings outlook. This ties to JPMorgan's last catalyst: Trump's tariff bark is proving worse than its bite for large firms, which are managing to secure exemptions and even turn the tariff policies, aimed at sparking a manufacturing boom, into a tailwind. \"The latest example is President Donald Trump’s suggestion that imported semiconductors would be taxed at a 100% rate unless the companies commit to relocating production to the United States. Another sign? Apple products are exempted from the latest tariff rates on Indian goods. Indeed, the company also announced an additional $100 billion investment in U.S. manufacturing facilities. The stock gained almost 9% this week. Tariffs are not happening in a vacuum,\" analysts explained. Big firms gain an additional advantage from the One Big Beautiful Act (OBBA), under which firms can claim 100% bonus depreciation for purchases of qualified business property and immediate expense of domestic research and development costs. According to some analysts, the depreciation policy could increase free cash flow for some by over 30%, which could incentivize more investment. The bank added that its investment strategy remains focused on large-cap equities, particularly in the technology, financials, and utilities sectors, which it believes are best positioned to navigate this new economic environment. The crypto angle JPMorgan's positive outlook for stocks could bode well for cryptocurrencies, as both tend to move in tandem. The digital assets market has plenty going on for itself, with the Trump administration appointing pro-crypto officials to key regulatory positions. Recently, the U.S. Securities and Exchange Commission (SEC) ruled that liquid staking, under certain conditions, falls outside the purview of Securities Law. The ruling has raised hopes for staking spot ether ETFs winning regulatory approval. Ether has rallied over 13% to over $4,200, reaching levels last seen in 2021. Prices surged nearly 50% last month, CoinDesk data show ."
  },
  {
    "type": "doc",
    "url": "https://www.coindesk.com/markets/2025/08/10/u-s-spot-xrp-etfs-five-possible-reasons-behind-blackrock-s-hesitation-to-file-for-one",
    "title": "Why BlackRock Might be Reluctant to Pursue a U.S.-Listed Spot XRP ETF",
    "chunk": 0,
    "text": "BlackRock has made bold moves into bitcoin and ether ETFs, but on Friday, the asset manager said it had no immediate plans to file for a spot XRP exchange-traded fund (ETF), dashing the community’s hopes that its entry could help extend XRP’s 2025 rally. This statement — made the day after the U.S. Securities and Exchange Commission (SEC) and Ripple Labs jointly asked an appeals court to dismiss their respective appeals, signaling an end to their nearly five-year legal battle — has left investors questioning why BlackRock remains on the sidelines. While several asset managers, including ProShares, Grayscale, and Bitwise, have filed for XRP ETFs since late 2024, BlackRock’s absence is notable, especially given its dominance in the bitcoin and ether ETF markets. Here are five reasons why BlackRock appears in no hurry to launch a spot XRP ETF, despite the XRP community’s anticipation of a demand-driven price surge. First, BlackRock has cited limited client interest in cryptocurrencies beyond BTC and ETH. Back in March 2024, Robert Mitchnick, the asset manager's head of digital assets, said that there's a misconception that BlackRock will have a \"long tail\" of other crypto services. \"I can say that for our client base, bitcoin is overwhelmingly the No. 1 focus and a little bit ethereum,\" he said during a fireside chat at the inaugural Bitcoin Investor Day conference in New York on March 22. Second, BlackRock’s strategic caution around regulatory uncertainty plays a role. Although XRP sales on public exchanges are deemed non-securities, the broader regulatory framework for altcoins remains murky. BlackRock may be waiting for clearer SEC guidelines before entering the altcoin ETF space. The firm’s conservative approach contrasts with competitors like ProShares, which filed for a spot XRP ETF in January 2025 alongside leveraged and futures-based XRP ETFs, the latter tracking XRP futures contracts rather than the token’s spot price. Third, BlackRock may see diminishing returns in pursuing a spot XRP ETF given the crowded field. As of August 2025, at least seven firms, including Grayscale, Franklin Templeton and 21Shares, have a pending spot XRP ETF application. Fourth, the XRP community’s expectations of a price surge may not align with BlackRock’s data-driven strategy. Polymarket odds for the SEC approving a spot XTP ETF in 2025 stand at 77%. BlackRock's tokenized money market fund on Ethereum and Solana shows blockchain interest, but XRP’s smaller market footprint may not justify the operational costs of a new ETF. Finally, BlackRock’s global perspective prioritizes markets where XRP demand is less pronounced. While the XRP community, active on platforms like X, anticipates a spot ETF driving demand, much of XRP’s trading volume comes from Asia, where BlackRock’s ETF presence is less dominant. At press time, XRP was trading around $3.1852, down 3.92% in the past 24 hours, according to CoinDesk Data."
  },
  {
    "type": "doc",
    "url": "https://www.coindesk.com/markets/2025/08/09/bitcoin-trails-gold-in-2025-but-dominates-long-term-returns-across-major-asset-classes",
    "title": "BTC YTD Performance 2nd to Gold but 308,709x Higher Total Return Since 2011",
    "chunk": 0,
    "text": "Bitcoin slipped 0.11% in the past 24 hours to $116,702, according to CoinDesk Data, but it remains up 25% year to date, second only to gold’s 29% gain among major asset classes, according to data shared by financial strategist Charlie Bilello on X. Both assets have outperformed other major asset classes this year, such as emerging market stocks (VWO +15.6%), the Nasdaq 100 (QQQ +12.7%) and U.S. large caps (SPY +9.4%). Meanwhile, U.S. mid-caps (MDY) and small-caps (IWM) have only gained 0.2% and 0.8%, respectively, his data showed. This marks the first time gold and bitcoin have occupied the top two positions in Bilello’s annual asset class rankings since records began. The big picture However, zooming out, bitcoin has delivered an extraordinary 38,897,420% total return since 2011 — a figure that dwarfs all other asset classes in the dataset. Gold’s 126% cumulative return over the same period puts it in the middle of the pack, trailing equity benchmarks like the Nasdaq 100 (1101%) and U.S. large caps (559%), as well as mid caps (316%), small caps (244%) and emerging market stocks (57%). Based on Bilello’s figures, bitcoin’s total return has exceeded gold’s by more than 308,000 times over the past 14 years. When measured on an annualized basis, bitcoin’s dominance is equally clear. The flagship cryptocurrency has delivered a 141.7% average annual gain since 2011, compared with 5.7% for gold, 18.6% for the Nasdaq 100, 13.8% for U.S. large caps and 4.4% to 16.4% for other major equity and real estate indexes, Bilello's data showed. Gold’s long-term stability has made it a valuable hedge in certain market cycles, but its pace of appreciation has been far slower than bitcoin’s exponential climb. Store of value: Gold vs. bitcoin Renowned trader Peter Brandt weighed in on Aug. 8, contrasting gold’s merits as a store of value with bitcoin’s potential to surpass all fiat alternatives. “Some think gold is a great store of value — and it is. But the ultimate store of value will prove to be bitcoin,” he said on X, sharing a long-term chart of the U.S. dollar’s purchasing power. His comments echo the growing narrative that bitcoin’s scarcity and decentralization make it uniquely positioned to outperform traditional hedges over time. Read more: Chart of the Week: Tariff Carnage Starting to Fulfill Bitcoin's 'Store of Value' Promise Bitcoin’s ability to hold above six figures in 2025 while maintaining a top-two performance among major assets underscores its resilience in a volatile macro backdrop. Traders are watching whether it can retest the year’s peak near $123,000, while long-term holders point to its outperformance since 2011 as evidence of its staying power. Market participants say upcoming macro data and risk appetite across equities and commodities could set the tone for the next leg. Read more: Bitcoin Still on Track for $140K This Year, But 2026 Will Be Painful: Elliott Wave Expert"
  },
  {
    "type": "doc",
    "url": "https://news.bitcoin.com/another-win-for-xrp-as-ripple-secures-new-sec-waiver-accelerating-institutional-adoption/",
    "title": "Another Win for XRP as Ripple Secures New SEC Waiver, Accelerating Institutional Adoption",
    "chunk": 0,
    "text": "You need to enable JavaScript to run this app."
  },
  {
    "type": "doc",
    "url": "https://news.bitcoin.com/latam-insights-brazil-discusses-strategic-reserve-el-salvador-passes-bitcoin-friendly-banking-law/",
    "title": "Latam Insights: Brazil Discusses Strategic Reserve; El Salvador Passes Bitcoin-Friendly Banking Law",
    "chunk": 0,
    "text": "You need to enable JavaScript to run this app."
  },
  {
    "type": "doc",
    "url": "https://news.bitcoin.com/inside-the-powerhouse-the-50-most-profitable-bitcoin-miners-aug-9-10-2025/",
    "title": "Inside the Powerhouse: The 50 Most Profitable Bitcoin Miners, Aug. 9-10, 2025",
    "chunk": 0,
    "text": "You need to enable JavaScript to run this app."
  },
  {
    "type": "doc",
    "url": "https://coinmarketcap.com/",
    "title": "Cryptocurrency Prices, Charts And Market Capitalizations",
    "chunk": 0,
    "text": "Today’s Cryptocurrency Prices, Charts and Data Welcome to CoinMarketCap.com! This site was founded in May 2013 by Brandon Chez to provide up-to-date cryptocurrency prices, charts and data about the emerging cryptocurrency markets. Since then, the world of blockchain and cryptocurrency has grown exponentially and we are very proud to have grown with it. We take our data very seriously and we do not change our data to fit any narrative: we stand for accurately, timely and unbiased information. All Your Crypto Market Data Needs in One Place Here at CoinMarketCap, we work very hard to ensure that all the relevant and up-to-date information about cryptocurrencies, coins and tokens can be located in one easily discoverable place. From the very first day, the goal was for the site to be the number one location online for crypto market data, and we work hard to empower our users with our unbiased and accurate information. We Provide Live and Historic Crypto Charts for Free Each of our coin data pages has a graph that shows both the current and historic price information for the coin or token. Normally, the graph starts at the launch of the asset, but it is possible to select specific to and from dates to customize the chart to your own needs. These charts and their information are free to visitors of our website. The most experienced and professional traders often choose to use the best crypto API on the market. Our API enables millions of calls to track current prices and to also investigate historic prices and is used by some of the largest crypto exchanges and financial institutions in the world. CoinMarketCap also provides data about the most successful traders for you to monitor. We also provide data about the latest trending cryptos and trending DEX pairs . How Do We Calculate Our Cryptocurrency Prices? We receive updated cryptocurrency prices directly from many exchanges based on their pairs. We then convert the number to USD. A full explanation can be found here . Related Links Are you ready to learn more? Visit our glossary and crypto learning center. Are you interested in the scope of crypto assets? Investigate our list of cryptocurrency categories. Are you interested in knowing which the hottest dex pairs are currently? How Do We Calculate Our Crypto Valuations? We calculate our valuations based on the total circulating supply of an asset multiplied by the currency reference price. The topic is explained in more detail here . How Do We Calculate the Cryptocurrency Market Cap? We calculate the total cryptocurrency market capitalization as the sum of all cryptocurrencies listed on the site. Does CoinMarketCap.com List All Cryptocurrencies? Almost. We have a process that we use to verify assets. Once verified, we create a coin description page like this . The world of crypto now contains many coins and tokens that we feel unable to verify. In those situations, our Dexscan product lists them automatically by taking on-chain data for newly created smart contracts. We do not cover every chain, but at the time of writing we track the top 70 crypto chains, which means that we list more than 97% of all tokens. How Big Is the Global Coin Market? At the time of writing, we estimate that there are more than 2 million pairs being traded, made up of coins, tokens and projects in the global coin market. As mentioned above, we have a due diligence process that we apply to new coins before they are listed. This process controls how many of the cryptocurrencies from the global market are represented on our site. What Is an Altcoin? The very first cryptocurrency was Bitcoin . Since it is open source, it is possible for other people to use the majority of the code, make a few changes and then launch their own separate currency. Many people have done exactly this. Some of these coins are very similar to Bitcoin, with just one or two amended features (such as Litecoin ), while others are very different, with varying models of security, issuance and governance. However, they all share the same moniker — every coin issued after Bitcoin is considered to be an altcoin. What Is a Smart Contract? The first chain to launch smart contracts was Ethereum . A smart contract enables multiple scripts to engage with each other using clearly defined rules, to execute on tasks which can become a coded form of a contract. They have revolutionized the digital asset space because they have enabled decentralized exchanges, decentralized finance, ICOs, IDOs and much more. A huge proportion of the value created and stored in cryptocurrency is enabled by smart contracts. What Is a Stablecoin? Price volatility has long been one of the features of the cryptocurrency market. When asset prices move quickly"
  },
  {
    "type": "doc",
    "url": "https://coinmarketcap.com/",
    "title": "Cryptocurrency Prices, Charts And Market Capitalizations",
    "chunk": 1,
    "text": "in either direction and the market itself is relatively thin, it can sometimes be difficult to conduct transactions as might be needed. To overcome this problem, a new type of cryptocurrency tied in value to existing currencies — ranging from the U.S. dollar, other fiats or even other cryptocurrencies — arose. These new cryptocurrency are known as stablecoins , and they can be used for a multitude of purposes due to their stability. What Is an NFT? NFTs are multi-use images that are stored on a blockchain. They can be used as art, a way to share QR codes, ticketing and many more things. The first breakout use was for art, with projects like CryptoPunks and Bored Ape Yacht Club gaining large followings. We also list all of the top NFT collections available, including the related NFT coins and tokens.. We collect latest sale and transaction data, plus upcoming NFT collection launches onchain. NFTs are a new and innovative part of the crypto ecosystem that have the potential to change and update many business models for the Web 3 world. What Are In-game Tokens? Play-to-earn (P2E) games, also known as GameFi , has emerged as an extremely popular category in the crypto space. It combines non-fungible tokens (NFT), in-game crypto tokens, decentralized finance (DeFi) elements and sometimes even metaverse applications. Players have an opportunity to generate revenue by giving their time (and sometimes capital) and playing these games. One of the biggest winners is Axie Infinity — a Pokémon-inspired game where players collect Axies (NFTs of digital pets), breed and battle them against other players to earn Smooth Love Potion (SLP) — the in-game reward token. This game was extremely popular in developing countries like The Philippines, due to the level of income they could earn. Players in the Philippines can check the price of SLP to PHP today directly on CoinMarketCap. What Are ETFs? In January 2024 the SEC approved 11 exchange traded funds to invest in Bitcoin. There were already a number of Bitcoin ETFs available in other countries, but this change allowed them to be available to retail investors in the United States. This opens the way for a much wider range of investors to be able to add some exposure to cryptocurrency in their portfolios. Which Is the Best Cryptocurrency to Invest in? CoinMarketCap does not offer financial or investment advice about which cryptocurrency, token or asset does or does not make a good investment, nor do we offer advice about the timing of purchases or sales. We are strictly a data company. Please remember that the prices, yields and values of financial assets change. This means that any capital you may invest is at risk. We recommend seeking the advice of a professional investment advisor for guidance related to your personal circumstances. If You Are Investing in Cryptocurrency — CoinMarketCap.com Is for You TThe data at CoinMarketCap updates every few seconds, which means that it is possible to check in on the value of your investments and assets at any time and from anywhere in the world. We look forward to seeing you regularly!"
  },
  {
    "type": "doc",
    "url": "https://www.coingecko.com/",
    "title": "Cryptocurrency Prices, Charts, and Crypto Market Cap",
    "chunk": 0,
    "text": "Your All-in-One Platform For Cryptocurrency Market Data Welcome to CoinGecko! As crypto traders and investors ourselves, we understand the hassle of browsing multiple websites and exchanges to find reliable information and market data for a coin. That’s why we built CoinGecko – so you can access all crypto information in one place. Get Reliable Live Cryptocurrency Prices Crypto prices and market data have always been at the core of our product – it’s what we do best. We provide unbiased cryptocurrency data for the community, whether to help you make an investment decision or check the value of your crypto assets. We use an average price as crypto prices vary between markets. Crypto prices on an exchange depend on its market condition, influenced by factors like liquidity, trading pairs, offerings, and economic conditions. As exchanges may sometimes show abnormal prices, the crypto community relies on tools like CoinGecko for more accurate coin prices. We Offer Market Data, Crypto Charts & Latest News Crypto prices alone don’t mean much, so we’ve included data like market cap and fully diluted value. It’s only when you pair current prices with historical data, statistics, news, and more that you get a full picture of a coin’s performance. Another popular feature on CoinGecko is our free and powerful crypto charts. What’s neat is you can compare a coin’s performance against Bitcoin and/or Ethereum on a single graph. Our price charts come in both line and candlestick formats, and we also offer market cap charts. How Does CoinGecko Calculate Crypto Prices? Our prices are calculated using an average price formula based on available trading pairs across multiple exchanges. We also have algorithms to detect and exclude anomalous tickers from our prices. Learn more about our methodology . Where Can You Get Cryptocurrency Prices? You can get real-time coin prices with our cryptocurrency price tracker by clicking on the coins in the table above. Our price trackers are usually sufficient for regular investors. But if you’re an advanced trader looking for more granular price data or a developer building crypto applications showing market data, our API gives you a comprehensive set of crypto data, including live and historical crypto prices, market cap, market volume, and more. Crypto Prices in Global Currencies If you’re looking for crypto prices in your local currencies, check out our cryptocurrency pairs: Overall most popular cryptocurrency pairs: BTC-USD , ETH-USD , XRP-USD , SOL-USD and DOGE-USD Popular cryptocurrency pairs in Australia: BTC-AUD , ETH-AUD , XRP-AUD , SOL-AUD and DOGE-AUD Popular cryptocurrency pairs in Canada: BTC-CAD , ETH-CAD , XRP-CAD , SOL-CAD and USDT-CAD Popular cryptocurrency pairs in the United Kingdom: BTC-GBP , ETH-GBP , XRP-GBP , SOL-GBP and USDT-GBP Popular cryptocurrency pairs in India: BTC-INR , ETH-INR , USDT-INR , SOL-INR and XRP-INR What Is Crypto Market Cap? Crypto market cap is the total value of a cryptocurrency in circulation, calculated by multiplying the total number of coins by the current market price. It’s used to determine the valuation of a cryptocurrency based on the total money invested, not just the price. Why Is Crypto Market Cap Important? Investors use crypto market caps to determine if a coin has more room for growth or is currently overvalued by comparing it to established cryptocurrencies with similar use cases as a benchmark. While market cap is important, it’s only one of many factors to consider when investing in a coin. How To Compare Crypto Market Cap? Crypto market cap can be divided into three categories: Large-cap cryptocurrencies like Bitcoin and Ethereum have market caps above $10 billion. These established projects have higher liquidity and lower risks. Mid-cap cryptocurrencies have market caps between $1 billion and $10 billion. They have higher growth potential than large-cap coins but are also riskier. Small-cap cryptocurrencies have market caps below $1 billion. They are usually new projects with the potential for exponential growth but have lower liquidity and higher risk. These categories are often used in investment strategies, and a “good market cap” depends on your risk appetite. If you’re willing to take on higher risks, you can look into small- or mid-cap cryptocurrencies. For risk-averse investors, consider large-cap cryptocurrencies. List of Top Ranking Cryptocurrencies Our cryptocurrency list features the top cryptocurrencies today, including Bitcoin, Ethereum, and over ten thousand altcoins. What sets our list apart is we aggregate cryptos from various exchanges to give a full picture of the crypto market and not just for coins on a particular exchange. Our team curates the cryptocurrency list. We vet each coin to reduce the risk of scams and remove inactive coins or dead projects to keep our list relevant to the market. If you can’t find a coin on CoinGecko, try searching on our DEX tracker GeckoTerminal . How Are The Top Cryptocurrencies Ranked? We rank cryptocurrencies by market"
  },
  {
    "type": "doc",
    "url": "https://www.coingecko.com/",
    "title": "Cryptocurrency Prices, Charts, and Crypto Market Cap",
    "chunk": 1,
    "text": "cap. Market cap represents the market share of a coin or token, so the higher the crypto rank, the more dominant it is in the crypto market. What Are Altcoins? Altcoins are every other cryptocurrency created after Bitcoin, and they can be coins or tokens. Coins are native currencies to the blockchain, while tokens are cryptos built on the blockchain. There are many types of altcoins. Stablecoins are altcoins whose value is pegged to assets like fiat currencies and commodities. Another example is governance tokens, which let you vote for the Web3 project’s future. You can explore more types of altcoins on our categories pages. Get The Most Value Out Of CoinGecko Getting into crypto can be daunting. But our beginner-friendly articles and videos are there to help you take your first steps into the decentralized internet. As for crypto investors, you can create custom portfolios to track the performance of coins you're interested in. Many of our pro-users use our crypto highlights to discover crypto gems and see what’s popular today. If you're a developer building in the crypto space, you can tap into our robust, comprehensive crypto API . Not only does it provide prices and market data for over two million coins, you can also access crypto exchange data to compare prices across markets and build candlestick charts with our OHLCV data . Our API supports crypto data across 200+ networks, including major chains like Solana , Ethereum , BNB Chain , Polygon , and Base . No matter where you are in your crypto journey, we want to empower you with unbiased fundamental crypto data you need to thrive in this Web3 world. We’re excited to see you around!"
  },
  {
    "type": "doc",
    "url": "https://www.gate.io/",
    "title": "Buy/Sell Bitcoin, Ethereum and 3,500+ Altcoins",
    "chunk": 0,
    "text": "Reserve Audit Proof Gate, the first mainstream exchange to commit to 100% reserve"
  },
  {
    "type": "doc",
    "url": "https://www.okx.com/markets/prices",
    "title": "Cryptocurrency Prices | Cryptocurrency List | Market Cap | OKX",
    "chunk": 0,
    "text": "You can start investing in cryptocurrency today on OKX. You can buy major cryptocurrencies, including BTC, ETH, USDT and LTC, with a credit card, Apple Pay or other convenient payment method. To get started, create an account using the button Sign up. Once you have verified your email, you can head to the Buy/Sell section to buy a range of top cryptocurrencies with popular payment methods. If you want to invest in a cryptocurrency from the list on this page, but don’t see it in the Buy/Sell section, that means you’ll need to use a top cryptocurrency like BTC or USDT to buy the asset. Select Trade button near any asset on this page to buy it for crypto."
  },
  {
    "type": "doc",
    "url": "https://www.gemini.com/prices",
    "title": "Crypto Prices - Bitcoin, Solana, and Other Coin Prices & Values",
    "chunk": 0,
    "text": "The pricing data and asset descriptions are for general informational purposes only and are not investment advice. Buying, selling, and trading cryptocurrency involves risks. Past performance is not a reliable indicator of future results. Full returns are not guaranteed; excluding trading fees and changes in currency fluctuations, values change frequently, and past performance may not be repeated. For tokens not supported for trading on the Gemini Exchange, pricing data is provided by CoinGecko, a third party data provider, with no affiliation to Gemini. For tokens that are supported for trading on the Gemini Exchange, the prices quoted are provided directly by Gemini. Trading fees vary by product and order size. Please see our fee schedules here ."
  },
  {
    "type": "doc",
    "url": "https://www.huobi.com/en-us/exchange/",
    "title": "The Leading Global Digital Asset Exchange",
    "chunk": 0,
    "text": "Quick Trade Buy virtual assets with Visa, Mastercard, and more"
  },
  {
    "type": "doc",
    "url": "https://www.kucoin.com/markets",
    "title": "Cryptocurrency Markets | Crypto Prices & Market Cap | KuCoin",
    "chunk": 0,
    "text": "2025-08-06 15:58:22 (UTC+8) 24h Volume 6,712,808,924 USDT"
  },
  {
    "type": "doc",
    "url": "https://www.investopedia.com/terms/t/technicalanalysis.asp",
    "title": "Technical Analysis: What It Is and How to Use It in Investing",
    "chunk": 0,
    "text": "What Is Technical Analysis? Technical analysis is a method of evaluating statistical trends in trading activity, typically involving price movement and volume. It is used to identify trading and investment opportunities. Unlike fundamental analysis, which attempts to evaluate a security's value based on financial information such as sales and earnings, technical analysis focuses on price and volume to draw conclusions about future price movements. Key Takeaways Technical analysis is used to evaluate price trends and patterns and thereby identify potential investments and trading opportunities. Technical analysts believe past trading activity and a security's price changes can be valuable indicators of the security's future price movements. Technical analysis may be contrasted with fundamental analysis, which focuses on a company's financials rather than historical price patterns or stock trends. Technical analysis was introduced by Charles Dow. Investopedia / Candra Huff Understanding Technical Analysis Technical analysis is used to scrutinize the ways supply and demand for a security affect changes in price, volume, and implied volatility. It assumes that past trading activity and price changes of a security can be valuable indicators of the security's future price movements when paired with appropriate investing or trading rules. Technical analysis' various charting tools are often used to generate short-term trading signals. They can also help improve the evaluation of a security's strength or weakness relative to the broader market or one of its sectors. This information helps analysts improve their overall valuation estimate. Technical analysis as we know it today was first introduced by Charles Dow as the Dow Theory in the late 1800s. Several noteworthy researchers including William P. Hamilton, Robert Rhea, Edson Gould, and John Magee further contributed to Dow Theory concepts. Nowadays, technical analysis has evolved to include hundreds of patterns and signals developed through years of research. How Technical Analysis Is Used Professional analysts often use technical analysis in conjunction with other forms of research. Retail traders may make decisions based solely on the price charts of a security and similar statistics. But practicing equity analysts rarely limit their research to fundamental or technical analysis alone. Technical analysis can be applied to any security with historical trading data. This includes stocks, futures , commodities , fixed-income securities, currencies, and more. In fact, technical analysis is prevalent in commodities and forex markets where traders focus on short-term price movements. Technical analysis attempts to forecast the price movement of virtually any tradable instrument that is generally subject to forces of supply and demand. Some view technical analysis as simply the supply and demand forces reflected by the market price movements of a security. Technical analysis most commonly applies to price changes, but some analysts track numbers other than just price, such as trading volume or open interest figures. Technical Analysis Indicators Hundreds of patterns and signals have been developed by researchers to support technical analysis trading. Technical analysts have also developed numerous types of trading systems to help them forecast and trade on price movements. Some indicators focus primarily on identifying the current market trend, including support and resistance areas. Others focus on determining the strength of a trend and the likelihood of its continuation. Commonly used technical indicators and charting patterns include trendlines, channels, moving averages, and momentum indicators. In general, technical analysts look at the following broad types of indicators: Fast Fact The CMT Association supports the largest collection of chartered or certified analysts using technical analysis professionally around the world. The association's Chartered Market Technician (CMT) designation can be obtained after three levels of exams that cover both a broad and deep look at technical analysis tools. Underlying Assumptions of Technical Analysis Technical analysis attempts to decipher the market sentiment behind price trends by looking for price patterns and trends. Charles Dow released a series of editorials discussing technical analysis theory. He had two basic assumptions that continue to form the framework for technical analysis trading. Markets are efficient with values that represent factors that influence a security's price. Even random market price movements appear to move in identifiable patterns and trends that tend to repeat over time. Today the field of technical analysis builds on Dow's work. Professional analysts typically accept three general assumptions: The market discounts everything: Technical analysts believe that everything from a company's fundamentals to broad market factors to market psychology is already priced into a stock. The Efficient Markets Hypothesis (EMH) draws a similar conclusion about prices. The only thing remaining is the analysis of price movements, which technical analysts view as the product of supply and demand for a particular stock. Price moves in trends: Technical analysts expect that prices, even in random market movements, will exhibit trends regardless of the time frame being observed. In other words, a stock price is more likely to continue a past trend than to move erratically. Most"
  },
  {
    "type": "doc",
    "url": "https://www.investopedia.com/terms/t/technicalanalysis.asp",
    "title": "Technical Analysis: What It Is and How to Use It in Investing",
    "chunk": 1,
    "text": "technical trading strategies are based on this assumption. History tends to repeat itself: The repetitive nature of price movements is often attributed to market psychology, which tends to be very predictable and based on emotions such as fear and excitement. Technical analysis uses chart patterns to analyze these emotions and subsequent price movements to understand trends. While many forms of technical analysis have been used for more than 100 years, they are believed to be relevant still because they illustrate patterns in price movements that often repeat themselves. Fundamental Analysis vs. Technical Analysis Fundamental analysis and technical analysis, the major schools of thought when it comes to approaching the markets, are at opposite ends of the spectrum. Both methods are used to research and forecast future trends in stock prices , and like any investment strategy or philosophy, both have their advocates and adversaries. Fundamental Analysis Fundamental analysis is a method of evaluating securities by attempting to measure the intrinsic value of a stock. Fundamental analysts study everything from the overall economy and industry conditions to the financial condition and management of companies. Earnings , expenses , assets, and liabilities are all important characteristics of fundamental analysis that help analysts determine the fair value of a business. Technical Analysis Technical analysis differs from fundamental analysis in that the stock's price and volume are the only inputs. The core assumption is that all publicly known fundamentals have factored into price; thus, there is no need to pay close attention to them. Technical analysts do not attempt to measure a security's intrinsic value, but instead, use stock charts to identify patterns and trends that suggest how a stock's price will move in the future. Limitations of Technical Analysis 1. For some analysts and academic researchers, the EMH demonstrates why no actionable information is contained in historical price and volume data. However, by the same reasoning, nor should business fundamentals provide actionable information. These points of view are known as the weak form and semi-strong form of the EMH. 2. Another criticism of technical analysis is that history does not repeat itself exactly, so price pattern study is of dubious importance and can be ignored. Prices seem to be better modeled as a random walk. 3. A third criticism of technical analysis is that it works in some cases but only because it constitutes a self-fulfilling prophecy. For example, many technical traders will place a stop-loss order below the 200-day moving average of a certain company. If a large number of traders have done so and the stock reaches this price, there will be a large number of sell orders, which will push the stock price down, confirming the movement traders anticipated. Then, other traders will see the price decrease and sell their positions, reinforcing the strength of the trend. This short-term selling pressure can be considered self-fulfilling, but it will have little bearing on where the asset's price will be weeks or months from now. In sum, if enough people use the same signals, they could cause the movement foretold by the signal. However, over the long run, this sole group of traders cannot drive the price. What Assumptions Do Technical Analysts Make? Professional technical analysts typically assume three things. First, the market discounts everything. Second, prices, even in random market movements, will exhibit trends regardless of the time frame being observed. Third, history tends to repeat itself. The repetitive nature of price movements is often attributed to market psychology, which tends to be very predictable. What's the Difference Between Fundamental and Technical Analysis? Fundamental analysis is a method of evaluating securities by attempting to measure the intrinsic value of a stock. The core assumption of technical analysis, on the other hand, is that all known fundamentals are factored into price; thus, there is no need to pay close attention to them. Technical analysts do not attempt to measure a security's intrinsic value, but instead, use stock charts to identify patterns and trends that might suggest how the security's price will move in the future. How Can I Learn Technical Analysis? Your first step is to learn about investing, stocks, markets, and financials. This can be done through books, online courses and materials, and in-person classes. Once you understand the basics, you can start studying technical analysis . The Bottom Line Technical analysis is a longstanding method of analyzing the price and volume data of securities to determine future price action. This data usually appears on charts. Investors and professional traders apply a variety of technical indicators to these price and volume charts to draw conclusions and make decisions about entry and exit points for trades."
  },
  {
    "type": "doc",
    "url": "https://www.tradingview.com/support/solutions/43000521824-technical-analysis-overview/",
    "title": "Pivot Points Standard",
    "chunk": 0,
    "text": "VIDEO The Pivot Points Standard technical indicator displays levels at which price might meet support or resistance. The Pivot Points indicator defines a single pivot point (P) level and several support (S) and resistance (R) levels. Inputs Type Specifies the type of pivot point calculation. Each pivot point type uses a different calculation logic (described in the Calculation section below) and draws its own number of levels. Pivots Timeframe The timeframe used for the pivots calculation. All available timeframes are daily or higher. For example, if \"Weekly\" is chosen, the indicator uses the current week's open and the previous week's open, high, low, and close (OHLC) to calculate the pivot point levels, and as a result, the levels change once a week. The “Auto” setting selects the following pivot timeframes: 1D for chart resolutions up to and including 15 min 1W for chart resolutions greater than 15 min and less than 1 day 1M for chart resolutions from 1 day and above Number of Pivots Back Specifies how many sets of pivots the indicator displays. For example, with “Pivot Timeframe” set to “Weekly”, the default “Number of Pivots Back” value of 15 means that the indicator displays pivots for each of the last 15 weeks. Note that the indicator can draw no more than 500 lines regardless of the value of this input. Use Daily-based Values Timeframes higher than 1D are built using daily data (for example, the weekly high is the highest daily high that week), and are called “daily-based” for this reason. By default, this setting is turned on, and the indicator requests data directly from the higher timeframe defined by the “Pivots Timeframe” setting (which is always daily or higher). With this setting off, the indicator constructs higher timeframe data from the chart data. Therefore, if the chart timeframe is 1D or above, this setting does not affect the pivot points calculation. However, intraday data on many symbols is a separate data feed from higher timeframe data with different OHLC values, and this difference can affect the pivot calculations. Turning this setting off on intraday charts can help to mitigate issues in some cases. Stocks can have different values for these reasons: Intraday and daily data sets often include different trades, resulting in different prices. Extended hours data, if provided, is usually not included in the daily candle. As a result, the Pivot Points calculated on intraday extended hours and those calculated on a daily-based chart are very different. Futures can have different values for these reasons: The daily close usually represents the settlement price, which is an average value and not specifically the price of the last trade (unless the \"Use settlement as close\" option is explicitly turned off). On intraday timeframes, the last price of the session is the price of the last trade. The daily candle uses Electronic Trading Hours, while the intraday charts might allow switching between different sessions. An advantage of having the “Use Daily-based Values” option turned on is that the pivot values remain consistent between intraday and higher timeframes. However, in some cases you might want to turn it off: To include Extended Trading Hours data for a stock such as AAPL, turn the setting off — the extended hours data exists only on intraday timeframes, and the daily-based calculation does not include it. To include Regular Trading Hours data on ES1! futures, turn the setting off — the daily data is always based on the Extended Trading Hours session. To calculate the Pivot Points on AAPL or ES1! and ensure that the pivots displayed on your 1 hour chart and the pivots on the 1 day chart have the same values, turn this option on. Otherwise, the pivots are calculated on the 1 hour dataset, which differs from the 1 day dataset, and the results are also different. Note: For all symbols, the number of bars of intraday data available on the chart is limited by your subscription, while the daily data is always presented in full regardless of your plan. If you use a small timeframe on the chart (for example, 1 minute) and a large timeframe as the Pivot Timeframe (for example, Yearly), your chart only has a few months of 1-minute data — not enough to match the higher timeframe. In this case, the indicator returns an error, prompting you to switch this setting on to ensure that the indicator has enough data to calculate pivots. Show Labels/Prices Specify whether to show the labels (\"P\", \"R1\", \"S1\", etc.) and their associated prices beside the pivot lines. Labels Position Specifies whether to display the labels and prices to the left or the right of the pivot line. Line Width Specifies the width of the pivot lines. P, S1, R1, etc. Specify whether to draw particular support and"
  },
  {
    "type": "doc",
    "url": "https://www.tradingview.com/support/solutions/43000521824-technical-analysis-overview/",
    "title": "Pivot Points Standard",
    "chunk": 1,
    "text": "resistance lines, and set their color. Note that not all pivot types include the full set of lines: for example, only Traditional and Camarilla pivots calculate S5 and R5 lines; all other pivot types do not display them even if the P5 and S5 inputs are checked. Calculation The pivot, resistance, and support values are calculated in different ways depending on the pivot point type set in the “Type” input. Types The Standard Pivot Points indicator uses the following types of pivot: Traditional Fibonacci Woodie Classic DM Camarilla The calculation formulas for each type are given below. The \"current\" and \"previous\" values refer to the current and previous bar of the timeframe selected in the “Pivots Timeframe” option. Traditional P = (prevHigh + prevLow + prevClose) / 3 R1 = P * 2 - prevLow S1 = P * 2 - prevHigh R2 = P + (prevHigh - prevLow) S2 = P - (prevHigh - prevLow) R3 = P * 2 + (prevHigh - 2 * prevLow) S3 = P * 2 - (2 * prevHigh - prevLow) R4 = P * 3 + (prevHigh - 3 * prevLow) S4 = P * 3 - (3 * prevHigh - prevLow) R5 = P * 4 + (prevHigh - 4 * prevLow) S5 = P * 4 - (4 * prevHigh - prevLow) Fibonacci P = (prevHigh + prevLow + prevClose) / 3 R1 = P + 0.382 * (prevHigh - prevLow) S1 = P - 0.382 * (prevHigh - prevLow) R2 = P + 0.618 * (prevHigh - prevLow) S2 = P - 0.618 * (prevHigh - prevLow) R3 = P + (prevHigh - prevLow) S3 = P - (prevHigh - prevLow) Woodie P = (prevHigh + prevLow + 2 * currOpen) / 4 R1 = 2 * P - prevLow S1 = 2 * P - prevHigh R2 = P + (prevHigh - prevLow) S2 = P - (prevHigh - prevLow) R3 = prevHigh + 2 * (P - prevLow) S3 = prevLow - 2 * (prevHigh - P) R4 = R3 + (prevHigh - prevLow) S4 = S3 - (prevHigh - prevLow) Classic P = (prevHigh + prevLow + prevClose) / 3 R1 = 2 * P - prevLow S1 = 2 * P - prevHigh R2 = P + (prevHigh - prevLow) S2 = P - (prevHigh - prevLow) R3 = P + 2 * (prevHigh - prevLow) S3 = P - 2 * (prevHigh - prevLow) R4 = P + 3 * (prevHigh - prevLow) S4 = P - 3 * (prevHigh - prevLow) DM if prevOpen == prevClose X = prevHigh + prevLow + 2 * prevClose else if prevClose > prevOpen X = 2 * prevHigh + prevLow + prevClose else X = 2 * prevLow + prevHigh + prevClose P = X / 4 R1 = X / 2 - prevLow S1 = X / 2 - prevHigh Camarilla P = (prevHigh + prevLow + prevClose) / 3 R1 = prevClose + 1.1 * (prevHigh - prevLow) / 12 S1 = prevClose - 1.1 * (prevHigh - prevLow) / 12 R2 = prevClose + 1.1 * (prevHigh - prevLow) / 6 S2 = prevClose - 1.1 * (prevHigh - prevLow) / 6 R3 = prevClose + 1.1 * (prevHigh - prevLow) / 4 S3 = prevClose - 1.1 * (prevHigh - prevLow) / 4 R4 = prevClose + 1.1 * (prevHigh - prevLow) / 2 S4 = prevClose - 1.1 * (prevHigh - prevLow) / 2 R5 = (prevHigh / prevLow) * prevClose S5 = prevClose - (R5 - prevClose)"
  },
  {
    "type": "doc",
    "url": "https://www.babypips.com/learn/forex/technical-analysis",
    "title": "What is Technical Analysis?",
    "chunk": 0,
    "text": "Technical analysis is the framework in which traders study price movement. The theory is that a person can look at historical price movements and determine the current trading conditions and potential price movement. Someone who uses technical analysis is called a technical analyst . Traders who use technical analysis are known as technical traders . The main evidence for using technical analysis is that, theoretically, all current market information is reflected in the price . Technical traders generally ascribe to the belief that “ It’s all in the charts! ” This simply means that all known fundamental information is priced into the current market price. If price reflects all the information that is out there, then price action is all one would really need to make a trade. Technical analysis looks at the rhythm, flow, and trends in price action. Now, have you ever heard the old adage, “ History tends to repeat itself “? Well, that’s basically what technical analysis is all about! If a certain price held as a major support or resistance level in the past, forex traders will keep an eye out for it and base their trades around that historical price level. Technical analysts look for similar patterns that have formed in the past and will form trade ideas, believing that price could possibly act the same way that it did before. Technical analysis is NOT so much about prediction as it is about POSSIBILITY. Technical analysis is the study of historical price action in order to identify patterns and determine the possibilities of the future direction of price . Technical analysis is the study of historical price action. So, how the heck does one “ study historical price action “? In the world of trading, when someone says “technical analysis”, the first thing that comes to mind is a chart . Technical analysts use charts because they are the easiest way to visualize historical data ! Technical analysts live, eat, and breathe charts, which is why they are often called chartists . Chartists believe that price action is the most reliable indicator of future price action. The Philosophy Behind Technical Analysis Technical analysis is rooted in several key philosophical principles: Market Efficiency: Prices reflect all publicly available information, making fundamental analysis less relevant. Crowd Psychology: Market participants’ emotions and behaviors create patterns and trends. History Repeats Itself: Human nature and market dynamics lead to similar patterns and cycles. Fractals: Markets exhibit fractal properties, with small patterns resembling larger ones. Market patterns repeat at different scales and timeframes. Probabilistic Thinking: Technical analysis deals with probabilities, not certainties. Adaptive Markets: Markets evolve, and technical analysis adapts to changing conditions You can look at past data to help you spot trends and patterns that could help you find some great trading opportunities. What’s more, with all the traders who rely on technical analysis, these price patterns and indicator signals tend to become self-fulfilling. As more and more forex traders look for certain price levels and chart patterns, the more likely that these patterns will manifest themselves in the markets. You should know, though, that technical analysis is VERY subjective. Just because Michelangelo, Donatello, Leonardo, and Raphael are looking at the exact same chart setup or indicators doesn’t mean that they will come up with the same idea of where price may be headed. The important thing is that you understand the concepts under technical analysis so you won’t get nosebleeds whenever somebody starts talking about Fibonacci, Bollinger Bands , or pivot points. Technical analysis does predict the future . It is the study of historical price action, which reflects the psychology of market participants. By examining past behavior, we can identify statistical patterns in how traders and investors have acted. These patterns help form educated guesses about how they might behave in the near term, assuming similar conditions apply. Now we know you’re thinking to yourself, “Geez, these guys are smart. They use crazy words like ‘Fibonacci’ and ‘Bollinger’. I can never learn this stuff!” Don’t worry yourself too much. After you’re done with the School of Pipsology, you too will be just as… uhmmm… “smart” as us."
  },
  {
    "type": "doc",
    "url": "https://trendspider.com/learning-center/",
    "title": "Learning Resources for Investors",
    "chunk": 0,
    "text": "Explore the product See how TrendSpider can help you become a better, smarter, more strategic investor"
  },
  {
    "type": "doc",
    "url": "https://decrypt.co/",
    "title": "Decrypt: AI, Bitcoin, Culture, Gaming, and Crypto News",
    "chunk": 0,
    "text": "The Morning Minute spans crypto and NFT news and market analysis, written by Tyler Warner, an ex-banking consultant turned high-volume NFT trader, Pudgy Penguin maxi, and NFT market analyst at Lucky Trader."
  },
  {
    "type": "doc",
    "url": "https://cryptoslate.com/",
    "title": "Crypto News, Insights & Data",
    "chunk": 0,
    "text": "Inside the mind of Lyn Alden: Bitcoin, AI, and the unstoppable deficit train Slate Sundays 3 hours ago"
  },
  {
    "type": "doc",
    "url": "https://bitcoinmagazine.com/",
    "title": "Bitcoin News, Analysis & Insights",
    "chunk": 0,
    "text": "Our Mission is Unchanged Bitcoin Magazine is the oldest and most established source of news, information and expert commentary on Bitcoin, its underlying blockchain technology and the industry that has been built up around it. Since 2012, Bitcoin Magazine has provided analysis, research, education and thought leadership at the intersection of finance and technology."
  },
  {
    "type": "doc",
    "url": "https://www.newsbtc.com/",
    "title": "The Latest Bitcoin News & Expert Crypto Insights",
    "chunk": 0,
    "text": "As the first-ever and most valuable cryptocurrency by market cap, Bitcoin continues to dominate headlines and shape the trajectory of the entire crypto market. First introduced in a whitepaper by ‘Satoshi Nakamoto’ in 2008, and officially released to the world in January 2009 as the very first decentralized cryptocurrency, Bitcoin was the first financial instrument of its kind to use cartography to record and send transactions over the blockchain. The fact that a currency could be transacted peer-to-peer away from central banks and had a permanent and publicly accessible ledger (in the form of the blockchain), was groundbreaking at the time, and it comes as no surprise that this new form of currency would grow in popularity as it did. As a digital currency, Bitcoin does not need to be bought and sold in whole units and can be exchanged in fractions made up of satoshis (100 million satoshis make up one bitcoin). This is why despite there only ever being a finite pool of 21 million Bitcoins that can be created or mined, almost anyone can own some part of a Bitcoin. This finite pool also offers partial resistance to economic issues such as hyperinflation or a banking collapse. To some degree, this perhaps explains why Bitcoin’s adoption and appeal has become so widespread in the last few years. Let’s cover some brief history with this crypto. There was no funding round or launch in the manner we see with newer ICOs these days. The individual or group known as Satoshi Nakamoto launched the Bitcoin network back in 2009 when they mined the Bitcoin genesis block and the 50BTC first entered circulation at $0. Before the emergence of crypto exchanges, Bitcoin transactions weren’t easy, PayPal was one of the primary methods used to buy and sell Bitcoin. Bitcoin was being exchanged but it took until 2010 for the first Bitcoin transaction to take place (for a legitimate physical item). Laszlo posted in the ‘bitcointalk’ forum and ended up paying 10,000 Bitcoins for 2 Papa John’s pizzas (little did he know then! In his defence, this was a period, where Bitcoin was worth little to nothing). Following 2011, Bitcoin started to gain far more adopters and attention, with its earliest and more savvy adopters exploring its potential in web transactions. Several websites popped up allowing Bitcoin users to transact with their crypto for physical goods, which fuelled further adoption, and the very first Bitcoin price spike in 2013 where one Bitcoin would cost just over $1000. It was also around this period, that NewsBTC first started as a Bitcoin and crypto news outlet. 2014 marked a slight downturn in Bitcoin’s price trajectory, when one of the first crypto trading exchanges, Mt. Gox, suffered a security breach with hackers stealing $60 million, leading to the closure of the exchange and a slightly muted sentiment towards the cryptocurrency. Investor and user confidence did recover in the years following, with the price of bitcoin starting to break the $4000 mark in 2017 causing media coverage to grow and draw in more and more retail customers, Bitcoin finally surged to an incredible new all-time high of over $18,000 in December 2021. As the hype, continuous adoption around the world and price of Bitcoin have increased, coupled with the fact that financial institutions, Washington and the rest of the world governments are finally interested in its potential, it’s more important than ever to keep an eye on Bitcoin. Over the years, more and more physical and online stores have accepted it as payment, with financial institutions finally investing in it, and some countries accepting it as legal tender (El Salvador), Bitcoin can now be seen as a legitimate asset with all the features of a revolutionary new technology that will be here to stay for the long-haul. With the amount of Bitcoin that has already been mined to date, technological innovations such as the move from cold storage to wallets offered by crypto exchanges, overnight millionaires and the interest poured in from major financial institutions, it’s clear Bitcoin isn’t going away anytime soon. A recent surge in March 2024 gave Bitcoin another new all-time high (exceeding a value of over 73,000 USD), the lowered costs to transact, and the launch of new ETFs (exchange-traded funds) approved by the Securities and Exchange Commission (SEC), there’s greater consensus, that now is a more exciting time than ever for individuals and expert investors looking to get invested in digital assets like Bitcoin. Despite the obstacles, this crypto faced so far from hacks, security breaches, or even country-wide bans, Bitcoin’s ecosystem and infrastructure have evolved, with greater strides made in privacy, usability and improving scalability. Keeping a pulse on reputable Bitcoin news is paramount for traders, investors, active users and enthusiasts alike, as it provides valuable insights into investing,"
  },
  {
    "type": "doc",
    "url": "https://www.newsbtc.com/",
    "title": "The Latest Bitcoin News & Expert Crypto Insights",
    "chunk": 1,
    "text": "market sentiment, bitcoin price movements, regulation, and the overarching trends influencing the markets and this groundbreaking financial technology. The latest bitcoin and crypto news should not be solely restricted to price fluctuations. A comprehensive Bitcoin analysis should deep dive into the technical and fundamental aspects of BTC, exploring the intricacies of the underlying blockchain technology, Bitcoin mining mechanisms, and the potential impact of new layers and upgrades. Our in-depth examination of all these areas in Bitcoin and crypto aims to equip readers with a much more well-rounded understanding of the forces shaping the Bitcoin ecosystem. Beyond BTC news, great cryptocurrency news should explore as many altcoins, decentralized finance (DeFi) projects, and other emerging technologies like AI out there, that could very well be the next disrupter. As we’ve all seen, financial advisors are increasingly interested in understanding crypto, its potential profitability and its implications for their clients. As such, NewsBTC is dedicated to giving readers the most accurate window into these important developments, partnerships, and innovations, allowing our readers to stay far ahead of the curve. Our analysis provides thorough examinations of this rapidly advancing space. From evaluating the feasibility of new projects to assessing the potential impact of government policies, our expert analysis equips investors and enthusiasts with the knowledge necessary to navigate the complexities of the crypto space with confidence. Our crypto market news will address a broad spectrum of topics, including regulatory updates, institutional adoption, ICOs, and the integration of cryptocurrencies into various sectors. As more countries explore central bank digital currencies (CBDCs) and businesses embrace crypto payments, staying informed about these developments becomes crucial. After all, the U.S. Securities and Exchange Commission’s recent efforts have had a critical role in shaping the current market environment. NewsBTC’s commitment to delivering the latest Bitcoin news (as well as broader cryptocurrency news), extends beyond merely reporting on events. The platform’s team of experienced journalists and analysts delve into the heart of the matter, providing insightful commentary, conducting in-depth interviews with industry leaders, and offering expert opinions on the future of the cryptocurrency space. Our comprehensive approach ensures that readers stay updated and receive a well-rounded understanding of the complex and ever-evolving world of cryptocurrencies, the more articles you read, the more confident we hope you will be! Furthermore, NewsBTC recognizes the importance of timely and accurate information in this fast-paced crypto market. The latest cryptocurrency news plays a pivotal role in helping investors identify potential investment opportunities and manage their portfolios effectively. By staying on top of the latest cryptocurrency news, investors can gain valuable insights into emerging trends in the crypto space, new projects, and overall market sentiment, enabling them to make well-informed decisions that align with their investment goals and risk tolerance. This dedication to promptness and precision sets NewsBTC apart as a trusted source for cryptocurrency enthusiasts, traders, and investors worldwide. As the go-to source for Bitcoin news, cryptocurrency news, and crypto market analysis, NewsBTC remains committed to empowering its readers with the knowledge and tools necessary to navigate the exciting and transformative world of digital assets."
  },
  {
    "type": "doc",
    "url": "https://u.today/",
    "title": "IT, AI and Fintech Daily News for You Today",
    "chunk": 0,
    "text": "Successful! Thank you. We'll contact you shortly. Ok"
  },
  {
    "type": "doc",
    "url": "https://cryptopotato.com/",
    "title": "Bitcoin & Altcoins, Crypto News and Guides",
    "chunk": 0,
    "text": "A mix of retail enthusiasm, institutional buying, and record supply growth has pushed Ethereum past $4,000 for the first time since 2024."
  },
  {
    "type": "doc",
    "url": "https://blockworks.co/",
    "title": "News and insights about digital assets.",
    "chunk": 0,
    "text": "133 W 19th St., New York, NY 10011"
  },
  {
    "type": "doc",
    "url": "https://cryptobriefing.com/",
    "title": "Crypto Briefing - Bitcoin, Ethereum and the future of finance",
    "chunk": 0,
    "text": "Ethereum's surge reflects growing institutional confidence, potentially accelerating mainstream adoption and influencing broader crypto markets. Markets Aug. 8, 2025"
  },
  {
    "type": "doc",
    "url": "https://cryptonews.com/",
    "title": "Latest Cryptocurrency News and Analysis",
    "chunk": 0,
    "text": "[LIVE] Crypto News Today: Latest Updates for August 08, 2025 – Crypto Market Soars as Trump Greenlights 401(k) Crypto Investments, ETH Tops $3,900 Industry Talk by Jai Pratap"
  },
  {
    "type": "doc",
    "url": "https://etherscan.io/",
    "title": "Ethereum (ETH) Blockchain Explorer",
    "chunk": 0,
    "text": "Please DO NOT store any passwords or private keys here. A private note (up to 100 characters) can be saved and is useful for transaction tracking."
  },
  {
    "type": "doc",
    "url": "https://bscscan.com/",
    "title": "BNB Smart Chain (BNB) Blockchain Explorer",
    "chunk": 0,
    "text": "Please DO NOT store any passwords or private keys here. A private note (up to 100 characters) can be saved and is useful for transaction tracking."
  },
  {
    "type": "doc",
    "url": "https://polygonscan.com/",
    "title": "Polygon PoS Chain (POL) Blockchain Explorer",
    "chunk": 0,
    "text": "Please DO NOT store any passwords or private keys here. A private note (up to 100 characters) can be saved and is useful for transaction tracking."
  },
  {
    "type": "doc",
    "url": "https://academy.binance.com/en",
    "title": "Free Crypto & Blockchain Education",
    "chunk": 0,
    "text": "BINANCE ACADEMY BLOCKCHAIN & CRYPTO EDUCATION Join a Global Community & Learn About Crypto and Blockchain for Free!"
  },
  {
    "type": "doc",
    "url": "https://www.investopedia.com/cryptocurrency-4427699",
    "title": "Investing in cryptocurrency",
    "chunk": 0,
    "text": "Decentralized finance, also known as DeFi, uses new technology to remove third parties such as banks and other traditional financial institutions in financial transactions. By removing centralized control by banks and other institutions over money, financial products, and financial services, the new financial applications may lower related maintenance costs and fees charged by banks—and also increase the speed of such services."
  },
  {
    "type": "doc",
    "url": "https://www.coinbureau.com/",
    "title": "The Coin Bureau - Your Crypto Gateway",
    "chunk": 0,
    "text": "Analysis Top Crypto Narratives to Watch Out in 2025 Discover the top crypto narratives of 2025, including AI agents, Ethereum's scalability, and tokenized real-world assets."
  },
  {
    "type": "doc",
    "url": "https://www.tradingview.com/ideas/",
    "title": "Trading Ideas and Technical Analysis From Top Traders",
    "chunk": 0,
    "text": "English English Select market data provided by ICE Data services. Select reference data provided by FactSet. Copyright © 2025 FactSet Research Systems Inc. © 2025 TradingView, Inc."
  },
  {
    "type": "doc",
    "url": "https://cryptostats.community/",
    "title": "Homepage | CryptoStats",
    "chunk": 0,
    "text": "Adapter Name Gitcoin Data type Treasury Data 24 hours fees: $564,507,541.79"
  },
  {
    "type": "doc",
    "url": "https://www.cryptocompare.com/",
    "title": "Cryptocurrency Prices, Portfolio, Forum, Rankings",
    "chunk": 0,
    "text": "In the fast-paced world of finance, staying ahead of the curve is crucial for successful investing. Enter the era of AI trading bot, revolutionizing the way we approach financial markets. These intelligent systems leverage artificial intelligence and…"
  },
  {
    "type": "doc",
    "url": "https://medium.com/topic/algorithmic-trading",
    "title": "Medium",
    "chunk": 0,
    "text": "404 Out of nothing, something. You can find (just about) anything on Medium — apparently even a page that doesn’t exist. Maybe these stories will take you somewhere new?"
  },
  {
    "type": "doc",
    "url": "https://towardsdatascience.com/tagged/algorithmic-trading",
    "title": "Algorithmic Trading",
    "chunk": 0,
    "text": "In this article, I have tested the famous Quandl API to download real estates, economics,…"
  },
  {
    "type": "doc",
    "url": "https://www.quantstart.com/articles/",
    "title": "Articles | QuantStart",
    "chunk": 0,
    "text": "Autoregressive Integrated Moving Average ARIMA(p, d, q) Models for Time Series Analysis"
  },
  {
    "type": "doc",
    "url": "https://www.coindesk.com/markets/2025/08/10/here-are-3-bullish-reasons-why-jpmorgan-sees-s-and-p-500-rallying-much-higher",
    "title": "Here Are 3 Bullish Reasons Why JPMorgan Sees S&P 500 Rallying Much Higher",
    "chunk": 0,
    "text": "JPMorgan remains bullish on U.S. stocks even as some observers warn that the economy is beginning to pay the price for President Donald Trump's tariffs. The investment banking giant forecasts that the S&P 500, Wall Street's benchmark index, will yield a \"high single-digit return over the next 12 months,\" driven by three key factors. One of the main reasons for optimism is that markets don't care about signs of an economic slowdown. Instead, traders are focused on resilient corporate earnings and the subsequent economic recovery. Since President Trump fired the first tariff salvo on April 2, economists have downgraded full-year U.S. growth forecasts from 2.3% to 1.5%. Still, the S&P 500 has gained over 28% in the four months. The index has held steady despite recent economic data revealing softness in the labour market and consumption , as well as stickiness in manufacturing and service sector inflation . While the macro analysts' warning is concerning and likely playing out in the background, corporate earnings in the U.S. are ignoring the slowdown risks, at least in the short term, making it the second catalyst for JPMorgan's bullish thesis. Over 80% of S&P 500 companies have recently reported their Q2 earnings, with 82% surpassing earnings expectations and 79% beating revenue forecasts—the strongest performance since the second quarter of 2021. The winners and losers According to JPMorgan, while Wall Street analysts initially projected earnings growth below 5%, the index is now on pace for an impressive 11% growth rate. This robust showing supports the ongoing bullish trend in the stock market. \"The full-year earnings expectations for both this year and next have already started to turn higher,\" analysts at JPMorgan's wealth management said in a market note on Friday, adding that the market is increasingly differentiating between the winners and losers of the Trump trade war. Additionally, the market is now figuring out and pricing in which companies are getting hit most by U.S. tariffs. So far, it looks like mega corporations will be just fine. This could bolster the case for further positive sentiment in the markets. JPMorgan analysts explained that consumer-facing and smaller companies with restrained bargaining power against their trading partners and rigid supply chains are facing a stagnant earnings outlook. This ties to JPMorgan's last catalyst: Trump's tariff bark is proving worse than its bite for large firms, which are managing to secure exemptions and even turn the tariff policies, aimed at sparking a manufacturing boom, into a tailwind. \"The latest example is President Donald Trump’s suggestion that imported semiconductors would be taxed at a 100% rate unless the companies commit to relocating production to the United States. Another sign? Apple products are exempted from the latest tariff rates on Indian goods. Indeed, the company also announced an additional $100 billion investment in U.S. manufacturing facilities. The stock gained almost 9% this week. Tariffs are not happening in a vacuum,\" analysts explained. Big firms gain an additional advantage from the One Big Beautiful Act (OBBA), under which firms can claim 100% bonus depreciation for purchases of qualified business property and immediate expense of domestic research and development costs. According to some analysts, the depreciation policy could increase free cash flow for some by over 30%, which could incentivize more investment. The bank added that its investment strategy remains focused on large-cap equities, particularly in the technology, financials, and utilities sectors, which it believes are best positioned to navigate this new economic environment. The crypto angle JPMorgan's positive outlook for stocks could bode well for cryptocurrencies, as both tend to move in tandem. The digital assets market has plenty going on for itself, with the Trump administration appointing pro-crypto officials to key regulatory positions. Recently, the U.S. Securities and Exchange Commission (SEC) ruled that liquid staking, under certain conditions, falls outside the purview of Securities Law. The ruling has raised hopes for staking spot ether ETFs winning regulatory approval. Ether has rallied over 13% to over $4,200, reaching levels last seen in 2021. Prices surged nearly 50% last month, CoinDesk data show ."
  },
  {
    "type": "doc",
    "url": "https://www.coindesk.com/markets/2025/08/10/u-s-spot-xrp-etfs-five-possible-reasons-behind-blackrock-s-hesitation-to-file-for-one",
    "title": "Why BlackRock Might be Reluctant to Pursue a U.S.-Listed Spot XRP ETF",
    "chunk": 0,
    "text": "BlackRock has made bold moves into bitcoin and ether ETFs, but on Friday, the asset manager said it had no immediate plans to file for a spot XRP exchange-traded fund (ETF), dashing the community’s hopes that its entry could help extend XRP’s 2025 rally. This statement — made the day after the U.S. Securities and Exchange Commission (SEC) and Ripple Labs jointly asked an appeals court to dismiss their respective appeals, signaling an end to their nearly five-year legal battle — has left investors questioning why BlackRock remains on the sidelines. While several asset managers, including ProShares, Grayscale, and Bitwise, have filed for XRP ETFs since late 2024, BlackRock’s absence is notable, especially given its dominance in the bitcoin and ether ETF markets. Here are five reasons why BlackRock appears in no hurry to launch a spot XRP ETF, despite the XRP community’s anticipation of a demand-driven price surge. First, BlackRock has cited limited client interest in cryptocurrencies beyond BTC and ETH. Back in March 2024, Robert Mitchnick, the asset manager's head of digital assets, said that there's a misconception that BlackRock will have a \"long tail\" of other crypto services. \"I can say that for our client base, bitcoin is overwhelmingly the No. 1 focus and a little bit ethereum,\" he said during a fireside chat at the inaugural Bitcoin Investor Day conference in New York on March 22. Second, BlackRock’s strategic caution around regulatory uncertainty plays a role. Although XRP sales on public exchanges are deemed non-securities, the broader regulatory framework for altcoins remains murky. BlackRock may be waiting for clearer SEC guidelines before entering the altcoin ETF space. The firm’s conservative approach contrasts with competitors like ProShares, which filed for a spot XRP ETF in January 2025 alongside leveraged and futures-based XRP ETFs, the latter tracking XRP futures contracts rather than the token’s spot price. Third, BlackRock may see diminishing returns in pursuing a spot XRP ETF given the crowded field. As of August 2025, at least seven firms, including Grayscale, Franklin Templeton and 21Shares, have a pending spot XRP ETF application. Fourth, the XRP community’s expectations of a price surge may not align with BlackRock’s data-driven strategy. Polymarket odds for the SEC approving a spot XTP ETF in 2025 stand at 77%. BlackRock's tokenized money market fund on Ethereum and Solana shows blockchain interest, but XRP’s smaller market footprint may not justify the operational costs of a new ETF. Finally, BlackRock’s global perspective prioritizes markets where XRP demand is less pronounced. While the XRP community, active on platforms like X, anticipates a spot ETF driving demand, much of XRP’s trading volume comes from Asia, where BlackRock’s ETF presence is less dominant. At press time, XRP was trading around $3.1852, down 3.92% in the past 24 hours, according to CoinDesk Data."
  },
  {
    "type": "doc",
    "url": "https://www.coindesk.com/markets/2025/08/09/bitcoin-trails-gold-in-2025-but-dominates-long-term-returns-across-major-asset-classes",
    "title": "BTC YTD Performance 2nd to Gold but 308,709x Higher Total Return Since 2011",
    "chunk": 0,
    "text": "Bitcoin slipped 0.11% in the past 24 hours to $116,702, according to CoinDesk Data, but it remains up 25% year to date, second only to gold’s 29% gain among major asset classes, according to data shared by financial strategist Charlie Bilello on X. Both assets have outperformed other major asset classes this year, such as emerging market stocks (VWO +15.6%), the Nasdaq 100 (QQQ +12.7%) and U.S. large caps (SPY +9.4%). Meanwhile, U.S. mid-caps (MDY) and small-caps (IWM) have only gained 0.2% and 0.8%, respectively, his data showed. This marks the first time gold and bitcoin have occupied the top two positions in Bilello’s annual asset class rankings since records began. The big picture However, zooming out, bitcoin has delivered an extraordinary 38,897,420% total return since 2011 — a figure that dwarfs all other asset classes in the dataset. Gold’s 126% cumulative return over the same period puts it in the middle of the pack, trailing equity benchmarks like the Nasdaq 100 (1101%) and U.S. large caps (559%), as well as mid caps (316%), small caps (244%) and emerging market stocks (57%). Based on Bilello’s figures, bitcoin’s total return has exceeded gold’s by more than 308,000 times over the past 14 years. When measured on an annualized basis, bitcoin’s dominance is equally clear. The flagship cryptocurrency has delivered a 141.7% average annual gain since 2011, compared with 5.7% for gold, 18.6% for the Nasdaq 100, 13.8% for U.S. large caps and 4.4% to 16.4% for other major equity and real estate indexes, Bilello's data showed. Gold’s long-term stability has made it a valuable hedge in certain market cycles, but its pace of appreciation has been far slower than bitcoin’s exponential climb. Store of value: Gold vs. bitcoin Renowned trader Peter Brandt weighed in on Aug. 8, contrasting gold’s merits as a store of value with bitcoin’s potential to surpass all fiat alternatives. “Some think gold is a great store of value — and it is. But the ultimate store of value will prove to be bitcoin,” he said on X, sharing a long-term chart of the U.S. dollar’s purchasing power. His comments echo the growing narrative that bitcoin’s scarcity and decentralization make it uniquely positioned to outperform traditional hedges over time. Read more: Chart of the Week: Tariff Carnage Starting to Fulfill Bitcoin's 'Store of Value' Promise Bitcoin’s ability to hold above six figures in 2025 while maintaining a top-two performance among major assets underscores its resilience in a volatile macro backdrop. Traders are watching whether it can retest the year’s peak near $123,000, while long-term holders point to its outperformance since 2011 as evidence of its staying power. Market participants say upcoming macro data and risk appetite across equities and commodities could set the tone for the next leg. Read more: Bitcoin Still on Track for $140K This Year, But 2026 Will Be Painful: Elliott Wave Expert"
  },
  {
    "type": "doc",
    "url": "https://news.bitcoin.com/xrp-strategy-from-natures-miracle-spans-gaming-dining-travel-evs/",
    "title": "XRP Strategy From Nature’s Miracle Spans Gaming, Dining, Travel, EVs",
    "chunk": 0,
    "text": "You need to enable JavaScript to run this app."
  },
  {
    "type": "doc",
    "url": "https://news.bitcoin.com/top-bitcoin-casinos-usa-august-2025/",
    "title": "Top Bitcoin Casinos - USA [August 2025] - Reviews Bitcoin News",
    "chunk": 0,
    "text": "You need to enable JavaScript to run this app."
  },
  {
    "type": "doc",
    "url": "https://news.bitcoin.com/another-win-for-xrp-as-ripple-secures-new-sec-waiver-accelerating-institutional-adoption/",
    "title": "Another Win for XRP as Ripple Secures New SEC Waiver, Accelerating Institutional Adoption",
    "chunk": 0,
    "text": "You need to enable JavaScript to run this app."
  },
  {
    "type": "doc",
    "url": "https://www.coingecko.com/",
    "title": "Cryptocurrency Prices, Charts, and Crypto Market Cap",
    "chunk": 0,
    "text": "Your All-in-One Platform For Cryptocurrency Market Data Welcome to CoinGecko! As crypto traders and investors ourselves, we understand the hassle of browsing multiple websites and exchanges to find reliable information and market data for a coin. That’s why we built CoinGecko – so you can access all crypto information in one place. Get Reliable Live Cryptocurrency Prices Crypto prices and market data have always been at the core of our product – it’s what we do best. We provide unbiased cryptocurrency data for the community, whether to help you make an investment decision or check the value of your crypto assets. We use an average price as crypto prices vary between markets. Crypto prices on an exchange depend on its market condition, influenced by factors like liquidity, trading pairs, offerings, and economic conditions. As exchanges may sometimes show abnormal prices, the crypto community relies on tools like CoinGecko for more accurate coin prices. We Offer Market Data, Crypto Charts & Latest News Crypto prices alone don’t mean much, so we’ve included data like market cap and fully diluted value. It’s only when you pair current prices with historical data, statistics, news, and more that you get a full picture of a coin’s performance. Another popular feature on CoinGecko is our free and powerful crypto charts. What’s neat is you can compare a coin’s performance against Bitcoin and/or Ethereum on a single graph. Our price charts come in both line and candlestick formats, and we also offer market cap charts. How Does CoinGecko Calculate Crypto Prices? Our prices are calculated using an average price formula based on available trading pairs across multiple exchanges. We also have algorithms to detect and exclude anomalous tickers from our prices. Learn more about our methodology . Where Can You Get Cryptocurrency Prices? You can get real-time coin prices with our cryptocurrency price tracker by clicking on the coins in the table above. Our price trackers are usually sufficient for regular investors. But if you’re an advanced trader looking for more granular price data or a developer building crypto applications showing market data, our API gives you a comprehensive set of crypto data, including live and historical crypto prices, market cap, market volume, and more. Crypto Prices in Global Currencies If you’re looking for crypto prices in your local currencies, check out our cryptocurrency pairs: Overall most popular cryptocurrency pairs: BTC-USD , ETH-USD , XRP-USD , SOL-USD and DOGE-USD Popular cryptocurrency pairs in Australia: BTC-AUD , ETH-AUD , XRP-AUD , SOL-AUD and DOGE-AUD Popular cryptocurrency pairs in Canada: BTC-CAD , ETH-CAD , XRP-CAD , SOL-CAD and USDT-CAD Popular cryptocurrency pairs in the United Kingdom: BTC-GBP , ETH-GBP , XRP-GBP , SOL-GBP and USDT-GBP Popular cryptocurrency pairs in India: BTC-INR , ETH-INR , USDT-INR , SOL-INR and XRP-INR What Is Crypto Market Cap? Crypto market cap is the total value of a cryptocurrency in circulation, calculated by multiplying the total number of coins by the current market price. It’s used to determine the valuation of a cryptocurrency based on the total money invested, not just the price. Why Is Crypto Market Cap Important? Investors use crypto market caps to determine if a coin has more room for growth or is currently overvalued by comparing it to established cryptocurrencies with similar use cases as a benchmark. While market cap is important, it’s only one of many factors to consider when investing in a coin. How To Compare Crypto Market Cap? Crypto market cap can be divided into three categories: Large-cap cryptocurrencies like Bitcoin and Ethereum have market caps above $10 billion. These established projects have higher liquidity and lower risks. Mid-cap cryptocurrencies have market caps between $1 billion and $10 billion. They have higher growth potential than large-cap coins but are also riskier. Small-cap cryptocurrencies have market caps below $1 billion. They are usually new projects with the potential for exponential growth but have lower liquidity and higher risk. These categories are often used in investment strategies, and a “good market cap” depends on your risk appetite. If you’re willing to take on higher risks, you can look into small- or mid-cap cryptocurrencies. For risk-averse investors, consider large-cap cryptocurrencies. List of Top Ranking Cryptocurrencies Our cryptocurrency list features the top cryptocurrencies today, including Bitcoin, Ethereum, and over ten thousand altcoins. What sets our list apart is we aggregate cryptos from various exchanges to give a full picture of the crypto market and not just for coins on a particular exchange. Our team curates the cryptocurrency list. We vet each coin to reduce the risk of scams and remove inactive coins or dead projects to keep our list relevant to the market. If you can’t find a coin on CoinGecko, try searching on our DEX tracker GeckoTerminal . How Are The Top Cryptocurrencies Ranked? We rank cryptocurrencies by market"
  },
  {
    "type": "doc",
    "url": "https://www.coingecko.com/",
    "title": "Cryptocurrency Prices, Charts, and Crypto Market Cap",
    "chunk": 1,
    "text": "cap. Market cap represents the market share of a coin or token, so the higher the crypto rank, the more dominant it is in the crypto market. What Are Altcoins? Altcoins are every other cryptocurrency created after Bitcoin, and they can be coins or tokens. Coins are native currencies to the blockchain, while tokens are cryptos built on the blockchain. There are many types of altcoins. Stablecoins are altcoins whose value is pegged to assets like fiat currencies and commodities. Another example is governance tokens, which let you vote for the Web3 project’s future. You can explore more types of altcoins on our categories pages. Get The Most Value Out Of CoinGecko Getting into crypto can be daunting. But our beginner-friendly articles and videos are there to help you take your first steps into the decentralized internet. As for crypto investors, you can create custom portfolios to track the performance of coins you're interested in. Many of our pro-users use our crypto highlights to discover crypto gems and see what’s popular today. If you're a developer building in the crypto space, you can tap into our robust, comprehensive crypto API . Not only does it provide prices and market data for over two million coins, you can also access crypto exchange data to compare prices across markets and build candlestick charts with our OHLCV data . Our API supports crypto data across 200+ networks, including major chains like Solana , Ethereum , BNB Chain , Polygon , and Base . No matter where you are in your crypto journey, we want to empower you with unbiased fundamental crypto data you need to thrive in this Web3 world. We’re excited to see you around!"
  },
  {
    "type": "doc",
    "url": "https://coinmarketcap.com/",
    "title": "Cryptocurrency Prices, Charts And Market Capitalizations",
    "chunk": 0,
    "text": "Today’s Cryptocurrency Prices, Charts and Data Welcome to CoinMarketCap.com! This site was founded in May 2013 by Brandon Chez to provide up-to-date cryptocurrency prices, charts and data about the emerging cryptocurrency markets. Since then, the world of blockchain and cryptocurrency has grown exponentially and we are very proud to have grown with it. We take our data very seriously and we do not change our data to fit any narrative: we stand for accurately, timely and unbiased information. All Your Crypto Market Data Needs in One Place Here at CoinMarketCap, we work very hard to ensure that all the relevant and up-to-date information about cryptocurrencies, coins and tokens can be located in one easily discoverable place. From the very first day, the goal was for the site to be the number one location online for crypto market data, and we work hard to empower our users with our unbiased and accurate information. We Provide Live and Historic Crypto Charts for Free Each of our coin data pages has a graph that shows both the current and historic price information for the coin or token. Normally, the graph starts at the launch of the asset, but it is possible to select specific to and from dates to customize the chart to your own needs. These charts and their information are free to visitors of our website. The most experienced and professional traders often choose to use the best crypto API on the market. Our API enables millions of calls to track current prices and to also investigate historic prices and is used by some of the largest crypto exchanges and financial institutions in the world. CoinMarketCap also provides data about the most successful traders for you to monitor. We also provide data about the latest trending cryptos and trending DEX pairs . How Do We Calculate Our Cryptocurrency Prices? We receive updated cryptocurrency prices directly from many exchanges based on their pairs. We then convert the number to USD. A full explanation can be found here . Related Links Are you ready to learn more? Visit our glossary and crypto learning center. Are you interested in the scope of crypto assets? Investigate our list of cryptocurrency categories. Are you interested in knowing which the hottest dex pairs are currently? How Do We Calculate Our Crypto Valuations? We calculate our valuations based on the total circulating supply of an asset multiplied by the currency reference price. The topic is explained in more detail here . How Do We Calculate the Cryptocurrency Market Cap? We calculate the total cryptocurrency market capitalization as the sum of all cryptocurrencies listed on the site. Does CoinMarketCap.com List All Cryptocurrencies? Almost. We have a process that we use to verify assets. Once verified, we create a coin description page like this . The world of crypto now contains many coins and tokens that we feel unable to verify. In those situations, our Dexscan product lists them automatically by taking on-chain data for newly created smart contracts. We do not cover every chain, but at the time of writing we track the top 70 crypto chains, which means that we list more than 97% of all tokens. How Big Is the Global Coin Market? At the time of writing, we estimate that there are more than 2 million pairs being traded, made up of coins, tokens and projects in the global coin market. As mentioned above, we have a due diligence process that we apply to new coins before they are listed. This process controls how many of the cryptocurrencies from the global market are represented on our site. What Is an Altcoin? The very first cryptocurrency was Bitcoin . Since it is open source, it is possible for other people to use the majority of the code, make a few changes and then launch their own separate currency. Many people have done exactly this. Some of these coins are very similar to Bitcoin, with just one or two amended features (such as Litecoin ), while others are very different, with varying models of security, issuance and governance. However, they all share the same moniker — every coin issued after Bitcoin is considered to be an altcoin. What Is a Smart Contract? The first chain to launch smart contracts was Ethereum . A smart contract enables multiple scripts to engage with each other using clearly defined rules, to execute on tasks which can become a coded form of a contract. They have revolutionized the digital asset space because they have enabled decentralized exchanges, decentralized finance, ICOs, IDOs and much more. A huge proportion of the value created and stored in cryptocurrency is enabled by smart contracts. What Is a Stablecoin? Price volatility has long been one of the features of the cryptocurrency market. When asset prices move quickly"
  },
  {
    "type": "doc",
    "url": "https://coinmarketcap.com/",
    "title": "Cryptocurrency Prices, Charts And Market Capitalizations",
    "chunk": 1,
    "text": "in either direction and the market itself is relatively thin, it can sometimes be difficult to conduct transactions as might be needed. To overcome this problem, a new type of cryptocurrency tied in value to existing currencies — ranging from the U.S. dollar, other fiats or even other cryptocurrencies — arose. These new cryptocurrency are known as stablecoins , and they can be used for a multitude of purposes due to their stability. What Is an NFT? NFTs are multi-use images that are stored on a blockchain. They can be used as art, a way to share QR codes, ticketing and many more things. The first breakout use was for art, with projects like CryptoPunks and Bored Ape Yacht Club gaining large followings. We also list all of the top NFT collections available, including the related NFT coins and tokens.. We collect latest sale and transaction data, plus upcoming NFT collection launches onchain. NFTs are a new and innovative part of the crypto ecosystem that have the potential to change and update many business models for the Web 3 world. What Are In-game Tokens? Play-to-earn (P2E) games, also known as GameFi , has emerged as an extremely popular category in the crypto space. It combines non-fungible tokens (NFT), in-game crypto tokens, decentralized finance (DeFi) elements and sometimes even metaverse applications. Players have an opportunity to generate revenue by giving their time (and sometimes capital) and playing these games. One of the biggest winners is Axie Infinity — a Pokémon-inspired game where players collect Axies (NFTs of digital pets), breed and battle them against other players to earn Smooth Love Potion (SLP) — the in-game reward token. This game was extremely popular in developing countries like The Philippines, due to the level of income they could earn. Players in the Philippines can check the price of SLP to PHP today directly on CoinMarketCap. What Are ETFs? In January 2024 the SEC approved 11 exchange traded funds to invest in Bitcoin. There were already a number of Bitcoin ETFs available in other countries, but this change allowed them to be available to retail investors in the United States. This opens the way for a much wider range of investors to be able to add some exposure to cryptocurrency in their portfolios. Which Is the Best Cryptocurrency to Invest in? CoinMarketCap does not offer financial or investment advice about which cryptocurrency, token or asset does or does not make a good investment, nor do we offer advice about the timing of purchases or sales. We are strictly a data company. Please remember that the prices, yields and values of financial assets change. This means that any capital you may invest is at risk. We recommend seeking the advice of a professional investment advisor for guidance related to your personal circumstances. If You Are Investing in Cryptocurrency — CoinMarketCap.com Is for You TThe data at CoinMarketCap updates every few seconds, which means that it is possible to check in on the value of your investments and assets at any time and from anywhere in the world. We look forward to seeing you regularly!"
  },
  {
    "type": "doc",
    "url": "https://academy.binance.com/en",
    "title": "Free Crypto & Blockchain Education",
    "chunk": 0,
    "text": "BINANCE ACADEMY BLOCKCHAIN & CRYPTO EDUCATION Join a Global Community & Learn About Crypto and Blockchain for Free!"
  },
  {
    "type": "doc",
    "url": "https://www.coinbureau.com/",
    "title": "The Coin Bureau - Your Crypto Gateway",
    "chunk": 0,
    "text": "Analysis Top Crypto Narratives to Watch Out in 2025 Discover the top crypto narratives of 2025, including AI agents, Ethereum's scalability, and tokenized real-world assets."
  },
  {
    "type": "doc",
    "url": "https://decrypt.co/",
    "title": "Decrypt: AI, Bitcoin, Culture, Gaming, and Crypto News",
    "chunk": 0,
    "text": "The Morning Minute spans crypto and NFT news and market analysis, written by Tyler Warner, an ex-banking consultant turned high-volume NFT trader, Pudgy Penguin maxi, and NFT market analyst at Lucky Trader."
  },
  {
    "type": "doc",
    "url": "https://blockworks.co/",
    "title": "News and insights about digital assets.",
    "chunk": 0,
    "text": "133 W 19th St., New York, NY 10011"
  },
  {
    "type": "doc",
    "url": "https://cryptobriefing.com/",
    "title": "Crypto Briefing - Bitcoin, Ethereum and the future of finance",
    "chunk": 0,
    "text": "Ethereum's surge reflects growing institutional confidence, potentially accelerating mainstream adoption and influencing broader crypto markets. Markets Aug. 8, 2025"
  },
  {
    "type": "doc",
    "url": "https://cryptoslate.com/",
    "title": "Crypto News, Insights & Data",
    "chunk": 0,
    "text": "Inside the mind of Lyn Alden: Bitcoin, AI, and the unstoppable deficit train Slate Sundays 4 hours ago"
  },
  {
    "type": "doc",
    "url": "https://bitcoinmagazine.com/",
    "title": "Bitcoin News, Analysis & Insights",
    "chunk": 0,
    "text": "Our Mission is Unchanged Bitcoin Magazine is the oldest and most established source of news, information and expert commentary on Bitcoin, its underlying blockchain technology and the industry that has been built up around it. Since 2012, Bitcoin Magazine has provided analysis, research, education and thought leadership at the intersection of finance and technology."
  },
  {
    "type": "doc",
    "url": "https://www.newsbtc.com/",
    "title": "The Latest Bitcoin News & Expert Crypto Insights",
    "chunk": 0,
    "text": "As the first-ever and most valuable cryptocurrency by market cap, Bitcoin continues to dominate headlines and shape the trajectory of the entire crypto market. First introduced in a whitepaper by ‘Satoshi Nakamoto’ in 2008, and officially released to the world in January 2009 as the very first decentralized cryptocurrency, Bitcoin was the first financial instrument of its kind to use cartography to record and send transactions over the blockchain. The fact that a currency could be transacted peer-to-peer away from central banks and had a permanent and publicly accessible ledger (in the form of the blockchain), was groundbreaking at the time, and it comes as no surprise that this new form of currency would grow in popularity as it did. As a digital currency, Bitcoin does not need to be bought and sold in whole units and can be exchanged in fractions made up of satoshis (100 million satoshis make up one bitcoin). This is why despite there only ever being a finite pool of 21 million Bitcoins that can be created or mined, almost anyone can own some part of a Bitcoin. This finite pool also offers partial resistance to economic issues such as hyperinflation or a banking collapse. To some degree, this perhaps explains why Bitcoin’s adoption and appeal has become so widespread in the last few years. Let’s cover some brief history with this crypto. There was no funding round or launch in the manner we see with newer ICOs these days. The individual or group known as Satoshi Nakamoto launched the Bitcoin network back in 2009 when they mined the Bitcoin genesis block and the 50BTC first entered circulation at $0. Before the emergence of crypto exchanges, Bitcoin transactions weren’t easy, PayPal was one of the primary methods used to buy and sell Bitcoin. Bitcoin was being exchanged but it took until 2010 for the first Bitcoin transaction to take place (for a legitimate physical item). Laszlo posted in the ‘bitcointalk’ forum and ended up paying 10,000 Bitcoins for 2 Papa John’s pizzas (little did he know then! In his defence, this was a period, where Bitcoin was worth little to nothing). Following 2011, Bitcoin started to gain far more adopters and attention, with its earliest and more savvy adopters exploring its potential in web transactions. Several websites popped up allowing Bitcoin users to transact with their crypto for physical goods, which fuelled further adoption, and the very first Bitcoin price spike in 2013 where one Bitcoin would cost just over $1000. It was also around this period, that NewsBTC first started as a Bitcoin and crypto news outlet. 2014 marked a slight downturn in Bitcoin’s price trajectory, when one of the first crypto trading exchanges, Mt. Gox, suffered a security breach with hackers stealing $60 million, leading to the closure of the exchange and a slightly muted sentiment towards the cryptocurrency. Investor and user confidence did recover in the years following, with the price of bitcoin starting to break the $4000 mark in 2017 causing media coverage to grow and draw in more and more retail customers, Bitcoin finally surged to an incredible new all-time high of over $18,000 in December 2021. As the hype, continuous adoption around the world and price of Bitcoin have increased, coupled with the fact that financial institutions, Washington and the rest of the world governments are finally interested in its potential, it’s more important than ever to keep an eye on Bitcoin. Over the years, more and more physical and online stores have accepted it as payment, with financial institutions finally investing in it, and some countries accepting it as legal tender (El Salvador), Bitcoin can now be seen as a legitimate asset with all the features of a revolutionary new technology that will be here to stay for the long-haul. With the amount of Bitcoin that has already been mined to date, technological innovations such as the move from cold storage to wallets offered by crypto exchanges, overnight millionaires and the interest poured in from major financial institutions, it’s clear Bitcoin isn’t going away anytime soon. A recent surge in March 2024 gave Bitcoin another new all-time high (exceeding a value of over 73,000 USD), the lowered costs to transact, and the launch of new ETFs (exchange-traded funds) approved by the Securities and Exchange Commission (SEC), there’s greater consensus, that now is a more exciting time than ever for individuals and expert investors looking to get invested in digital assets like Bitcoin. Despite the obstacles, this crypto faced so far from hacks, security breaches, or even country-wide bans, Bitcoin’s ecosystem and infrastructure have evolved, with greater strides made in privacy, usability and improving scalability. Keeping a pulse on reputable Bitcoin news is paramount for traders, investors, active users and enthusiasts alike, as it provides valuable insights into investing,"
  },
  {
    "type": "doc",
    "url": "https://www.newsbtc.com/",
    "title": "The Latest Bitcoin News & Expert Crypto Insights",
    "chunk": 1,
    "text": "market sentiment, bitcoin price movements, regulation, and the overarching trends influencing the markets and this groundbreaking financial technology. The latest bitcoin and crypto news should not be solely restricted to price fluctuations. A comprehensive Bitcoin analysis should deep dive into the technical and fundamental aspects of BTC, exploring the intricacies of the underlying blockchain technology, Bitcoin mining mechanisms, and the potential impact of new layers and upgrades. Our in-depth examination of all these areas in Bitcoin and crypto aims to equip readers with a much more well-rounded understanding of the forces shaping the Bitcoin ecosystem. Beyond BTC news, great cryptocurrency news should explore as many altcoins, decentralized finance (DeFi) projects, and other emerging technologies like AI out there, that could very well be the next disrupter. As we’ve all seen, financial advisors are increasingly interested in understanding crypto, its potential profitability and its implications for their clients. As such, NewsBTC is dedicated to giving readers the most accurate window into these important developments, partnerships, and innovations, allowing our readers to stay far ahead of the curve. Our analysis provides thorough examinations of this rapidly advancing space. From evaluating the feasibility of new projects to assessing the potential impact of government policies, our expert analysis equips investors and enthusiasts with the knowledge necessary to navigate the complexities of the crypto space with confidence. Our crypto market news will address a broad spectrum of topics, including regulatory updates, institutional adoption, ICOs, and the integration of cryptocurrencies into various sectors. As more countries explore central bank digital currencies (CBDCs) and businesses embrace crypto payments, staying informed about these developments becomes crucial. After all, the U.S. Securities and Exchange Commission’s recent efforts have had a critical role in shaping the current market environment. NewsBTC’s commitment to delivering the latest Bitcoin news (as well as broader cryptocurrency news), extends beyond merely reporting on events. The platform’s team of experienced journalists and analysts delve into the heart of the matter, providing insightful commentary, conducting in-depth interviews with industry leaders, and offering expert opinions on the future of the cryptocurrency space. Our comprehensive approach ensures that readers stay updated and receive a well-rounded understanding of the complex and ever-evolving world of cryptocurrencies, the more articles you read, the more confident we hope you will be! Furthermore, NewsBTC recognizes the importance of timely and accurate information in this fast-paced crypto market. The latest cryptocurrency news plays a pivotal role in helping investors identify potential investment opportunities and manage their portfolios effectively. By staying on top of the latest cryptocurrency news, investors can gain valuable insights into emerging trends in the crypto space, new projects, and overall market sentiment, enabling them to make well-informed decisions that align with their investment goals and risk tolerance. This dedication to promptness and precision sets NewsBTC apart as a trusted source for cryptocurrency enthusiasts, traders, and investors worldwide. As the go-to source for Bitcoin news, cryptocurrency news, and crypto market analysis, NewsBTC remains committed to empowering its readers with the knowledge and tools necessary to navigate the exciting and transformative world of digital assets."
  },
  {
    "type": "doc",
    "url": "https://cryptopotato.com/",
    "title": "Bitcoin & Altcoins, Crypto News and Guides",
    "chunk": 0,
    "text": "A mix of retail enthusiasm, institutional buying, and record supply growth has pushed Ethereum past $4,000 for the first time since 2024."
  },
  {
    "type": "doc",
    "url": "https://www.cryptocompare.com/",
    "title": "Cryptocurrency Prices, Portfolio, Forum, Rankings",
    "chunk": 0,
    "text": "In the fast-paced world of finance, staying ahead of the curve is crucial for successful investing. Enter the era of AI trading bot, revolutionizing the way we approach financial markets. These intelligent systems leverage artificial intelligence and…"
  },
  {
    "type": "doc",
    "url": "https://www.investopedia.com/terms/t/technicalanalysis.asp",
    "title": "Technical Analysis: What It Is and How to Use It in Investing",
    "chunk": 0,
    "text": "What Is Technical Analysis? Technical analysis is a method of evaluating statistical trends in trading activity, typically involving price movement and volume. It is used to identify trading and investment opportunities. Unlike fundamental analysis, which attempts to evaluate a security's value based on financial information such as sales and earnings, technical analysis focuses on price and volume to draw conclusions about future price movements. Key Takeaways Technical analysis is used to evaluate price trends and patterns and thereby identify potential investments and trading opportunities. Technical analysts believe past trading activity and a security's price changes can be valuable indicators of the security's future price movements. Technical analysis may be contrasted with fundamental analysis, which focuses on a company's financials rather than historical price patterns or stock trends. Technical analysis was introduced by Charles Dow. Investopedia / Candra Huff Understanding Technical Analysis Technical analysis is used to scrutinize the ways supply and demand for a security affect changes in price, volume, and implied volatility. It assumes that past trading activity and price changes of a security can be valuable indicators of the security's future price movements when paired with appropriate investing or trading rules. Technical analysis' various charting tools are often used to generate short-term trading signals. They can also help improve the evaluation of a security's strength or weakness relative to the broader market or one of its sectors. This information helps analysts improve their overall valuation estimate. Technical analysis as we know it today was first introduced by Charles Dow as the Dow Theory in the late 1800s. Several noteworthy researchers including William P. Hamilton, Robert Rhea, Edson Gould, and John Magee further contributed to Dow Theory concepts. Nowadays, technical analysis has evolved to include hundreds of patterns and signals developed through years of research. How Technical Analysis Is Used Professional analysts often use technical analysis in conjunction with other forms of research. Retail traders may make decisions based solely on the price charts of a security and similar statistics. But practicing equity analysts rarely limit their research to fundamental or technical analysis alone. Technical analysis can be applied to any security with historical trading data. This includes stocks, futures , commodities , fixed-income securities, currencies, and more. In fact, technical analysis is prevalent in commodities and forex markets where traders focus on short-term price movements. Technical analysis attempts to forecast the price movement of virtually any tradable instrument that is generally subject to forces of supply and demand. Some view technical analysis as simply the supply and demand forces reflected by the market price movements of a security. Technical analysis most commonly applies to price changes, but some analysts track numbers other than just price, such as trading volume or open interest figures. Technical Analysis Indicators Hundreds of patterns and signals have been developed by researchers to support technical analysis trading. Technical analysts have also developed numerous types of trading systems to help them forecast and trade on price movements. Some indicators focus primarily on identifying the current market trend, including support and resistance areas. Others focus on determining the strength of a trend and the likelihood of its continuation. Commonly used technical indicators and charting patterns include trendlines, channels, moving averages, and momentum indicators. In general, technical analysts look at the following broad types of indicators: Fast Fact The CMT Association supports the largest collection of chartered or certified analysts using technical analysis professionally around the world. The association's Chartered Market Technician (CMT) designation can be obtained after three levels of exams that cover both a broad and deep look at technical analysis tools. Underlying Assumptions of Technical Analysis Technical analysis attempts to decipher the market sentiment behind price trends by looking for price patterns and trends. Charles Dow released a series of editorials discussing technical analysis theory. He had two basic assumptions that continue to form the framework for technical analysis trading. Markets are efficient with values that represent factors that influence a security's price. Even random market price movements appear to move in identifiable patterns and trends that tend to repeat over time. Today the field of technical analysis builds on Dow's work. Professional analysts typically accept three general assumptions: The market discounts everything: Technical analysts believe that everything from a company's fundamentals to broad market factors to market psychology is already priced into a stock. The Efficient Markets Hypothesis (EMH) draws a similar conclusion about prices. The only thing remaining is the analysis of price movements, which technical analysts view as the product of supply and demand for a particular stock. Price moves in trends: Technical analysts expect that prices, even in random market movements, will exhibit trends regardless of the time frame being observed. In other words, a stock price is more likely to continue a past trend than to move erratically. Most"
  },
  {
    "type": "doc",
    "url": "https://www.investopedia.com/terms/t/technicalanalysis.asp",
    "title": "Technical Analysis: What It Is and How to Use It in Investing",
    "chunk": 1,
    "text": "technical trading strategies are based on this assumption. History tends to repeat itself: The repetitive nature of price movements is often attributed to market psychology, which tends to be very predictable and based on emotions such as fear and excitement. Technical analysis uses chart patterns to analyze these emotions and subsequent price movements to understand trends. While many forms of technical analysis have been used for more than 100 years, they are believed to be relevant still because they illustrate patterns in price movements that often repeat themselves. Fundamental Analysis vs. Technical Analysis Fundamental analysis and technical analysis, the major schools of thought when it comes to approaching the markets, are at opposite ends of the spectrum. Both methods are used to research and forecast future trends in stock prices , and like any investment strategy or philosophy, both have their advocates and adversaries. Fundamental Analysis Fundamental analysis is a method of evaluating securities by attempting to measure the intrinsic value of a stock. Fundamental analysts study everything from the overall economy and industry conditions to the financial condition and management of companies. Earnings , expenses , assets, and liabilities are all important characteristics of fundamental analysis that help analysts determine the fair value of a business. Technical Analysis Technical analysis differs from fundamental analysis in that the stock's price and volume are the only inputs. The core assumption is that all publicly known fundamentals have factored into price; thus, there is no need to pay close attention to them. Technical analysts do not attempt to measure a security's intrinsic value, but instead, use stock charts to identify patterns and trends that suggest how a stock's price will move in the future. Limitations of Technical Analysis 1. For some analysts and academic researchers, the EMH demonstrates why no actionable information is contained in historical price and volume data. However, by the same reasoning, nor should business fundamentals provide actionable information. These points of view are known as the weak form and semi-strong form of the EMH. 2. Another criticism of technical analysis is that history does not repeat itself exactly, so price pattern study is of dubious importance and can be ignored. Prices seem to be better modeled as a random walk. 3. A third criticism of technical analysis is that it works in some cases but only because it constitutes a self-fulfilling prophecy. For example, many technical traders will place a stop-loss order below the 200-day moving average of a certain company. If a large number of traders have done so and the stock reaches this price, there will be a large number of sell orders, which will push the stock price down, confirming the movement traders anticipated. Then, other traders will see the price decrease and sell their positions, reinforcing the strength of the trend. This short-term selling pressure can be considered self-fulfilling, but it will have little bearing on where the asset's price will be weeks or months from now. In sum, if enough people use the same signals, they could cause the movement foretold by the signal. However, over the long run, this sole group of traders cannot drive the price. What Assumptions Do Technical Analysts Make? Professional technical analysts typically assume three things. First, the market discounts everything. Second, prices, even in random market movements, will exhibit trends regardless of the time frame being observed. Third, history tends to repeat itself. The repetitive nature of price movements is often attributed to market psychology, which tends to be very predictable. What's the Difference Between Fundamental and Technical Analysis? Fundamental analysis is a method of evaluating securities by attempting to measure the intrinsic value of a stock. The core assumption of technical analysis, on the other hand, is that all known fundamentals are factored into price; thus, there is no need to pay close attention to them. Technical analysts do not attempt to measure a security's intrinsic value, but instead, use stock charts to identify patterns and trends that might suggest how the security's price will move in the future. How Can I Learn Technical Analysis? Your first step is to learn about investing, stocks, markets, and financials. This can be done through books, online courses and materials, and in-person classes. Once you understand the basics, you can start studying technical analysis . The Bottom Line Technical analysis is a longstanding method of analyzing the price and volume data of securities to determine future price action. This data usually appears on charts. Investors and professional traders apply a variety of technical indicators to these price and volume charts to draw conclusions and make decisions about entry and exit points for trades."
  },
  {
    "type": "doc",
    "url": "https://www.tradingview.com/support/solutions/43000521824-technical-analysis-overview/",
    "title": "Pivot Points Standard",
    "chunk": 0,
    "text": "VIDEO The Pivot Points Standard technical indicator displays levels at which price might meet support or resistance. The Pivot Points indicator defines a single pivot point (P) level and several support (S) and resistance (R) levels. Inputs Type Specifies the type of pivot point calculation. Each pivot point type uses a different calculation logic (described in the Calculation section below) and draws its own number of levels. Pivots Timeframe The timeframe used for the pivots calculation. All available timeframes are daily or higher. For example, if \"Weekly\" is chosen, the indicator uses the current week's open and the previous week's open, high, low, and close (OHLC) to calculate the pivot point levels, and as a result, the levels change once a week. The “Auto” setting selects the following pivot timeframes: 1D for chart resolutions up to and including 15 min 1W for chart resolutions greater than 15 min and less than 1 day 1M for chart resolutions from 1 day and above Number of Pivots Back Specifies how many sets of pivots the indicator displays. For example, with “Pivot Timeframe” set to “Weekly”, the default “Number of Pivots Back” value of 15 means that the indicator displays pivots for each of the last 15 weeks. Note that the indicator can draw no more than 500 lines regardless of the value of this input. Use Daily-based Values Timeframes higher than 1D are built using daily data (for example, the weekly high is the highest daily high that week), and are called “daily-based” for this reason. By default, this setting is turned on, and the indicator requests data directly from the higher timeframe defined by the “Pivots Timeframe” setting (which is always daily or higher). With this setting off, the indicator constructs higher timeframe data from the chart data. Therefore, if the chart timeframe is 1D or above, this setting does not affect the pivot points calculation. However, intraday data on many symbols is a separate data feed from higher timeframe data with different OHLC values, and this difference can affect the pivot calculations. Turning this setting off on intraday charts can help to mitigate issues in some cases. Stocks can have different values for these reasons: Intraday and daily data sets often include different trades, resulting in different prices. Extended hours data, if provided, is usually not included in the daily candle. As a result, the Pivot Points calculated on intraday extended hours and those calculated on a daily-based chart are very different. Futures can have different values for these reasons: The daily close usually represents the settlement price, which is an average value and not specifically the price of the last trade (unless the \"Use settlement as close\" option is explicitly turned off). On intraday timeframes, the last price of the session is the price of the last trade. The daily candle uses Electronic Trading Hours, while the intraday charts might allow switching between different sessions. An advantage of having the “Use Daily-based Values” option turned on is that the pivot values remain consistent between intraday and higher timeframes. However, in some cases you might want to turn it off: To include Extended Trading Hours data for a stock such as AAPL, turn the setting off — the extended hours data exists only on intraday timeframes, and the daily-based calculation does not include it. To include Regular Trading Hours data on ES1! futures, turn the setting off — the daily data is always based on the Extended Trading Hours session. To calculate the Pivot Points on AAPL or ES1! and ensure that the pivots displayed on your 1 hour chart and the pivots on the 1 day chart have the same values, turn this option on. Otherwise, the pivots are calculated on the 1 hour dataset, which differs from the 1 day dataset, and the results are also different. Note: For all symbols, the number of bars of intraday data available on the chart is limited by your subscription, while the daily data is always presented in full regardless of your plan. If you use a small timeframe on the chart (for example, 1 minute) and a large timeframe as the Pivot Timeframe (for example, Yearly), your chart only has a few months of 1-minute data — not enough to match the higher timeframe. In this case, the indicator returns an error, prompting you to switch this setting on to ensure that the indicator has enough data to calculate pivots. Show Labels/Prices Specify whether to show the labels (\"P\", \"R1\", \"S1\", etc.) and their associated prices beside the pivot lines. Labels Position Specifies whether to display the labels and prices to the left or the right of the pivot line. Line Width Specifies the width of the pivot lines. P, S1, R1, etc. Specify whether to draw particular support and"
  },
  {
    "type": "doc",
    "url": "https://www.tradingview.com/support/solutions/43000521824-technical-analysis-overview/",
    "title": "Pivot Points Standard",
    "chunk": 1,
    "text": "resistance lines, and set their color. Note that not all pivot types include the full set of lines: for example, only Traditional and Camarilla pivots calculate S5 and R5 lines; all other pivot types do not display them even if the P5 and S5 inputs are checked. Calculation The pivot, resistance, and support values are calculated in different ways depending on the pivot point type set in the “Type” input. Types The Standard Pivot Points indicator uses the following types of pivot: Traditional Fibonacci Woodie Classic DM Camarilla The calculation formulas for each type are given below. The \"current\" and \"previous\" values refer to the current and previous bar of the timeframe selected in the “Pivots Timeframe” option. Traditional P = (prevHigh + prevLow + prevClose) / 3 R1 = P * 2 - prevLow S1 = P * 2 - prevHigh R2 = P + (prevHigh - prevLow) S2 = P - (prevHigh - prevLow) R3 = P * 2 + (prevHigh - 2 * prevLow) S3 = P * 2 - (2 * prevHigh - prevLow) R4 = P * 3 + (prevHigh - 3 * prevLow) S4 = P * 3 - (3 * prevHigh - prevLow) R5 = P * 4 + (prevHigh - 4 * prevLow) S5 = P * 4 - (4 * prevHigh - prevLow) Fibonacci P = (prevHigh + prevLow + prevClose) / 3 R1 = P + 0.382 * (prevHigh - prevLow) S1 = P - 0.382 * (prevHigh - prevLow) R2 = P + 0.618 * (prevHigh - prevLow) S2 = P - 0.618 * (prevHigh - prevLow) R3 = P + (prevHigh - prevLow) S3 = P - (prevHigh - prevLow) Woodie P = (prevHigh + prevLow + 2 * currOpen) / 4 R1 = 2 * P - prevLow S1 = 2 * P - prevHigh R2 = P + (prevHigh - prevLow) S2 = P - (prevHigh - prevLow) R3 = prevHigh + 2 * (P - prevLow) S3 = prevLow - 2 * (prevHigh - P) R4 = R3 + (prevHigh - prevLow) S4 = S3 - (prevHigh - prevLow) Classic P = (prevHigh + prevLow + prevClose) / 3 R1 = 2 * P - prevLow S1 = 2 * P - prevHigh R2 = P + (prevHigh - prevLow) S2 = P - (prevHigh - prevLow) R3 = P + 2 * (prevHigh - prevLow) S3 = P - 2 * (prevHigh - prevLow) R4 = P + 3 * (prevHigh - prevLow) S4 = P - 3 * (prevHigh - prevLow) DM if prevOpen == prevClose X = prevHigh + prevLow + 2 * prevClose else if prevClose > prevOpen X = 2 * prevHigh + prevLow + prevClose else X = 2 * prevLow + prevHigh + prevClose P = X / 4 R1 = X / 2 - prevLow S1 = X / 2 - prevHigh Camarilla P = (prevHigh + prevLow + prevClose) / 3 R1 = prevClose + 1.1 * (prevHigh - prevLow) / 12 S1 = prevClose - 1.1 * (prevHigh - prevLow) / 12 R2 = prevClose + 1.1 * (prevHigh - prevLow) / 6 S2 = prevClose - 1.1 * (prevHigh - prevLow) / 6 R3 = prevClose + 1.1 * (prevHigh - prevLow) / 4 S3 = prevClose - 1.1 * (prevHigh - prevLow) / 4 R4 = prevClose + 1.1 * (prevHigh - prevLow) / 2 S4 = prevClose - 1.1 * (prevHigh - prevLow) / 2 R5 = (prevHigh / prevLow) * prevClose S5 = prevClose - (R5 - prevClose)"
  },
  {
    "type": "doc",
    "url": "https://www.babypips.com/learn/forex/technical-analysis",
    "title": "What is Technical Analysis?",
    "chunk": 0,
    "text": "Technical analysis is the framework in which traders study price movement. The theory is that a person can look at historical price movements and determine the current trading conditions and potential price movement. Someone who uses technical analysis is called a technical analyst . Traders who use technical analysis are known as technical traders . The main evidence for using technical analysis is that, theoretically, all current market information is reflected in the price . Technical traders generally ascribe to the belief that “ It’s all in the charts! ” This simply means that all known fundamental information is priced into the current market price. If price reflects all the information that is out there, then price action is all one would really need to make a trade. Technical analysis looks at the rhythm, flow, and trends in price action. Now, have you ever heard the old adage, “ History tends to repeat itself “? Well, that’s basically what technical analysis is all about! If a certain price held as a major support or resistance level in the past, forex traders will keep an eye out for it and base their trades around that historical price level. Technical analysts look for similar patterns that have formed in the past and will form trade ideas, believing that price could possibly act the same way that it did before. Technical analysis is NOT so much about prediction as it is about POSSIBILITY. Technical analysis is the study of historical price action in order to identify patterns and determine the possibilities of the future direction of price . Technical analysis is the study of historical price action. So, how the heck does one “ study historical price action “? In the world of trading, when someone says “technical analysis”, the first thing that comes to mind is a chart . Technical analysts use charts because they are the easiest way to visualize historical data ! Technical analysts live, eat, and breathe charts, which is why they are often called chartists . Chartists believe that price action is the most reliable indicator of future price action. The Philosophy Behind Technical Analysis Technical analysis is rooted in several key philosophical principles: Market Efficiency: Prices reflect all publicly available information, making fundamental analysis less relevant. Crowd Psychology: Market participants’ emotions and behaviors create patterns and trends. History Repeats Itself: Human nature and market dynamics lead to similar patterns and cycles. Fractals: Markets exhibit fractal properties, with small patterns resembling larger ones. Market patterns repeat at different scales and timeframes. Probabilistic Thinking: Technical analysis deals with probabilities, not certainties. Adaptive Markets: Markets evolve, and technical analysis adapts to changing conditions You can look at past data to help you spot trends and patterns that could help you find some great trading opportunities. What’s more, with all the traders who rely on technical analysis, these price patterns and indicator signals tend to become self-fulfilling. As more and more forex traders look for certain price levels and chart patterns, the more likely that these patterns will manifest themselves in the markets. You should know, though, that technical analysis is VERY subjective. Just because Michelangelo, Donatello, Leonardo, and Raphael are looking at the exact same chart setup or indicators doesn’t mean that they will come up with the same idea of where price may be headed. The important thing is that you understand the concepts under technical analysis so you won’t get nosebleeds whenever somebody starts talking about Fibonacci, Bollinger Bands , or pivot points. Technical analysis does predict the future . It is the study of historical price action, which reflects the psychology of market participants. By examining past behavior, we can identify statistical patterns in how traders and investors have acted. These patterns help form educated guesses about how they might behave in the near term, assuming similar conditions apply. Now we know you’re thinking to yourself, “Geez, these guys are smart. They use crazy words like ‘Fibonacci’ and ‘Bollinger’. I can never learn this stuff!” Don’t worry yourself too much. After you’re done with the School of Pipsology, you too will be just as… uhmmm… “smart” as us."
  },
  {
    "type": "doc",
    "url": "https://etherscan.io/",
    "title": "Ethereum (ETH) Blockchain Explorer",
    "chunk": 0,
    "text": "Please DO NOT store any passwords or private keys here. A private note (up to 100 characters) can be saved and is useful for transaction tracking."
  },
  {
    "type": "doc",
    "url": "https://polygonscan.com/",
    "title": "Polygon PoS Chain (POL) Blockchain Explorer",
    "chunk": 0,
    "text": "Please DO NOT store any passwords or private keys here. A private note (up to 100 characters) can be saved and is useful for transaction tracking."
  },
  {
    "type": "doc",
    "url": "https://bscscan.com/",
    "title": "BNB Smart Chain (BNB) Blockchain Explorer",
    "chunk": 0,
    "text": "Please DO NOT store any passwords or private keys here. A private note (up to 100 characters) can be saved and is useful for transaction tracking."
  },
  {
    "type": "doc",
    "url": "https://messari.io/",
    "title": "Crypto Research, Reports, AI News, Live Prices, Token Unlocks, and Fundraising Data",
    "chunk": 0,
    "text": "• Bitcoin surpassed $117,000 amid optimism linked to Trump appointing Stephen Miran to the Federal Reserve and speculation about allowing crypto in 401(k) plans, driving gains across major cryptocurrencies and boosting overall market sentiment . • Roman Storm, a Tornado Cash developer, was found guilty of operating an unlicens ... This daily recap will be updated throughout the day"
  },
  {
    "type": "doc",
    "url": "https://insights.glassnode.com/",
    "title": "Glassnode Insights - On-Chain Market Intelligence",
    "chunk": 0,
    "text": "Subscribe to our newsletter Subscribe to receive our latest analysis, updates, and best-in-class research on Bitcoin, Ethereum, DeFi, and more. By subscribing, you agree to our Terms and Conditions and Privacy Policies ."
  },
  {
    "type": "doc",
    "url": "https://coinglass.com/",
    "title": "Cryptocurrency Derivatives Data Analysis,Bitcoin Open interest,Bitcoin Options",
    "chunk": 0,
    "text": "Derivatives Spot Categories Memes ETH SOL BSC Layer-1 AI AI Agents Gaming DOT Made in USA RWA DePIN DeFAI"
  },
  {
    "type": "doc",
    "url": "https://research.kaiko.com/",
    "title": "The crypto industry’s leading data-driven research.",
    "chunk": 0,
    "text": "The crypto industry’s leading data-driven research . Read by thousands of industry professionals, powered by Kaiko data."
  },
  {
    "type": "doc",
    "url": "https://www.coinmetrics.io/library/",
    "title": "Coin Metrics",
    "chunk": 0,
    "text": "It seems we can't find what you're looking for."
  },
  {
    "type": "doc",
    "url": "https://token.unlocks.app/",
    "title": "Track The Most Updated Data and Complete Vesting Schedules",
    "chunk": 0,
    "text": "$4.70 $1.41 $23.79 $0.136 $0.014 $0.079 $0.322 $0.466 $0.058 $0.635 $4.58 $0.002836 $0.206 $2.21 $1.23 $0.086 $0.017 $0.171 $0.717 $0.528 $0.789 $3.87 $0.656 $0.810 $0.596 $3.94 $0.140 $2.53 $0.280 $0.037 $0.116 $0.837 $45.24 $9.26 $1.03 $4.45 $0.089 $2.08 $45.25 $0.089 $0.127 $0.003116 $1.40 $182.40 $0.234 $0.800 $21.93 $9.11 $0.000014 $10.99"
  },
  {
    "type": "doc",
    "url": "https://santiment.net/blog/",
    "title": "404: Not found",
    "chunk": 0,
    "text": "Not found You just hit a route that doesn't exist... the sadness."
  },
  {
    "type": "doc",
    "url": "https://bankless.com/",
    "title": "The Ultimate Guide To Crypto",
    "chunk": 0,
    "text": "Not financial or tax advice. Bankless content is strictly educational and is not investment advice or a solicitation to buy or sell any assets or to make any financial decisions. This newsletter is not tax advice. Talk to your accountant. Do your own research. Disclosure. From time-to-time we may add links in this newsletter to products we use. We may receive commission if you make a purchase through one of these links. Additionally, the Bankless team hold crypto assets. See our investment disclosures here . This site is protected by reCAPTCHA. Read Bankless in: English - Spanish - German"
  },
  {
    "type": "doc",
    "url": "https://thedefiant.io/",
    "title": "The Defiant",
    "chunk": 0,
    "text": "The SEC’s decision to end its legal battle with Ripple Labs closes a tumultuous chapter but leaves regulatory uncertainty looming."
  },
  {
    "type": "doc",
    "url": "https://www.okx.com/learn",
    "title": "Learn about Bitcoin, Crypto, Trading, NFTs & Web3",
    "chunk": 0,
    "text": "Decrypt crypto Grow your crypto and blockchain knowledge, from market analysis to the latest tokens and protocols."
  },
  {
    "type": "doc",
    "url": "https://blog.bitmex.com/research/",
    "title": "Research Archives | BitMEX Blog",
    "chunk": 0,
    "text": "21 Jul 2025 12 Jul 2025 19 May 2025 7 May 2025 24 Apr 2025 5 Apr 2025 4 Apr 2025 31 Mar 2025 24 Mar 2025 24 Feb 2025 7 Jan 2025 2 Jan 2025 30 Dec 2024 24 Oct 2024 19 Oct 2024 14 Aug 2024 25 Jul 2024 16 Jul 2024 3 Jul 2024 29 Apr 2024 26 Apr 2024 2 Apr 2024 6 Mar 2024 26 Feb 2024 19 Feb 2024 6 Feb 2024 30 Dec 2023 26 Dec 2023 25 Dec 2023 4 Dec 2023 7 Nov 2023 2 Nov 2023 31 Oct 2023 31 Oct 2023 30 Oct 2023 25 Sep 2023 21 Sep 2023 7 Aug 2023 18 Jul 2023 2 Jun 2023 31 May 2023 30 May 2023 25 May 2023 22 May 2023 17 May 2023 9 May 2023 8 Apr 2023 2 Apr 2023 27 Mar 2023 25 Mar 2023 18 Mar 2023 12 Mar 2023 4 Mar 2023 26 Feb 2023 19 Feb 2023 11 Feb 2023 8 Feb 2023 5 Feb 2023 29 Jan 2023 22 Jan 2023 15 Jan 2023 8 Jan 2023 2 Jan 2023 26 Dec 2022 18 Dec 2022 12 Dec 2022 10 Dec 2022 4 Dec 2022 28 Nov 2022 21 Nov 2022 14 Nov 2022 14 Nov 2022 7 Nov 2022 7 Nov 2022 31 Oct 2022 21 Oct 2022 18 Oct 2022 21 Sep 2022 13 Sep 2022 19 Aug 2022 1 Aug 2022 12 Jul 2022 28 Jun 2022 23 Jun 2022 13 Jun 2022 6 Jun 2022 30 May 2022 26 May 2022 23 May 2022 16 May 2022 13 May 2022 12 May 2022 12 May 2022 6 May 2022 22 Apr 2022 11 Apr 2022 30 Mar 2022 21 Mar 2022 21 Feb 2022 14 Feb 2022 11 Feb 2022 3 Feb 2022 28 Dec 2021 2 Dec 2021 22 Nov 2021 18 Nov 2021 3 Nov 2021 2 Nov 2021 11 Oct 2021 13 Sep 2021 3 Sep 2021 1 Sep 2021 13 Aug 2021 12 Aug 2021 8 Aug 2021 31 Jul 2021 24 Jul 2021 18 Jul 2021 10 Jul 2021 4 Jul 2021 28 Jun 2021 21 Jun 2021 14 Jun 2021 9 Jun 2021 7 Jun 2021 31 May 2021 24 May 2021 20 May 2021 19 May 2021 17 May 2021 11 May 2021 10 May 2021 3 May 2021 26 Apr 2021 19 Apr 2021 12 Apr 2021 5 Apr 2021 29 Mar 2021 22 Mar 2021 15 Mar 2021 8 Mar 2021 9 Feb 2021 1 Feb 2021 27 Jan 2021 29 Dec 2020 9 Dec 2020 26 Nov 2020 3 Nov 2020 15 Oct 2020 13 Oct 2020 22 Sep 2020 14 Sep 2020 7 Sep 2020 18 Aug 2020 17 Aug 2020 17 Aug 2020 28 Jul 2020 18 Jul 2020 6 Jul 2020 16 Jun 2020 5 Jun 2020 28 May 2020 22 May 2020 7 May 2020 4 May 2020 12 Apr 2020 3 Apr 2020 2 Apr 2020 28 Mar 2020 18 Mar 2020 17 Mar 2020 25 Feb 2020 28 Jan 2020 27 Jan 2020 13 Jan 2020 8 Jan 2020 23 Dec 2019 18 Dec 2019 15 Dec 2019 29 Nov 2019 21 Nov 2019 20 Nov 2019 31 Oct 2019 30 Oct 2019 17 Oct 2019 3 Sep 2019 1 Aug 2019 15 Jul 2019 28 Jun 2019 28 May 2019 24 May 2019 13 May 2019 8 May 2019 21 Apr 2019 27 Mar 2019 13 Mar 2019 12 Feb 2019 11 Feb 2019 23 Jan 2019 16 Jan 2019 4 Jan 2019 3 Jan 2019 10 Dec 2018 21 Nov 2018 16 Nov 2018 6 Nov 2018 5 Nov 2018 15 Oct 2018 14 Oct 2018 2 Oct 2018 1 Oct 2018 26 Sep 2018 30 Aug 2018 20 Aug 2018 19 Jul 2018 10 Jul 2018 2 Jul 2018 31 May 2018 21 May 2018 24 Apr 2018 11 Apr 2018 28 Mar 2018 22 Mar 2018 19 Mar 2018 1 Mar 2018 18 Feb 2018 14 Feb 2018 6 Feb 2018 1 Feb 2018 25 Jan 2018 17 Jan 2018 28 Dec 2017 21 Dec 2017 13 Dec 2017 1 Dec 2017 29 Nov 2017 22 Nov 2017 16 Nov 2017 16 Nov 2017 12 Nov 2017 1 Nov 2017 27 Oct 2017 24 Oct 2017 20 Oct 2017 20 Oct 2017 13 Oct 2017 13 Oct 2017 29 Sep 2017 29 Sep 2017 22 Sep 2017 22 Sep 2017 15 Sep 2017 15 Sep 2017 8 Sep 2017 8 Sep 2017 7 Sep 2017"
  },
  {
    "type": "doc",
    "url": "https://www.delphidigital.io/research",
    "title": "Crypto Research For Investors and Builders",
    "chunk": 0,
    "text": "This report explores the transformative shifts in crypto secondary markets, driven by regulatory advancements like legislative progress, spot ETF approvals and the resurgence of crypto IPOs. This drivers are unlocking institutional capital and blending crypto with traditional finance. Key trends include the dominance of SAFT deals alongside a rise in equity rounds, with deal volumes surging 73% year-over-year in H1 2025 amid seasonal growth patterns. Pricing dynamics reveal a prevalence of down-rounds and discounts influenced by macro factors, token supply overhangs, and the emergence of new buyers. The report highlights emerging buyer cohorts, sector winners such as L1 protocols, the catalytic impact of Circle's listing on assets like Kraken, and the accelerating tokenization of real-world assets (RWAs). Looking ahead, the report predicts surges in pre-IPO equity rounds, discount compression, and broader institutional adoption, signaling a maturing market ripe for innovation and investment."
  },
  {
    "type": "doc",
    "url": "https://www.gsr.io/insights/",
    "title": "Crypto Research & Market Analysis",
    "chunk": 0,
    "text": "Home Insights Market-Leading Research Stay ahead in crypto with expert market analysis, timely industry insights, and in-depth research from GSR."
  },
  {
    "type": "doc",
    "url": "https://a16z.com/crypto/",
    "title": "a16z crypto",
    "chunk": 0,
    "text": "we back bold entrepreneurs building the next internet a16z crypto is a venture capital fund that invests in crypto and web3 startups"
  },
  {
    "type": "doc",
    "url": "https://panteracapital.com/blog/",
    "title": "Blog - 2022 In Review - Platform Team",
    "chunk": 0,
    "text": "The responsibility we have to our founders doesn’t end when the investment is announced — it continues, through the good times and the bad. Sometimes founders need help managing publicity for a major announcement; at other times introductions to other industry players. Not to mention the never-ending security audits. These aspects of a business aren’t sexy but they are necessary for builders to continue building. The Pantera Platform team is always looking ways we can add value, so that we can better serve our portfolio companies and continue building the future of decentralized finance. A lot of what we do is behind the scenes — as it’s meant to be. But we are excited to share some of our favorite things we were a part of in 2022. Expanding The Platform Team This year, we more than doubled our Platform team by adding four new members to the organization in areas we feel will benefit our portfolio companies. These new hires represent the best in talent, community and event management, marketing, communications, and legal. Nick Zurick is Head of Talent – @NickZurick1 – Follow Nick on LinkedIn Nick supports our portfolio companies by providing talent strategy consultations, talent process audits, and executive search. Previously, Nick was the first leadership recruiter at Robinhood and led international recruiting. John Munson is Head of Community – @JohnWMunson – Follow John on LinkedIn John’s role is to build a vibrant community for the 215+ portfolio companies that make up the Pantera family. Through a combination of in-person, virtual, and remote experiences he aims to facilitate and support relationships amongst these amazing founders and teams. Andrew Harris is Senior Platform Associate – @Andrayday – Follow Andrew on LinkedIn Andrew provides legal support in everything from business development, strategy, strategic partnerships growth, M&A advisory, governance, structuring, general legal and regulatory matters, and more. Tony Sakich is Director of Marketing & Communications – @Tony_Swish – Follow Tony on LinkedIn Tony’s role at Pantera is to work with founders on funding announcements, marketing strategies, design, branding and PR. Previously, Tony was a founding member of Augur with Pantera Co-CIO Joey Krug and managed many pioneering crypto marketing campaigns and industry firsts, including the first nationally televised Bitcoin commercials. Pantera Bazaar In 2022, we launched the Pantera Bazaar, the crypto-native portal for our portfolio founders to search, compare, and acquire the tools/partners they need to be successful, such as: So far over $1.2 million in discounts from 100+ companies have been redeemed on the Pantera Bazaar. The Pantera House A home away from home for Pantera founders VIDEO Midway through an especially intense year of travel, it occurred to us that there was one challenge all our founders faced, surviving the conference circuit. What if we created a place that offered everything to both work and relax, apart from (but near) the hubbub of a conference? Our Head of Community John Munson envisioned The Pantera House as a hub for founders and execs from our portfolio to relax, work, eat, drink, and socialize in a comfortable location away from the congestion and chaos of a large crypto conference. Our first two versions of Pantera House in Lisbon, Portugal and Bogota, Colombia were successfully packed with founders and Pantera team members throughout the day. The vibe was intimate, the conversation and drinks flowed, old friends caught up, and new friends were made. Watch for more Pantera House events throughout 2023 around the world! Learn more about Pantera House here . Pantera Blockchain Summits The Pantera Capital Blockchain Summit is an annual conference that brings together investors, partners, and portfolio company founders for a day of networking, learning, and inspiration. This year, the Summit was held in two locations: San Francisco and Singapore. The San Francisco Summit featured a range of panels and keynotes from 30+ industry experts and thought leaders. Over 500 attendees had the opportunity to network with other investors and founders and learn about the latest trends and developments in the blockchain industry. After a full day of panels, we wrapped up the Summit with whiskey tasting and our Pantera poker tournament! Videos of all San Francisco Summits speakers and panels can be found here . VIDEO The Singapore Summit was our first Summit held outside of the United States and focused on the growing blockchain ecosystem in Asia. We presented speakers from Amber Group , Coinbase , Temasek , Bybit , Anchorage and more to an audience of over 275 institutional investors and partners. We also had an amazing custom NFT gallery that featured 6529 Fund , Origin Protocol , and MakersPlace ! Videos of all Singapore Summit speakers and panels can be found here . Talent In Q4 2022, we: Testimonial from Rift Finance : “ Any early stage company should do a Talent Audit like"
  },
  {
    "type": "doc",
    "url": "https://panteracapital.com/blog/",
    "title": "Blog - 2022 In Review - Platform Team",
    "chunk": 1,
    "text": "this to help spot the gaps in their talent processes. I’m a second-time founder and I found it super helpful. There were gaps in our candidate experience, pipeline management, and talent evaluation. Nick helped us set up the behind-the-scenes infrastructure to level up on all of these dimensions.” “As CEO, one of my top priorities is getting the best talent on our team to help us achieve our mission. It’s one of the most difficult things when you’re growing a company. Nick’s Talent Audit and hands-on setup is helping our team hire faster, close more candidates at the finish line, and mitigate bad hires.” ** Testimonial from Stride Labs : “Nick’s talent audit kicked our recruiting process into high gear. After reviewing all our docs and processes on his own time, he spent 4 hours 1:1 with our founding team. He revamped our hiring funnels, JDs, ATS system, candidate pitch, technical interviews SLAs, offer process, interview evaluation rubrics, and much more. On top of that, rather than reinventing the wheel he shared the best templates he created based on what has worked for other Web3 portfolio companies – basically all the nitty gritty docs and frameworks you need to land top talent.” ** We’ve defined our function. And going into 2023, we are planning to focus our efforts on three core offerings: Talent Strategy – we will help our portfolio companies structure your search, the process, put them in touch with the right partner, and help them manage the search & any applicable 3rd parties. Example: helping portfolio companies negotiate and manage a contingent/retained search for the first time with an external vendor, talent mapping, compensation questions, ad-hoc questions, etc. Reach out and let us know if you would like a Talent Audit or help in any of these areas! Partnerships Throughout the past year, the Platform team has continued to negotiate with some of the industry’s top organizations on deals and partnerships that will benefit portfolio companies. This initiative is slated to grow greatly in 2023, as we develop high-level partnerships with the best organizations in the industry. Our most recent partnership with Quantstamp enables our portfolio companies to “skip the line” to access their services, most importantly security audits which often have a lengthy wait time. In addition to expedited audit services, companies can access audit readiness preparation and open office hours with Quantstamp professionals. In October, we partnered with CryptoCurrencyJobs.co , one of the leading job boards in the space to share content and new opportunities. In 2023, our partnerships will continue to expand and a variety of new offerings will be made available to our portfolio companies. **Certain statements in this blog post have been made by former or current executives of portfolio companies managed by Pantera or its affiliates (collectively, “Pantera”). While none of the executives quoted in this blog post were compensated for their participation in or statements made in the blog post, conflicts of interest may nevertheless exist in that Pantera, including funds managed by Pantera, and/or the executives of the portfolio companies could benefit from the success of the portfolio company. [In addition, some executives of portfolio companies managed by Pantera may also be investors in funds managed by Pantera and may receive certain economic benefits, including, but not limited to, receiving reduced fee arrangements, co-investment opportunities and/or other economic arrangements. None of these economic arrangements were provided in exchange for statements made in the blog post."
  },
  {
    "type": "doc",
    "url": "https://jumpcrypto.com/research/",
    "title": "Research | Jump Crypto",
    "chunk": 0,
    "text": "Discord and Harmony in Networks Andrea Galeotii Benjamin Golub Sanjeev Goyal Rithvik Rao Consider a coordination game played on a network, where agents prefer taking actions closer to those of their neighbors and to their own ideal points in action space. We explore what welfare outcomes depend on and what it all means."
  },
  {
    "type": "doc",
    "url": "https://blog.chainalysis.com/",
    "title": "Blog - Chainalysis",
    "chunk": 0,
    "text": "Subscribe to our weekly newsletter"
  },
  {
    "type": "doc",
    "url": "https://www.coincenter.org/",
    "title": "Coin Center",
    "chunk": 0,
    "text": "Coin Center is the leading non-profit focused on the policy issues facing cryptocurrencies. We engage in research, educate policymakers, and advocate for sensible regulatory approaches to this technology. These are our priorities ."
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 0,
    "text": "Aug 8, 2025 This week’s newsletter announces draft BIPs for Utreexo, summarizes continued discussion about lowering the minimum transaction relay feerate, and describes a proposal to allow nodes to share their block templates to mitigate problems with divergent mempool policies. Also included are our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure projects. We also include a correction to last week’s newsletter and a recommendation to readers. Aug 1, 2025 This week’s newsletter summarizes the results of a test of compact block relay prefilling and links to a mempool-based fee estimation library. Also included are our regular sections summarizing discussion about changing Bitcoin’s consensus rules, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Jul 25, 2025 This week’s newsletter summarizes a vulnerability affecting old versions of LND, describes an idea for improving privacy when using co-signer services, and examines the impact of switching to quantum-resistant signature algorithms on HD wallets, scriptless multisig, and silent payments. Also included are our regular sections summarizing popular questions and answers on the Bitcoin Stack Exchange, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Jul 18, 2025 This week’s newsletter includes our regular sections summarizing updates to services and client software, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Jul 11, 2025 This week’s newsletter briefly describes a new library allowing output script descriptors to be compressed for use in QR codes. Also included are our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Jul 4, 2025 This week’s newsletter describes a proposal to separate the network connections and peer management used for onion message relay from those used for HTLC relay in LN. Also included are our regular sections summarizing discussion about changing Bitcoin’s consensus and listing recent changes to popular Bitcoin infrastructure software. Jun 27, 2025 This week’s newsletter summarizes research about fingerprinting full nodes using P2P protocol messages and seeks feedback about possibly removing support for H in BIP32 paths in the BIP380 specification of descriptors. Also included are our regular sections summarizing top questions and answers on the Bitcoin Stack Exchange, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Jun 20, 2025 This week’s newsletter describes a proposal to limit public participation in Bitcoin Core repositories, announces a significant improvements to BitVM-style contracts, and summarizes research into LN channel rebalancing. Also included are our regular sections summarizing recent changes to clients and services, announcing new releases and release candidates, and describing recent changes to popular Bitcoin infrastructure software. Jun 13, 2025 This week’s newsletter describes how the selfish mining danger threshold can be calculated, summarizes an idea about preventing filtering of high feerate transactions, seeks feedback about a proposed change to BIP390 musig() descriptors, and announces a new library for encrypting descriptors. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club, announcements of new releases and release candidates, and descriptions of recent changes to popular Bitcoin infrastructure projects. Jun 6, 2025 This week’s newsletter shares an analysis about syncing full nodes without old witnesses. Also included are our regular sections with descriptions of discussions about changing consensus, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. May 30, 2025 This week’s newsletter summarizes a discussion about the possible effects of attributable failures on LN privacy. Also included are our regular sections with selected questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and descriptions of recent changes to popular Bitcoin infrastructure software. May 23, 2025 This week’s newsletter includes our regular sections describing changes to services and client software, announcing new releases and release candidates, and summarizing recent changes to popular Bitcoin infrastructure software. May 16, 2025 This week’s newsletter describes a fixed vulnerability affecting old versions of Bitcoin Core. Also included are our regular sections summarizing recent discussions about changing Bitcoin’s consensus rules, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. May 9, 2025 This week’s newsletter describes a recently discovered theoretical consensus failure vulnerability and links to a proposal to avoid reuse of BIP32 wallet paths. Also included are our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcing new releases and release candidates, and describing notable code changes to popular Bitcoin infrastructure software. May 2, 2025 This week’s newsletter links to comparisons between different cluster linearization techniques and briefly summarizes discussion about increasing or removing Bitcoin Core’s"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 1,
    "text": "OP_RETURN size limit. Also included are our regular sections announcing new releases and release candidates and summarizing notable changes to popular Bitcoin infrastructure software. Apr 25, 2025 This week’s newsletter announces a new aggregate signature protocol compatible with secp256k1 and describes a standardized backup scheme for wallet descriptors. Also included are our regular sections summarizing recent Bitcoin Stack Exchange questions and answers, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Apr 18, 2025 This week’s newsletter includes our regular sections describing recent changes to services and client software, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Also included is a correction to some details from our story last week about SwiftSync. Apr 11, 2025 This week’s newsletter describes a proposal for speeding up Bitcoin Core initial block download, with a proof-of-concept implementation that shows a roughly 5x speed up compared to Bitcoin Core’s defaults. Also included are our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure projects. Apr 4, 2025 This week’s newsletter links to an educational implementation of elliptic curve cryptography for Bitcoin’s secp256k1 curve. Also included are our regular sections with descriptions of discussions about changing consensus, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Mar 28, 2025 This week’s newsletter describes a proposal to allow LN to support upfront and hold fees based on burnable outputs, summarizes discussion about testnets 3 and 4 (including a hard fork proposal), and announces a plan to begin relaying certain transactions containing taproot annexes. Also included are our regular sections summarizing selected questions and answers from the Bitcoin Stack Exchange, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure projects. Mar 21, 2025 This week’s newsletter summarizes a discussion about LND’s updated dynamic feerate adjustment system. Also included are our regular sections describing recent changes to services and client software, announcing new releases and release candidates, and summarizing recent merges to popular Bitcoin infrastructure software. Mar 14, 2025 This week’s newsletter looks at an analysis of P2P traffic experienced by a typical full node, summarizes research into LN pathfinding, and describes a new approach for creating probabilistic payments. Also included are our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure projects. Mar 7, 2025 This week’s newsletter announces the disclosure of a vulnerability affecting old versions of LND and summarizes a discussion about the Bitcoin Core Project’s priorities. Also included are our regular sections describing discussion related to consensus changes, announcing new releases and release candidates, and summarizing notable changes to popular Bitcoin infrastructure software. Feb 28, 2025 This week’s newsletter summarizes a post about having full nodes ignore transactions that are relayed without being requested first. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Feb 21, 2025 This week’s newsletter describes an idea for allowing mobile wallets to settle LN channels without extra UTXOs and summarizes continued discussion about adding a quality-of-service flag for LN pathfinding. Also included are our regular sections describing recent changes to clients, services, and popular Bitcoin infrastructure software. Feb 14, 2025 This week’s newsletter summarizes continued discussion about probabilistic payments, describes additional opinions about ephemeral anchor scripts for LN, relays statistics about evictions from the Bitcoin Core orphan pool, and announces an updated draft for a revised BIP process. Also included are our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Feb 7, 2025 This week’s newsletter announces a fixed vulnerability affecting LDK, summarizes discussion about zero-knowledge gossip for LN channel announcements, describes the discovery of previous research that can be applied to finding optimal cluster linearizations, provides an update on the development of the Erlay protocol for reducing transaction relay bandwidth, looks at tradeoffs between different scripts for implementing LN ephemeral anchors, relays a proposal for emulating an OP_RAND opcode in a privacy-preserving manner with no consensus changes required, and points to renewed discussion about lowering the minimum transaction feerate. Jan 31, 2025 This week’s newsletter describes a vulnerability affecting older versions of LDK, looks at a newly disclosed aspect of a vulnerability originally published in 2023, and summarizes renewed discussion about compact block reconstruction statistics. Also included are our regular sections summarizing popular questions on the Bitcoin Stack Exchange, announcing new releases and release candidates, and describing recent changes"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 2,
    "text": "to popular Bitcoin infrastructure software. Jan 24, 2025 This week’s newsletter announces a draft BIP for referencing unspendable keys in descriptors, examines how implementations are using PSBTv2, and corrects in depth our description last week of a new offchain DLC protocol. Also included are our regular sections describing changes to services and client software, announcing new releases and release candidates, and summarizing recent changes to popular Bitcoin infrastructure software. Jan 17, 2025 This week’s newsletter summarizes continued discussion about rewarding pool miners with tradeable ecash shares and describes a new proposal for enabling offchain resolution of DLCs. Also included are our regular sections announcing new releases and release candidates and describing notable changes to popular Bitcoin infrastructure software. Jan 10, 2025 This week’s newsletter describes a potential change to Bitcoin Core affecting miners, summarizes discussion about creating contract-level relative timelocks, and discusses a proposal for an LN-Symmetry variant with optional penalties. Also included are our regular sections announcing new releases and release candidates and summarizing notable changes to popular Bitcoin infrastructure software. Jan 3, 2025 This week’s newsletter links to information about longstanding deanonymization vulnerabilities in software using centralized coinjoin protocols and summarizes an update to a draft BIP about the ChillDKG distributed key generation protocol compatible with scriptless threshold signing. Also included are our regular sections summarizing discussion about changing Bitcoin’s consensus rules, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Dec 20, 2024 The seventh annual Bitcoin Optech Year-in-Review special summarizes notable developments in Bitcoin during all of 2024. Dec 13, 2024 This week’s newsletter describes a vulnerability that allowed stealing from old versions of various LN implementations, announces a deanonymization vulnerability affecting Wasabi and related software, summarizes a post and discussion about LN channel depletion, links to a poll for opinions about selected covenant proposals, describes two types of incentive-based pseudo-covenants, and references summaries of the periodic in-person Bitcoin Core developer meeting. Also included are our regular sections summarizing a Bitcoin Core PR Review Club meeting, listing changes to services and client software, linking to popular Bitcoin Stack Exchange questions and answers, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Dec 6, 2024 This week’s newsletter announces the disclosure of a transaction censorship vulnerability and summarizes discussion about the consensus cleanup soft fork proposal. Also included are our regular sections announcing new releases and release candidates and describing notable changes to popular Bitcoin infrastructure software. Nov 29, 2024 This week’s newsletter summarizes several recent discussions about a Lisp dialect for Bitcoin scripting and includes our regular sections with descriptions of popular questions and answers on the Bitcoin Stack Exchange, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure projects. Nov 22, 2024 This week’s newsletter summarizes a proposed change to the LN specification to allow pluggable channel factories, links to a report and a new website for examining transactions on the default signet that use proposed soft forks, describes an update to the LNHANCE multi-part soft fork proposal, and discusses a paper about covenants based on grinding rather than consensus changes. Also included are our regular sections summarizing recent changes to services, client software, and popular Bitcoin infrastructure software. Nov 15, 2024 This week’s newsletter summarizes a new offchain payment resolution protocol and links to papers about potential IP-layer tracking and censorship of LN payments. Also included are announcements of new releases and release candidates (including security critical updates for BTCPay Server) and descriptions of notable changes to popular Bitcoin infrastructure software. Nov 8, 2024 This week’s newsletter describes a vulnerability affecting old versions of Bitcoin Core and includes our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Nov 1, 2024 This week’s newsletter describes a proposal for timeout tree channel factories and summarizes a draft BIP for proofs of discrete log equivalence to be used when generating silent payments. Also included are our regular sections with announcements of new software releases and descriptions of notable changes made to popular Bitcoin infrastructure software. Oct 25, 2024 This week’s newsletter summarizes updates to a proposal for new LN channel announcements and describes a BIP for sending silent payments with PSBTs. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Oct 18, 2024 This week’s newsletter looks at summaries of some of the topics discussed at a recent LN developer meeting. Also include are our regular sections with descriptions of changes to popular clients and services, announcements of new releases and release candidates, and summaries of notable"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 3,
    "text": "changes to popular Bitcoin infrastructure software. Oct 11, 2024 This week’s newsletter announces three vulnerabilities affecting old versions of the Bitcoin Core full node, announces a separate vulnerability affecting old versions of the btcd full node, and links to a contributed Optech guide describing how to use multiple new P2P network features added in Bitcoin Core 28.0. Also included are our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Oct 4, 2024 This week’s newsletter announces a planned security disclosure and includes our regular sections describing new releases, release candidates, and notable changes to popular Bitcoin infrastructure software. Sep 27, 2024 This week’s newsletter announces a fixed vulnerability affecting older versions of Bitcoin Core, provides an update on hybrid channel jamming mitigation, summarizes a paper about more efficient and private client-side validation, and announces a proposal to update the BIP process. Also included are our regular sections summarizing top questions and answers from the Bitcoin Stack Exchange, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Sep 20, 2024 This week’s newsletter links to a proof-of-concept implementation for proving in zero-knowledge that an output is part of the UTXO set, describes one new and two previous proposals for allowing offline LN payments, and summarizes research about DNS seeding for non-IP network addresses. Also included are our regular sections describing changes to clients and services, announcing new releases and release candidates, and summarizing notable changes to popular Bitcoin infrastructure software. Sep 13, 2024 This week’s newsletter announces a new testing tool for Bitcoin Core and briefly describes a DLC-based loan contract. Also included are our regular sections summarizing a Bitcoin Core PR Review Club, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Sep 6, 2024 This week’s newsletter summarizes a proposal for allowing Stratum v2 pool miners to receive compensation for the transaction fees contained in the block templates they turn into shares, announces a research fund investigating the proposed OP_CAT opcode, and describes a discussion about mitigating merkle tree vulnerabilities with or without a soft fork. Also included are our regular sections announcing new releases and release candidates as well as describing notable changes to popular Bitcoin infrastructure software. Aug 30, 2024 This week’s newsletter announces a new mailing list to discuss Bitcoin mining. Also included are our regular sections summarizing popular questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and descriptions of recent changes to popular Bitcoin infrastructure software. Aug 23, 2024 This week’s newsletter summarizes discussion about an anti-exfiltration protocol that only requires one round trip of communication between a wallet and a signing device. Also included are our regular sections describing updates to clients and services, announcing new releases and release candidates, and summarizing recent changes to popular Bitcoin infrastructure software. Aug 16, 2024 This week’s newsletter describes a new time warp that’s particularly consequential for the new testnet4, summarizes discussion about proposed mitigations for onion message denial-of-service concerns, seeks feedback on a proposal to allow LN payers to optionally identify themselves, and announces a major change to Bitcoin Core’s build system that could affect downstream developers and integrators. Also included are our regular sections announcing new releases and release candidates and describing notable changes to popular Bitcoin infrastructure software. Aug 9, 2024 This week’s newsletter announces the Dark Skippy fast seed exfiltration attack, summarizes discussion about block withholding attacks and proposed solutions, shares statistics about compact block reconstruction, describes a replacement cycling attack against transactions with pay-to-anchor outputs, mentions a new BIP specifying threshold signing with FROST, and relays an announcement of an improvement to Eftrace that allows it to opportunistically verify zero-knowledge proofs using two proposed soft forks. Aug 2, 2024 This week’s newsletter announces the disclosure of two vulnerabilities affecting older versions of Bitcoin Core and summarizes a proposed approach to optimizing miner transaction selection when cluster mempool is in use. Also included are our regular sections announcing new releases and release candidates and describing notable changes to popular Bitcoin infrastructure software. Jul 26, 2024 This week’s newsletter summarizes a wide-ranging discussion about free relay and fee-bumping upgrades in Bitcoin Core. Also included are our regular sections sharing popular questions and answers from the Bitcoin Stack Exchange, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure projects. Jul 19, 2024 This week’s newsletter describes a distributed key generation protocol for the FROST scriptless threshold signature scheme and links to a comprehensive introduction to cluster linearization. Also included are our regular sections describing recent changes to clients, services, and popular Bitcoin infrastructure projects. Jul 12, 2024 This week’s newsletter includes our"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 4,
    "text": "regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Jul 5, 2024 This week’s newsletter summarizes the disclosure of 10 vulnerabilities affecting old versions of Bitcoin Core and describes a proposal to allow BOLT11 invoices to include blinded paths. Also included are our regular sections announcing new releases and release candidates and summarizing notable changes to popular Bitcoin infrastructure software. Jun 28, 2024 This week’s newsletter summarizes research into estimating the likelihood that an LN payment is feasible. Also included are our regular sections with descriptions of popular questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure projects. Jun 21, 2024 This week’s newsletter announces the disclosure of a vulnerability affecting old versions of LND and summarizes continued discussion about PSBTs for silent payments. Also included are our regular sections describing recent changes to services and client software, announcing new releases and release candidates, and summarizing notable changes to popular Bitcoin infrastructure software. Jun 14, 2024 This week’s newsletter announces a draft BIP for a quantum-safe Bitcoin address format and includes our regular sections with the summary of a Bitcoin Core PR Review Club, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Jun 7, 2024 This week’s newsletter announces an upcoming disclosure of vulnerabilities affecting older versions of Bitcoin Core, describes a draft BIP for a new version of testnet, summarizes a proposal for covenants based on functional encryption, examines an update to the proposal for performing 64-bit arithmetic in Bitcoin Script, links to a script for validating proof-of-work on signet with the OP_CAT opcode, and looks at a proposed update to the BIP21 specification of bitcoin: URIs. Also included are our regular sections announcing new releases and release candidates, plus summaries of notable changes to popular Bitcoin infrastructure software. May 31, 2024 This week’s newsletter describes a proposed light client protocol for silent payments, summarizes two new proposed descriptors for taproot, and links to a discussion about whether opcodes with overlapping features should be added in a soft fork. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. May 24, 2024 This week’s newsletter summarizes an analysis of several proposals for upgrading LN channels without closing and reopening them, discusses challenges in ensuring pool miners are paid appropriately, links to a discussion about safely using PSBTs for communicating information related to silent payments, announces a proposed BIP for miniscript, and summarizes a proposal for using frequent rebalancing of an LN channel to simulate a price futures contract. Also included are our regular sections summarizing changes to services and client software, announcing new releases and release candidates, and summarizing notable changes to popular Bitcoin infrastructure software. May 17, 2024 This week’s newsletter summarizes a new scheme for anonymous usage tokens that could be used for LN channel announcements and multiple other sybil-resistant coordination protocols, links to discussion about a new BIP39 seed phrase splitting scheme, announces an alternative to BitVM for verifying successful execution of arbitrary programs in interactive contract protocols, and relays suggestions for updating the BIPs process. May 15, 2024 This week’s newsletter announces the beta release of a full node supporting utreexo and summarizes a two proposed extensions to BIP119 OP_CHECKTEMPLATEVERIFY . Also included are our regular sections announcing new releases and release candidates and describing notable changes to popular Bitcoin infrastructure software. May 8, 2024 This week’s newsletter describes an idea for securing transactions with lamport signatures without requiring any consensus changes. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club, announcements of new releases and release candidates, and descriptions of changes to popular Bitcoin infrastructure software. May 1, 2024 This week’s newsletter summarizes a CTV-like proposal that uses commitments embedded in public keys, examines the analysis of a contract protocol with Alloy, announces the arrests of Bitcoin developers, and links to summaries of a CoreDev.tech developer meetup. Also included are our regular sections announcing new releases and release candidates and summarizing notable changes to popular Bitcoin infrastructure software. Apr 24, 2024 This week’s newsletter describes a proposal to relay weak blocks to improve compact block performance in a network with multiple divergent mempool policies and announces the addition of five BIP editors. Also included are our regular sections with selected questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Apr 17, 2024 This"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 5,
    "text": "week’s newsletter summarizes an analysis of how a node with cluster mempool behaved when tested with all transactions seen on the network in 2023. Also included are our regular sections describing recent updates to clients and services, announcing new releases and release candidates, and summarizing notable changes to popular Bitcoin infrastructure software. Apr 10, 2024 This week’s newsletter announces a new domain-specific language for experimenting with contract protocols, summarizes a discussion about modifying BIP editor responsibilities, and describes proposals to reset and modify testnet. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Apr 3, 2024 This week’s newsletter summarizes discussion about a new push for a consensus cleanup soft fork and announces a plan to choose additional BIP editors by the end of the week. Also included are our regular sections announcing new releases and describing changes to popular Bitcoin infrastructure software. Mar 27, 2024 This week’s newsletter announces the disclosure of a bandwidth-wasting attack affecting Bitcoin Core and related nodes, describes several improvements to the idea for transaction fee sponsorship, and summarizes a discussion about using live mempool data to improve Bitcoin Core’s feerate estimation. Also included are our regular sections with selected questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Mar 20, 2024 This week’s newsletter announces a project to create a BIP324 proxy for light clients and summarizes discussion about a proposed BTC Lisp language. Also included are our regular sections describing recent changes to clients and services, announcing new releases and release candidates, and summarizing notable changes to popular Bitcoin infrastructure software. Mar 13, 2024 This week’s newsletter summarizes a post about trustless onchain betting for potential soft forks and links to a detailed overview of Chia Lisp for Bitcoiners. Also included are our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Mar 6, 2024 This week’s newsletter summarizes a discussion about updating the specification for BIP21 bitcoin: URIs, describes a proposal to manage multiple concurrent MuSig2 signing sessions with a minimum of state, links to a thread about adding editors for the BIPs repository, and discusses a set of tools that allow quickly porting the Bitcoin Core GitHub project to a self-hosted GitLab project. Also included are our regular sections announcing new releases and release candidates and summarizing recent changes to popular Bitcoin infrastructure software. Feb 28, 2024 This week’s newsletter describes a proposed contract for trustless miner feerate futures, links to a coin selection algorithm for LN nodes providing dual funding liquidity, details a prototype for a vault using OP_CAT , and discusses sending and receiving ecash using LN and ZKCPs. Also included are our regular sections summarizing popular questions and answers from the Bitcoin Stack Exchange, announcing new releases and release candidates, and describing recent changes to popular Bitcoin infrastructure projects. Feb 21, 2024 This week’s newsletter describes a proposal for providing DNS-based human-readable Bitcoin payment instructions, summarizes a post with thoughts about mempool incentive compatibility, links to a thread discussing the design of Cashu and other ecash systems, briefly looks at continuing discussion about 64-bit arithmetic in Bitcoin scripts (including a specification for a previously proposed opcode), and gives an overview of an improved reproducible ASMap creation process. Also included are our regular sections describing updates to clients and services, new releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Feb 14, 2024 This week’s newsletter summarizes ideas for relay enhancements after cluster mempool is deployed, describes results of research into the topologies and sizes of LN-style anchor outputs in 2023, announces a new host for the Bitcoin-Dev mailing list, and encourages readers to celebrate I Love Free Software Day by thanking free software contributors. Also included are our regular sections summarizing a Bitcoin Core PR Review Club meeting and describing notable changes to popular Bitcoin infrastructure software. Feb 7, 2024 This week’s newsletter announces the public disclosure of a block stalling bug in Bitcoin Core affecting LN, relays a concern about how to securely open new zero-conf channels that are compatible with the proposed version 3 transaction topology restrictions, describes a rule many contract protocols must follow when allowing an external party to contribute an input to a transaction, summarizes multiple discussions about a proposal for new transaction replacement rules to avoid transaction pinning, and provides a brief update on the Bitcoin-Dev mailing list. Jan 31, 2024 This week’s newsletter describes a proposal to allow replacement of v3 transactions using RBF rules to ease the transition to cluster mempool and"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 6,
    "text": "summarizes an argument against OP_CHECKTEMPLATEVERIFY based on it commonly requiring exogenous fees. Also included are our regular sections summarizing top questions and answers from the Bitcoin Stack Exchange, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure projects. Jan 24, 2024 This week’s newsletter announces a fixed consensus failure in older versions of btcd, describes proposed changes to LN for v3 transaction relay and ephemeral anchors, and announces a new repository for Bitcoin-related specifications. Also included are our regular sections describing updates to services and client software, announcing new releases and release candidates, and summarizing notable changes to popular Bitcoin infrastructure software. Jan 17, 2024 This week’s newsletter discloses a past vulnerability affecting Core Lightning, announces two new soft fork proposals, provides an overview of the cluster mempool proposal, relays information about an updated specification and implementation of transaction compression, and summarizes a discussion about Miner Extractable Value (MEV) in non-zero ephemeral anchors. Also included are our regular sections with the announcements of new releases and descriptions of notable changes to popular Bitcoin infrastructure software. Jan 10, 2024 This week’s newsletter summarizes discussion about LN anchors and elements of the v3 transaction relay proposal and announces a research implementation of LN-Symmetry. Also included are our regular sections with the summary of Bitcoin Core PR Review Club meeting and the description of notable changes to popular Bitcoin infrastructure software. Jan 3, 2024 This week’s newsletter shares the disclosure of past vulnerabilities in LND, summarizes a proposal for fee-dependent timelocks, describes an idea for improving fee estimation using transaction clusters, discusses how to specify unspendable keys in descriptors, examines the cost of pinning in the v3 transaction relay proposal, mentions a proposed BIP to allow descriptors to be included in PSBTs, announces a tool that can be used with the MATT proposal to prove a program executed correctly, looks at a proposal for allowing highly efficient group exits from a pooled UTXO, and points to new coin selection strategies being proposed for Bitcoin Core. Also included are our regular sections announcing new software releases and describing notable changes to popular Bitcoin infrastructure. Dec 20, 2023 This special edition of the Optech Newsletter summarizes notable developments in Bitcoin during all of 2023. Dec 13, 2023 This week’s newsletter summarizes a discussion about griefing liquidity advertisements and includes our regular sections describing changes to services and client software, summarizing popular questions and answers of the Bitcoin Stack Exchange, announcing new software releases and release candidates, and examining recent changes to popular Bitcoin infrastructure software. Dec 6, 2023 This week’s newsletter describes several discussions about the proposed cluster mempool and summarizes the results of a test performed using warnet. Also included are our regular sections summarizing a meeting of the Bitcoin Core PR Review Club, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Nov 29, 2023 This week’s newsletter summarizes an update to the liquidity advertisements specification. Also included are our regular sections with selected questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Nov 22, 2023 This week’s newsletter describes a proposal to allow retrieval of LN offers using specific DNS addresses similar to lightning addresses. Also included are our regular sections summarizing changes to services and client software, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Nov 15, 2023 This week’s newsletter describes an update to the proposal for ephemeral anchors and provides a contributed field report about miniscript from a developer working at Wizardsardine. Also included are our regular sections announcing new software releases and release candidates and summarizing notable changes to popular Bitcoin infrastructure projects. Nov 8, 2023 This week’s newsletter announces an upcoming change to the Bitcoin-Dev mailing list and briefly summarizes a proposal to allow aggregating multiple HTLCs together. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Nov 1, 2023 This week’s newsletter follows up on several recent discussions about proposed changes to Bitcoin’s scripting language. Also included are our regular sections announcing new releases and describing notable changes to popular Bitcoin infrastructure software. Oct 25, 2023 This week’s newsletter describes the replacement cycling attack against HTLCs used in LN and other systems, examines the mitigations deployed for the attack, and summarizes several proposals for additional mitigations. Also described are a notable bug affecting a Bitcoin Core RPC, research into covenants with minimal changes to Bitcoin Script, and a proposed BIP for an OP_CAT opcode. Also included is our regular monthly section with summaries of popular questions and answers"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 7,
    "text": "from the Bitcoin Stack Exchange. Oct 18, 2023 This week’s newsletter briefly mentions a recent security disclosure affecting LN users, describes a paper about making payments contingent on the result of running arbitrary programs, and announces a proposed BIP to add fields to PSBTs for MuSig2. Also included are our regular sections that summarize improvements to clients and services, announce new releases and release candidates, and describe notable changes to popular Bitcoin infrastructure software. Oct 11, 2023 This week’s newsletter links to a specification for a proposed OP_TXHASH opcode and includes our regular sections summarizing a Bitcoin Core PR Review Club meeting, linking to new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Oct 4, 2023 This week’s newsletter summarizes a proposal for remotely controlling LN nodes using a hardware signing device, describes privacy-focused research and code for allowing LN forwarding nodes to dynamically split LN payments, and looks at a proposal for improving LN liquidity by allowing groups of forwarding nodes to pool funds separately from their normal channels. Also included are our regular sections announcing new releases and describing notable changes to popular Bitcoin infrastructure software. Sep 27, 2023 This week’s newsletter describes a proposal to use covenants to significantly improve LN’s scalability. Also included are our regular sections summarizing popular questions and answers on the Bitcoin Stack Exchange, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Sep 20, 2023 This week’s newsletter shares the announcement of an upcoming research event and includes our regular sections summarizing significant updates to various service and client software, announcing new software releases and release candidates, and describing recent changes to popular infrastructure software. Sep 13, 2023 This week’s newsletter links to draft specifications related to taproot assets and describes a summary of several alternative message protocols for LN that can help enable the use of PTLCs. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Sep 6, 2023 This week’s newsletter describes a new technique for compressing Bitcoin transactions and summarizes an idea for privacy-enhanced transaction cosigning. Also included are our regular sections announcing new releases and release candidates and describing notable changes to popular Bitcoin infrastructure software. Aug 30, 2023 This week’s newsletter announces the responsible disclosure of a vulnerability affecting old LN implementations and summarizes a suggestion for a mashup of proposed covenant opcodes. Also included are our regular sections with selected questions and answers from the Bitcoin Stack Exchange, announcements of new software releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Aug 23, 2023 This week’s newsletter describes fraud proofs for outdated backup state and includes our regular sections summarizing recent changes to services and client software, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Aug 16, 2023 This week’s newsletter summarizes a discussion about adding expiration dates to silent payment addresses and provides an overview of a draft BIP for serverless payjoin. A contributed field report describes the implementation and deployment of a MuSig2-based wallet for scriptless multisignatures. Also included are our regular sections with announcements of new releases and release candidates and descriptions of notable changes to popular Bitcoin infrastructure projects. Aug 9, 2023 This week’s newsletter warns about a severe vulnerability in uses of Libbitcoin’s Bitcoin Explorer (bx) tool, summarizes a discussion about the design of denial-of-service protection, announces a plan to begin testing and collecting data about HTLC endorsement, and describes two proposed changes to Bitcoin Core’s transaction relay policy. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Aug 2, 2023 This week’s newsletter links to transcripts of recent LN specification meetings and summarizes a thread about the safety of blind MuSig2 signing. Also included are our regular sections with descriptions of new releases, release candidates, and notable code changes to popular Bitcoin infrastructure software. Jul 26, 2023 This week’s newsletter describes a protocol for simplifying the communication related to mutual closing of LN channels and summarizes notes from a recent meeting of LN developers. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Jul 19, 2023 This week’s newsletter includes the final entry in our limited weekly series about mempool policy, plus our regular sections describing notable changes to clients, services, and popular Bitcoin infrastructure software. Jul 12, 2023 This week’s"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 8,
    "text": "newsletter describes a proposal to remove details from the LN specification that are no longer relevant to modern nodes and includes the penultimate entry in our limited weekly series about mempool policy, plus our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcing new releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Jul 5, 2023 This week’s newsletter includes another entry in our limited weekly series about mempool policy, plus our regular sections announcing new releases and release candidates and describing notable changes to popular Bitcoin infrastructure software. Jun 28, 2023 This week’s newsletter summarizes an idea for preventing the pinning of coinjoin transactions and describes a proposal for speculatively using hoped-for consensus changes. Also included is another entry in our limited weekly series about mempool policy, plus our regular sections describing popular questions and answers on the Bitcoin Stack Exchange, new releases and release candidates, and changes to popular Bitcoin infrastructure software. Jun 21, 2023 This week’s newsletter summarizes a discussion about extending BOLT11 invoices to request two payments. Also included is another entry in our limited weekly series about mempool policy, plus our regular sections describing updates to clients and services, new releases and release candidates, and changes to popular Bitcoin infrastructure software. Jun 14, 2023 This week’s newsletter summarizes a discussion about allowing relay of transactions containing data in the taproot annex field and links to a draft BIP for silent payments. Also included is another entry in our limited weekly series about mempool policy, plus our regular sections summarizing a Bitcoin Core PR Review Club meeting, announcing new software releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. Jun 7, 2023 This week’s newsletter summarizes mailing list discussion about using the MATT proposal to manage joinpools and replicate functions of the OP_CHECKTEMPLATEVERIFY proposal. Also included is another entry in our limited weekly series about mempool policy, plus our regular sections for announcing new software releases and release candidates and describing notable changes to popular Bitcoin infrastructure software. May 31, 2023 This week’s newsletter describes a proposal for a new managed joinpool protocol and summarizes an idea for relaying transactions using the Nostr protocol. Also included is another entry in our limited weekly series about mempool policy, plus our regular sections summarizing notable questions and answers posted to the Bitcoin Stack Exchange, listing new software releases and release candidates, and describing notable changes to popular Bitcoin infrastructure software. May 24, 2023 This week’s newsletter describes research into zero-knowledge validity proofs for Bitcoin and related protocols. Also included is another entry in our limited weekly series about mempool policy, plus our regular sections describing updates to clients and services, new releases and release candidates, and changes to popular Bitcoin infrastructure projects. May 17, 2023 This week’s newsletter describes a proposal to begin testing HTLC endorsement, relays a request for feedback about proposed specifications for Lightning Service Providers (LSPs), discusses challenges with opening zero-conf channels when using dual funding, looks at a suggestion for advanced applications of payjoin transactions, and links to summaries of a recent in-person meeting of Bitcoin Core developers. Also included in this week’s newsletter is the first part of a new series about policies for transaction relay and mempool inclusion, plus our regular sections announcing new releases and release candidates (including a security release of libsecp256k1) and describing notable changes to popular Bitcoin infrastructure software. May 10, 2023 This week’s newsletter summarizes a paper about the PoWswap protocol and includes our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Also included is a short section celebrating five years of Bitcoin Optech and our 250th newsletter. May 3, 2023 This week’s newsletter summarizes an analysis of using a flexible covenant design to reimplement the OP_VAULT proposal, summarizes a post about signature adaptor security, and relays a job announcement that may be particularly interesting to some readers. Also included are our regular sections describing new releases, release candidates, and notable changes to popular Bitcoin infrastructure software. Apr 26, 2023 This week’s newsletter relays a request for feedback about a proposal to remove support for the BIP35 mempool P2P protocol message from Bitcoin Core and includes our regular sections with popular questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Apr 19, 2023 This week’s newsletter provides an update on development of the RGB protocol and includes our regular sections that summarize recent updates to clients and services, announce new releases and release candidates, and describe notable changes to popular Bitcoin infrastructure software. Apr 12, 2023 This week’s newsletter"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 9,
    "text": "describes a discussion about LN splicing and links to a proposed BIP for recommended transaction terminology. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new releases and release candidates—including a security update for libsecp256k1—and and descriptions of notable changes made to popular Bitcoin infrastructure software. Apr 5, 2023 This week’s newsletter summarizes an idea for watchtower accountability proofs and includes our regular sections with announcements of new releases and release candidates and descriptions of notable changes to popular Bitcoin infrastructure software. Mar 29, 2023 This week’s newsletter describes a proposal to improve capital efficiency on LN using tunable penalties. Also included are our regular sections with summaries of top questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Mar 22, 2023 This week’s newsletter includes our regular sections with descriptions of changes to services and client software, plus summaries of notable changes to popular Bitcoin infrastructure software. Mar 15, 2023 This week’s newsletter relays the announcement of a service bit being used for testing Utreexo, links to several new software releases and release candidates, and describes a merged Bitcoin Core pull request. Mar 8, 2023 This week’s newsletter describes a proposal for an alternative design for OP_VAULT with several benefits and announces a new weekly Optech podcast. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Mar 1, 2023 This week’s newsletter summarizes a discussion about the fastest way to verify that a BIP32 master seed backup probably hasn’t been corrupted without using any digital devices. Also included are our regular sections with announcements of new releases and release candidates, plus summaries of notable changes to popular Bitcoin infrastructure software. Feb 22, 2023 This week’s newsletter links to a draft BIP for the proposed OP_VAULT opcode, summarizes a discussion about allowing LN nodes to set a quality-of-service flag on their channels, relays a request for feedback on LN neighbor-node evaluation criteria, and describes a draft BIP for a seed backup and recovery scheme that can be reliably performed without electronics. Also included are our regular sections with summaries of popular questions and answers from the Bitcoin StackExchange, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Feb 15, 2023 This week’s newsletter summarizes continued discussion about storing data in the Bitcoin block chain, describes a hypothetical fee dilution attack against some types of multiparty protocols, and describes how a tapscript signature commitment can be used with different parts of the same tree. Also included are our regular sections with summaries of changes to services and client software, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. We additionally provide one of our rare recommendations for a new search engine focused on Bitcoin technical documentation and discussion. Feb 8, 2023 This week’s newsletter summarizes a discussion about storing data in transaction witnesses and references a conversation about mitigating LN jamming. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting and descriptions of notable changes to popular Bitcoin infrastructure software. Feb 1, 2023 This week’s newsletter summarizes a proposal for serverless payjoin and describes an idea for allowing proof of payment for LN async payments. Also included is our regular section with descriptions of notable changes to popular Bitcoin infrastructure software. Jan 25, 2023 This week’s newsletter summarizes an analysis comparing proposals for ephemeral anchors to SIGHASH_GROUP and relays a request for researchers to investigate how to create proof that an LN async payment was accepted. Also included are our regular sections with summaries of popular questions and answers on the Bitcoin Stack Exchange and descriptions of notable changes to popular Bitcoin infrastructure software. Jan 18, 2023 This week’s newsletter describes a proposal for new vault-specific opcodes and includes our regular sections with summaries of interesting updates to clients and services, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Jan 11, 2023 This week’s newsletter describes an idea for allowing offline LN nodes to receive funds onchain that they’ll later be able to use offchain without extra delay. Also included are our regular sections with summaries of new software releases and release candidates, plus descriptions of notable changes to popular Bitcoin infrastructure software. Jan 4, 2023 This week’s newsletter warns users of Bitcoin Knots about a release signing key compromise, announces the release of two software forks of Bitcoin Core, and summarizes continued discussion about replace-by-fee"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 10,
    "text": "policies. Also included are our regular sections with the announcements of new software releases and release candidates, plus descriptions of notable changes to popular Bitcoin infrastructure software. Dec 21, 2022 This special edition of the Optech Newsletter summarizes notable developments in Bitcoin during all of 2022. Dec 14, 2022 This week’s newsletter summarizes a proposal for a modified version of LN that may improve compatibility with channel factories, describes software for mitigating some effects of channel jamming attacks without changing the LN protocol, and links to a website for tracking unsignaled transaction replacements. Also included are our regular sections with announcements of new client and service software, summaries of popular questions and answers on the Bitcoin Stack Exchange, and descriptions of notable changes to popular Bitcoin infrastructure software. Dec 7, 2022 This week’s newsletter describes an implementation of ephemeral anchors and includes our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Nov 30, 2022 This week’s newsletter describes a proposal to mitigate LN jamming attacks using reputation credential tokens. Also included are our regular sections with announcements of new software releases and release candidates and summaries of notable changes to popular Bitcoin infrastructure software. Nov 23, 2022 This week’s newsletter contains our regular sections with selected questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Nov 16, 2022 This week’s newsletter describes a proposal to enable generalized smart contracts on Bitcoin and summarizes a paper about addressing LN channel jamming attacks. Also included are our regular sections with descriptions of changes to services and client software, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Nov 9, 2022 This week’s newsletter summarizes continued discussion about a configuration option for enabling full-RBF in Bitcoin Core and describes a bug affecting BTCD, LND, and other software. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, descriptions of new releases and release candidates, and overviews of notable changes to popular Bitcoin infrastructure software. Nov 2, 2022 This week’s newsletter describes continued discussion about optionally allowing nodes to enable full RBF, relays a request for feedback on a design element of the BIP324 version 2 encrypted transport protocol, summarizes a proposal for reliably attributing LN failures and delays to particular nodes, and links to a discussion about an alternative to using anchor outputs for modern LN HTLCs. Also included are our regular sections with the announcements of new software releases and release candidates—including a security critical update for LND—and descriptions of notable changes to popular Bitcoin infrastructure software. Oct 26, 2022 This week’s newsletter summarizes continued discussion about enabling full RBF, provides overviews for several transcripts of discussions at a CoreDev.tech meeting, and describes a proposal for ephemeral anchor outputs designed for contract protocols like LN. Also included are our regular sections with summaries of popular questions and answers from the Bitcoin Stack Exchange, a list of new software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Oct 19, 2022 This week’s newsletter describes the block parsing bug affecting BTCD and LND last week, summarizes discussion about a planned Bitcoin Core feature change related to replace by fee, outlines research about validity rollups on Bitcoin, shares an announcement about a vulnerability in the draft BIP for MuSig2, examines a proposal to reduce the minimum size of an unconfirmed transaction that Bitcoin Core will relay, and links to an update of the BIP324 proposal for a version 2 encrypted transport protocol for Bitcoin. Also included are our regular sections with summaries of changes to services and client software, announcements of new releases and release candidates, and descriptions of notable merges to popular Bitcoin infrastructure projects. Oct 12, 2022 This week’s newsletter summarizes a proposal to allow casual LN users to stay offline for up to several months at a time and describes a document about allowing transaction information servers to host unused wallet addresses. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club, announcements of new software releases and release candidates (including a critical LND fix), and descriptions of notable changes to popular Bitcoin infrastructure software. Oct 5, 2022 This week’s newsletter describes a proposal for new opt-in transaction relay rules and summarizes research into helping LN channels stay balanced. Also included are our regular sections listing new software releases and release candidates plus notable changes to popular Bitcoin infrastructure projects. Sep 28, 2022 This week’s newsletter describes a proposal to allow LN nodes to advertise capacity-dependent feerates"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 11,
    "text": "and announces a software fork of Bitcoin Core focused on testing major protocol changes on signet. Also included are our regular sections with summaries of popular questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Sep 21, 2022 This week’s newsletter summarizes a discussion about using SIGHASH_ANYPREVOUT to emulate aspects of drivechains. Also included are our regular sections describing recent changes to services, client software, and popular Bitcoin infrastructure software. Sep 14, 2022 This week’s newsletter includes our regular section with the summary of a Bitcoin Core PR Review Club meeting, a list of new software releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure projects. Sep 7, 2022 This week’s newsletter summarizes several notable changes to popular Bitcoin infrastructure software. Aug 31, 2022 This week’s newsletter describes a proposal for a standardized wallet label export format and includes our regular sections with summaries of recent questions and answers from the Bitcoin StackExchange, a list of new software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Aug 24, 2022 This week’s newsletter links to the overview of a guide about channel jamming attacks and summarizes several updates to a PR for silent payments. Also included are our regular sections with descriptions of changes to popular services and clients, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Aug 17, 2022 This week’s newsletter describes how BLS signatures could be used to improve DLCs without consensus changes to Bitcoin and includes our regular sections with announcements of new software releases and release candidates, plus summaries of notable changes to popular Bitcoin infrastructure software. Aug 10, 2022 This week’s newsletter summarizes a discussion about lowering the default minimum transaction relay feerate in Bitcoin Core and other nodes. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Aug 3, 2022 This week’s newsletter describes a proposal to allow multiple derivation paths in a single output script descriptor and includes our regular section with summaries of notable changes to popular Bitcoin infrastructure projects. Jul 27, 2022 This week’s newsletter describes a proposed BIP for creating signed messages for non-legacy addresses and summarizes a discussion about provably burning small amounts of bitcoin for denial of service protection. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, announcements of new releases and releases candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Jul 20, 2022 This week’s newsletter summarizes several related discussions about providing a sustainable long term block reward for Bitcoin. Also included are our regular sections with descriptions of new features for clients and services, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Jul 13, 2022 This week’s newsletter summarizes discussions about half aggregation of schnorr signatures, a workaround for protocols that can’t reliably use x-only pubkeys, and allowing deliberately slow LN payment forwarding. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club, announcements of releases and relase candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Jul 6, 2022 This week’s newsletter summarizes discussions about long-term block reward funding, alternatives to BIP47 reusable payment codes, options for announcing LN channel splices, LN routing fee collection strategies, and onion message rate limiting. Also included are our regular sections with announcements of new software releases and release candidates, plus summaries of notable changes to popular Bitcoin infrastructure software. Jun 29, 2022 This week’s newsletter includes our regular sections summarizing popular questions and answers from Bitcoin Stack Exchange, announcing new software releases and release candidates, and describing recent changes to Bitcoin infrastructure software. Jun 22, 2022 This week’s newsletter describes a proposed option for Bitcoin Core that would make it easier to enable transaction replacement even for transactions that don’t opt-in to BIP125, links to information about the Hertzbleed sidechannel vulnerability, summarizes the conclusion of a discussion about time stamping system design, and examines a new anti-sybil protocol that uses Bitcoin UTXOs. Also included are our regular sections with descriptions of interesting new features in Bitcoin clients and services, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Jun 15, 2022 This week’s newsletter summarizes continued discussion about adding package relay to the Bitcoin P2P network, shares a summary of the recent LN developers meeting, and describes an argument for how spenders and routing nodes on LN can optimize for both reliability and"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 12,
    "text": "low fees in a way that benefits both groups. Also included are our regular sections with summaries of recent releases and release candidates plus notable changes to popular Bitcoin infrastructure software. Jun 8, 2022 This week’s newsletter includes our regular sections with the summary of a Bitcoin Core PR Review Club meeting, a list of new software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Jun 1, 2022 This week’s newsletter describes experimentation by developers working on silent payments and includes our regular sections with summaries of new releases and release candidates plus notable changes to popular Bitcoin infrastructure software. May 25, 2022 This week’s newsletter summarizes a draft BIP for package relay and provides an overview of a concern with Miner Extractable Value (MEV) for Bitcoin covenant design. Also included are our regular sections with the summary of top questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. May 18, 2022 This week’s newsletter summarizes a discussion about the minimum change to Bitcoin’s Script language necessary to enable recursive covenants, examines a revised proposal to add an OP_TX opcode, and reviews research into adapting output script descriptors for hardware signing devices. Also included are our regular sections with the summary of recent changes to Bitcoin services and clients, new releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Additionally, we celebrate the publication of Optech’s 200 th regular newsletter. May 11, 2022 This week’s short newsletter summarizes a Bitcoin Core PR Review Club meeting and describes an update to Rust Bitcoin. May 4, 2022 This week’s newsletter summarizes a post about implementing MuSig2, relays the responsible disclosure of a security issue affecting some older LN implementations, discusses a proposal for measuring support for consensus changes through transaction signaling, and examines the effect of rate limiting on more bandwidth efficient LN gossiping. Also included are our regular sections summarizing new software releases and release candidates plus notable changes to popular Bitcoin infrastructure projects. Apr 27, 2022 This week’s newsletter summarizes discussion about activating OP_CHECKTEMPLATEVERIFY and includes our regular sections highlighting top questions and answers on the Bitcoin Stack Exchange, new software releases and release candidates, and recent changes to popular Bitcoin infrastructure software. Apr 20, 2022 This week’s newsletter summarizes a discussion about allowing quantum-safe key exchange on Bitcoin and includes our regular sections with descriptions of notable changes to services and client software, releases and release candidates, and popular Bitcoin infrastructure software. Apr 13, 2022 This week’s newsletter describes a protocol for transferring non-bitcoin tokens in Bitcoin transactions and LN payments and links to a proposed BIP for the MuSig2 multisignature protocol. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Apr 6, 2022 This week’s newsletter describes a proposal for delinked reusable addresses, summarizes how the WabiSabi protocol may be used as an enhanced alternative to payjoin, examines a discussion about adding communication standards to the DLC specification, and looks at renewed discussion about updating LN commitment formats. Also included are our regular sections with summaries of new software releases and release candidates plus descriptions of notable changes to popular Bitcoin infrastructure software. Mar 30, 2022 This week’s newsletter describes a proposal for Bitcoin Core to allow replacing transaction witnesses in its mempool and summarizes continued discussion about updating the LN gossip protocol. Also included are our regular sections with selected questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Mar 23, 2022 This week’s newsletter summarizes discussion about the speedy trial soft fork activation mechanism and links to an update of an optimized LN pathfinding algorithm. Also included are our regular sections with descriptions of recent changes to services and client software, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Mar 16, 2022 This week’s newsletter describes proposals to extend or replace Bitcoin Script with new opcodes, summarizes recent discussions about improving RBF policy, and links to continued work on the OP_CHECKTEMPLATEVERIFY opcode. Also included is our regular section describing notable changes to popular Bitcoin infrastructure projects. Mar 9, 2022 This week’s newsletter describes multiple facets of a discussion about how much future soft forks should increase the expressiveness of Bitcoin’s Script and Tapscript languages and summarizes a proposal to charge for bandwidth used relaying onion messages. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new software releases and RCs,"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 13,
    "text": "and descriptions of notable changes to popular Bitcoin infrastructure projects. Mar 2, 2022 This week’s newsletter describes a new proposed OP_EVICT opcode and includes our regular sections with summaries of new releases and release candidates and notable changes to popular Bitcoin infrastructure software. Feb 23, 2022 This week’s newsletter summarizes a discussion about fee bumping and transaction fee sponsorship, describes a proposal for an updated LN gossip wire protocol, and advertises a signet for testing OP_CHECKTEMPLATEVERIFY . Also included are our regular sections with selected questions and answers from the Bitcoin Stack Exchange and descriptions of notable changes to popular Bitcoin infrastructure projects. Feb 16, 2022 This week’s newsletter describes continued discussion about covenants in Bitcoin and includes our regular sections with summaries of changes to services and client software and notable changes to popular Bitcoin infrastructure software. Feb 9, 2022 This week’s newsletter describes a discussion about changing relay policy for replace-by-fee transactions and includes our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Feb 2, 2022 This week’s newsletter describes an analysis of the proposed OP_CHECKTEMPLATEVERIFY (CTV) opcode’s effect on discreet log contracts and summarizes a discussion about alternative changes to tapscript to enable CTV and SIGHASH_ANYPREVOUT . Also included are our regular sections with announcements of new releases and notable changes to popular Bitcoin infrastructure software. Jan 26, 2022 This week’s newsletter describes a proposal to extend PSBTs with fields for spending outputs constructed using a pay-to-contract protocol and includes our regular sections with summaries of top posts from the Bitcoin Stack Exchange and notable changes to popular Bitcoin infrastructure projects. Jan 19, 2022 This week’s newsletter shares the announcement of a new legal defense fund for Bitcoin developers and summarizes some recent discussion about the proposed OP_CHECKTEMPLATEVERIFY soft fork. Also included are our regular sections with descriptions of some recent changes to services and client software, plus summaries of notable changes to popular Bitcoin infrastructure software. Jan 12, 2022 This week’s newsletter describes an idea to add accounts to Bitcoin for paying transaction fees and includes our regular sections with the summary of a Bitcoin Core PR Review Club meeting and descriptions of notable changes to popular Bitcoin infrastructure projects. Jan 5, 2022 This week’s newsletter describes an alternative proposal to slowly phase in full replace-by-fee and announces a series of meetings to review the proposed OP_CHECKTEMPLATEVERIFY soft fork. Also included are our regular sections with announcements of releases and release candidates and summaries of notable changes to popular Bitcoin infrastructure projects. Dec 22, 2021 This special edition of the Optech Newsletter summarizes notable developments in Bitcoin during all of 2021. Dec 15, 2021 This week’s newsletter describes a proposal to allow relay of transactions with zero-value outputs in some cases and summarizes a discussion about preparing LN for the adoption of PTLCs. Also included are our regular sections with a list of recent changes to services and client software, popular questions on the Bitcoin Stack Exchange, and notable changes to popular Bitcoin infrastructure software. Dec 8, 2021 This week’s newsletter describes a post about fee-bumping research and contains our regular sections with the summary of a Bitcoin Core PR Review Club meeting, the latest releases and release candidates for Bitcoin software, and notable changes to popular infrastructure projects. Dec 1, 2021 This week’s newsletter describes a recently fixed interoperability issue between different LN software and includes our regular sections with a list of new releases and release candidates plus notable changes to popular Bitcoin infrastructure software. Nov 24, 2021 This week’s newsletter links to a discussion about how to allow LN users to choose between higher fees and higher payment reliability. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, announcements of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Nov 17, 2021 This week’s newsletter provides information about the activation of taproot and includes our regular sections with summaries of changes to services and client software, new releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Nov 10, 2021 This week’s newsletter summarizes a post about ways of integrating discreet log contracts with LN channels, links to a detailed summary of the recent LN developer conference, and describes ideas for performing additional verification of compact block filters. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, our final column about preparing for taproot activation, descriptions of new releases and release candidates, and a list of notable changes to popular infrastructure software. Nov 3, 2021 This week’s newsletter summarizes a discussion about submitting transactions directly to miners, links"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 14,
    "text": "to a set of recommended taproot test vectors for wallet implementations, and includes our regular sections about preparing for taproot, new releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Oct 27, 2021 This week’s newsletter includes our regular sections with summaries of popular questions and answers from the Bitcoin Stack Exchange, information about preparing for taproot activation, a list of new releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Oct 20, 2021 This week’s newsletter summarizes a thread about paying frequently-offline LN nodes, describes a set of proposals for lowering the cost of LN payment path probing in order to make certain attacks more expensive, and links to instructions useful for creating taproot transactions on signet and testnet. Also included are our regular sections describing recent changes to clients and services, new releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Oct 13, 2021 This week’s newsletter describes a vulnerability recently fixed in several LN implementations and summarizes a proposal providing multiple benefits for upgrading the LN protocol to take advantage of features in taproot. Also included are our regular sections with the summary of a recent Bitcoin Core PR Review Club meeting, information about preparing for taproot, listings of new software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Oct 6, 2021 This week’s newsletter summarizes a proposal to add transaction heritage identifiers to Bitcoin and includes our regular sections with information about preparing for taproot, a list of new releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Sep 29, 2021 This week’s newsletter summarizes a proposal to implement breaking changes in the DLC specification, examines options for allowing recovery of closed LN channels using just a BIP32 seed, and describes an idea to generate stateless LN invoices. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, ideas for preparing for taproot’s activation, and descriptions of notable changes to popular Bitcoin infrastructure software. Sep 22, 2021 This week’s newsletter describes a proposed modification to the BIP process, summarizes a plan to add support for package relay to Bitcoin Core, and links to a discussion about adding LN node information to DNS. Also included are our regular sections with descriptions of changes to services and client software, how you can prepare for taproot, new releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Sep 15, 2021 This week’s newsletter describes a new proposal for a covenant opcode and summarizes a request for feedback on implementing regular reorgs on signet. Also included are our regular sections with ideas for preparing for taproot activation, a list of new releases and release candidates, and descriptions of popular Bitcoin infrastructure software. Sep 8, 2021 This week’s newsletter describes a proposal for Bitcoin-related MIME types and summarizes a paper about a design for a new decentralized mining pool. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, how to prepare for taproot, new releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Sep 1, 2021 This week’s newsletter describes a new web-based tool for decoding and modifying PSBTs and links to a blog post and proof-of-concept implementation of an eltoo-based LN payment channel. Also included are our regular sections with information about preparing for taproot, announcements of new software release candidates, and summaries of notable changes to popular Bitcoin infrastructure projects. Aug 25, 2021 This week’s newsletter summarizes a discussion about setting LN channel base fees to zero and includes our regular sections with popular questions and answers from the Bitcoin Stack Exchange, how you can prepare for taproot, new releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Aug 18, 2021 This week’s newsletter summarizes a discussion about the dust limit and includes our regular sections with descriptions of changes to services and client software, how you can prepare for taproot, new releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Aug 11, 2021 This week’s newsletter follows up on a previous description about fidelity bonds in JoinMarket and includes our regular sections with the summary of a Bitcoin Core PR Review Club meeting, suggestions for preparing for taproot, announcements of releases and release candidates, and descriptions of notable changes to popular infrastructure projects. Aug 4, 2021 This week’s newsletter includes our regular sections describing how you can prepare for taproot, summarizing the latest releases and release candidates, and listing notable changes to popular Bitcoin infrastructure projects. Jul 28, 2021 This week’s newsletter includes our regular sections with the best questions and answers of the past month from the Bitcoin Stack Exchange,"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 15,
    "text": "our latest column about preparing for taproot, a list of new software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Jul 21, 2021 This week’s newsletter describes recent changes to services and client software, discusses why wallets should wait before generating taproot addresses, lists new software releases and release candidates, and summarizes notable changes to popular Bitcoin infrastructure software. Jul 14, 2021 This week’s newsletter summarizes a discussion about a proposed new opcode and links to an updated wiki page for tracking bech32m support. Also included are our regular sections with highlights from a Bitcoin Core PR Review Club meeting, suggestions about preparing for taproot, and descriptions of notable changes to popular Bitcoin infrastructure projects. Jul 7, 2021 This week’s newsletter describes a set of BIPs for output script descriptors, summarizes a proposal to create a set of standards documents for LN protocol extensions and application interoperability, and discusses standardizing support for pre-trusted zero-conf channel opens. Also included are our regular sections describing how to prepare for taproot, releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Jun 30, 2021 This week’s newsletter summarizes two proposed BIPs related to wallet support for taproot and includes our regular sections describing selected questions and answers on the Bitcoin Stack Exchange, how to prepare for taproot, and notable changes to popular Bitcoin infrastructure projects. Jun 23, 2021 This week’s newsletter describes a proposal to allow universal transaction replacement by fee and includes the first post in a new weekly series about preparing for taproot. Also included are our regular sections describing updates to clients and services, new releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Jun 16, 2021 This week’s newsletter celebrates the lock-in of the taproot soft fork, describes a draft BIP for improving transaction privacy by varying the fields used to implement anti fee sniping, and features an article about the challenges of combining transaction replacement with payment batching. Also included are our regular sections with announcements of new software releases and release candidates, plus notable changes to popular Bitcoin infrastructure software. Jun 9, 2021 This week’s newsletter describes a proposal to allow LN nodes to receive payments without keeping their private keys online all the time. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Jun 2, 2021 This week’s newsletter describes a proposal to change Bitcoin Core’s transaction selection algorithm for miner block templates to slightly increase miner profitability and give fee bumping users more collective leverage. Also included are our regular sections describing software releases and release candidates, plus notable changes to popular Bitcoin infrastructure software. May 26, 2021 This week’s newsletter announces a change of networks for several IRC channels and celebrates Optech’s 150th newsletter. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, new software releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. May 19, 2021 This week’s newsletter provides updates on the previously proposed transaction relay reliability workshop and CVE-2021-31876. Also included are our regular sections describing updates to services and client software, new releases and release candidates, and notable changes to popular Bitcoin infrastructure software. May 12, 2021 This week’s newsletter describes a security disclosure affecting protocols depending on a certain BIP125 opt-in replace by fee behavior and includes our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of new software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. May 5, 2021 This week’s newsletter encourages miners to start signaling for taproot and describes continued discussion about closing lost LN channels using only a wallet seed. Also included are our regular sections with announcements of releases and release candidates, plus notable changes to popular Bitcoin infrastructure software. Apr 28, 2021 This week’s newsletter describes a draft specification for LN splicing, announces a workshop about transaction relay security, announces the addition of ECDSA signature adaptor support to libsecp256k1-zkp, and links to proposals to change the BIPs process. Also included are our regular sections with summaries of popular questions and answers from the Bitcoin Stack Exchange, announcements of software releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Apr 21, 2021 This week’s newsletter describes progress on activating taproot, summarizes an update to LN offers to partly address stuck payments, relays a request for feedback on anchor outputs in LND, and announces the public launch of the Sapio smart contract development toolkit. Also included are our regular sections with summaries of changes to popular clients and services, new releases"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 16,
    "text": "and release candidates, and notable changes to popular Bitcoin infrastructure software. Apr 14, 2021 This week’s newsletter summarizes recent progress on code to activate taproot and contains our regular sections with descriptions of a recent Bitcoin Core PR Review Club meeting and notable changes to popular Bitcoin infrastructure software. Apr 7, 2021 This week’s newsletter contains our regular sections with announcements of new releases and release candidates, plus notable changes to popular Bitcoin infrastructure projects. Mar 31, 2021 This week’s newsletter describes a paper and a short discussion about probabilistic path selection for LN and includes our regular sections with summaries of popular questions and answers from the Bitcoin Stack Exchange, release and release candidates, and notable changes to Bitcoin infrastructure software. Mar 24, 2021 This week’s newsletter describes a technique for signature delegation under Bitcoin’s existing consensus rules, summarizes a discussion about taproot’s effect on Bitcoin’s resistance to quantum cryptography, and announces a series of weekly meetings to help activate taproot. Also included are our regular sections describing notable changes to services and client software, new releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Mar 17, 2021 This week’s newsletter describes a discussion about rescuing lost LN funding transactions and includes our regular sections with announcements of releases, release candidates, and notable changes to popular Bitcoin infrastructure software. Mar 10, 2021 This week’s newsletter summarizes continued discussion about proposed methods for activating taproot and links to an effort to document existing software building on top of taproot. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, announcements of releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure projects. Mar 3, 2021 This week’s newsletter describes discussion about a desired replacement for some of the features of the BIP70 payment protocol and summarizes proposals for a standardized way to exchange fraud proofs for Discreet Log Contracts (DLCs). Also included are our regular sections describing new software releases, available release candidates, and notable changes to popular Bitcoin infrastructure software. Feb 24, 2021 This week’s newsletter describes the results of discussion about choosing activation parameters for a taproot soft fork and includes our regular sections with selected questions and answers from the Bitcoin Stack Exchange, releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Feb 17, 2021 This week’s newsletter describes the development of a new constant-time algorithm for generating and verifying transaction signatures, mentions a proposal to stop processing unsolicited transactions, summarizes a proposed BIP for setting up multisig wallets, shares insight from a discussion about managing escrows over LN, links to renewed discussion about bidirectional upfront LN fees, and mentions a new protocol for mitigating the risk of malicious hardware wallets. Also included are our regular sections with news about updated clients and services, announcements of new software releases and release candidates, and descriptions of notable changes in popular Bitcoin infrastructure software. Feb 10, 2021 This week’s newsletter links to the summary of last week’s taproot activation meeting and announces another scheduled meeting for next week, plus it describes recent progress in discreet log contracts and a new mailing list for discussing them. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, descriptions of releases and release candidates, and a list of notable changes to popular Bitcoin infrastructure projects. Feb 3, 2021 This week’s newsletter links to a blog post about how a small change to the Script language after taproot activation could enable increased contract flexibility and includes our regular sections with notable changes to popular Bitcoin infrastructure projects. Jan 27, 2021 This week’s newsletter announces a meeting to discuss taproot activation mechanisms, includes a link to a Bitcoin Core usage survey, and includes our regular sections with top questions and answers from the Bitcoin Stack Exchange, a list of releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Jan 20, 2021 This week’s newsletter summarizes posts to the Bitcoin-Dev mailing list about payjoin adoption and making hardware wallets compatible with more advanced Bitcoin features. Also included are our regular sections with overviews of changes to services and client software, new releases and release candidates, and changes to popular Bitcoin infrastructure software. Jan 13, 2021 This week’s newsletter describes a new proposed Bitcoin P2P protocol message, a BIP for the bech32 modified address format, and an idea for preventing UTXO probing in proposed dual-funded LN channels. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, a list of releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. Jan 6, 2021 This week’s newsletter describes a proposed update to BIP322 generic message signing and links"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 17,
    "text": "to a specification for LN trampoline routing. Also included are our regular sections with announcements of releases, release candidates, and notable changes to popular Bitcoin infrastructure software. Dec 23, 2020 This special edition of the Optech Newsletter summarizes notable developments in Bitcoin during all of 2020. Dec 16, 2020 This week’s newsletter describes two suggested improvements to LN static backups and links to a proposal for a new version of PSBTs. Also included are our regular sections with summaries of changes to services and client software, popular questions and answers from the Bitcoin Stack Exchange, new releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Dec 9, 2020 This week’s newsletter describes a proposed change to the bech32 address format for taproot, mentions a bug handling certain QR-encoded addresses, thanks new members of the Cryptocurrency Open Patent Alliance, and mentions new features of the Minsc policy language and compiler. Also included are our regular sections with summaries of a recent Bitcoin Core PR Review Club meeting, new software releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Dec 2, 2020 This week’s newsletter describes a proposal for using fidelity bonds on LN to prevent denial of service attacks, summarizes a PR to address a fee siphoning attack that could affect LN channels using anchor outputs, and links to a proposed specification for miniscript. Also included are our regular sections with releases, release candidates, and recent code changes in popular Bitcoin infrastructure software. Nov 25, 2020 This week’s newsletter links to a website that tracks miner support for taproot activation, announces a new organization funding Bitcoin research and development, and includes our regular sections with popular questions and answers from the Bitcoin Stack Exchange, announcements of releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Nov 18, 2020 This week’s newsletter contains a warning about backdoored VM images. Also included are our regular sections with summaries of notable improvements to clients and services, announcements of releases and release candidates, and changes to popular Bitcoin infrastructure software. Nov 11, 2020 This week’s newsletter shares the announcement of a marketplace for incoming LN channels. Also included are our regular sections with summaries of a Bitcoin Core PR Review Club meeting and notable changes to popular Bitcoin infrastructure software. Nov 4, 2020 This week’s newsletter summarizes a discussion about bi-directional upfront fees for LN and relays the results of a small survey among experts about their preferences for taproot activation. Also included are our regular sections with updates about various projects. Oct 28, 2020 This week’s newsletter describes the disclosure of two vulnerabilities in LND and includes our regular sections with summaries of popular questions and answers from the Bitcoin Stack Exchange, announcements of releases and release candidates, and descriptions of changes to popular Bitcoin infrastructure software. Oct 21, 2020 This week’s newsletter provides an overview of the new MuSig2 paper, summarizes additional discussion about upfront fees in LN, and describes a proposal to simplify management of LN payments. Also included are our regular sections with summaries of notable improvements to clients and services, announcements of releases and release candidates, and changes to popular Bitcoin infrastructure software. Oct 14, 2020 This week’s newsletter relays a security warning for LND users, summarizes discussion about LN upfront payments, describes a mailing list thread about updating bech32 addresses for taproot, and links to an updated proposal for an alternative way to secure LN payments. Also included are our regular sections with summaries of a Bitcoin Core PR Review Club meeting, releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Oct 7, 2020 This week’s newsletter describes a revised proposal for a generic message signing protocol. Also included are our regular sections describing releases, release candidates, and notable changes to popular Bitcoin infrastructure software. Sep 30, 2020 This week’s newsletter describes a compiler bug that casts doubt on the safety of secure systems and explains a technique that can be used to more efficiently verify ECDSA signatures in Bitcoin. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, announcements of releases and release candidates, and summaries of notable changes to popular Bitcoin infrastructure software. Sep 23, 2020 This week’s newsletter describes a proposed soft fork to enable a new type of fee bumping and summarizes research into scripts that can never be spent because they require satisfying both timelocks and heightlocks. Also included are our regular sections with summaries of updates to services and client software, a list of new releases and release candidates, and changes to popular Bitcoin infrastructure software. Sep 16, 2020 This week’s newsletter recommends checking for nodes vulnerable to the InvDoS attack, briefly describes that attack, summarizes another attack against LN channels, and links to"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 18,
    "text": "the announcement of the Crypto Open Patent Alliance. Also included are our regular sections with releases, release candidates, and notable changes to popular Bitcoin infrastructure projects. Sep 9, 2020 This week’s newsletter describes continued discussion about a protocol for making routable coinswaps and includes our regular sections summarizing a Bitcoin Core PR Review Club meeting and notable changes to popular Bitcoin infrastructure projects. Sep 2, 2020 This week’s newsletter describes a proposed change to the way LN commitment transactions are constructed, summarizes discussion of a default signet, and links to a proposal for standardizing temporarily trusted LN channels. Also included are our regular sections with recently transcribed talks and conversations, releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Aug 26, 2020 This week’s newsletter links to a discussion about routed coinswaps and includes our regular sections with summaries of questions and answers from the Bitcoin Stack Exchange, releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Aug 19, 2020 This week’s newsletter describes a possible update to the BIP340 specification of schnorr signatures and a new proposed BIP that specifies how Bitcoin nodes should handle P2P protocol feature negotiation in a forward compatible way. Also included are our regular sections with notable changes to services and client software, releases and release candidates, and changes to popular Bitcoin infrastructure software. Aug 12, 2020 This week’s newsletter summarizes a discussion about SIGHASH_ANYPREVOUT and eltoo, includes a field report showing how 57,000 BTC could have been saved in transaction fees using segwit and batching, and provides our regular sections with the summary of a Bitcoin Core PR Review Club meeting, releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Aug 5, 2020 This week’s newsletter describes the new Minsc policy language and contains our regular sections with recently transcribed talks and conversations, releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Jul 29, 2020 This week’s newsletter describes a proposal to allow upgrading LN channel commitment transaction formats without opening new channels and includes a field report from River Financial about building wallet software using PSBTs and descriptors. Also included are our regular sections with selected questions and answers from the Bitcoin Stack Exchange, recent releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Jul 22, 2020 This week’s newsletter links to several discussions about activating taproot and summarizes a proposed update to BIP173 bech32 addresses. Also included are our regular sections summarizing interesting changes to services and client software, releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Jul 15, 2020 This week’s newsletter describes a proposed update to the draft BIP118 SIGHASH_NOINPUT and summarizes notable changes to popular Bitcoin infrastructure projects. Jul 8, 2020 This week’s newsletter summarizes a proposed BIP for BIP32-based path templates and includes our regular sections with the summary of a Bitcoin Core PR Review Club meeting, releases and release candidates, and notable changes to popular Bitcoin infrastructure software. Jul 1, 2020 This week’s newsletter summarizes a discussion about mining incentives related to HTLCs and links to an announcement about a proposed service to store and relay presigned transactions. Also included are our regular sections with recently transcribed talks and conversations, new releases and release candidates, and notable changes to popular Bitcoin infrastructure projects. Jun 24, 2020 This week’s newsletter summarizes a newly published fee ransom attack against LN users, links to continued discussion about the attack against LN atomicity, and shares a reminder about collision attacks on RIPEMD160-based addresses in multiparty protocols. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, a list of releases and release candidates published this week, and notable changes to popular Bitcoin infrastructure projects. Jun 17, 2020 This week’s newsletter summarizes the CoinPool payment pool proposal and the WabiSabi coordinated coinjoin protocol. Also included are our regular sections with notable changes to services, client software, and infrastructure software. Jun 10, 2020 This week’s newsletter links to a blog post and paper about using eclipse attacks against Bitcoin software in order to steal from LN channels and describes the history and impact of a fee overpayment attack that affects hardware wallets. Also included are our regular sections with the summary of a Bitcoin Core PR Review Club meeting, descriptions of recent software releases, and summaries of recent changes to popular Bitcoin infrastructure projects. Jun 3, 2020 This week’s newsletter summarizes a proposed design for a coinswap implementation, describes new middleware for allowing lightweight wallets to request information directly from a user’s own node, and highlights two transaction size calculators. Also included are our regular sections with descriptions of several recently transcribed talks, new releases and release candidates, and notable changes to popular Bitcoin infrastructure software. A"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 19,
    "text": "special final section celebrates the publication of Newsletter #100. May 27, 2020 This week’s newsletter summarizes a discussion about the minimum allowed transaction size and includes our regular sections with popular questions and answers from the Bitcoin Stack Exchange, releases and release candidates, and notable merges from Bitcoin infrastructure projects. May 20, 2020 This week’s newsletter relays a request for comments on a proposed change to the BIP341 taproot transaction digest and briefly summarizes discussion about a new and more concise protocol for atomic swaps. Also included are our regular sections describing changes to services and client software, new releases and release candidates, and notable changes to popular Bitcoin infrastructure software. May 13, 2020 This week’s newsletter describes a proposal that taproot signatures make an additional commitment to spent scriptPubKeys and includes our regular sections with the summary of a Bitcoin Core PR Review Club meeting, a list of releases and release candidates, and descriptions of notable changes to popular Bitcoin infrastructure software. May 6, 2020 This week’s newsletter links to a discussion about using enhanced QR codes for communicating large transactions, includes a field report from Suredbits about building a high-availability LN node, and briefly summarizes several recently transcribed talks and conversations. Also included are our regular sections with releases, release candidates, and notable code changes from popular Bitcoin infrastructure software. Apr 29, 2020 This week’s newsletter summarizes the disclosure of an issue affecting the safety of routed LN payments and announces a new presigned vault proposal. Also included are our regular sections with popular questions and answers from the Bitcoin Stack Exchange, announcements of releases and release candidates, and descriptions of notable code changes in popular Bitcoin infrastructure projects. Apr 22, 2020 This week’s newsletter links to a prototype for creating vaults using pre-signed transactions and includes our regular sections about notable changes to services, client software, and popular Bitcoin infrastructure projects. Apr 15, 2020 This week’s newsletter summarizes a proposal for creating a unified multi-wallet backup that circumvents the inability to import BIP32 extended private keys into many wallets that support deterministic key derivation. Also included are our regular sections describing release candidates and changes to popular Bitcoin infrastructure software. Apr 8, 2020 This week’s newsletter describes work on simplified ECDSA adaptor signatures and includes our regular sections for Bitcoin Core PR Review Club discussion summaries, release announcements, and notable changes to popular Bitcoin infrastructure projects. Apr 1, 2020 This week’s newsletter describes a proposal to make statechains deployable on Bitcoin without consensus changes, summarizes a discussion about a schnorr nonce generation function that helps protect against differential power analysis, and links to a proposed update to BIP322 generic signmessage . Also included is our regular section about notable changes to popular Bitcoin infrastructure projects. Mar 25, 2020 This week’s newsletter summarizes several questions and answers from the Bitcoin Stack Exchange and describes notable changes to popular Bitcoin infrastructure projects. Mar 18, 2020 This week’s newsletter summarizes an update to a proposed standard for LN and includes our regular sections about notable changes to services, client software, and popular Bitcoin infrastructure projects. Mar 11, 2020 This week’s newsletter links to a description of methods for preventing hardware wallets from leaking private information through transaction signatures, provides an update on the BIP322 generic signmessage protocol, and summarizes a recent meeting of the Bitcoin Core PR Review Club. Also included are our regular sections about new releases and notable merges of popular Bitcoin infrastructure projects. Mar 4, 2020 This week’s newsletter describes a proposed update to BIP340 schnorr keys and signatures, seeks feedback on a proposal to improve startup feature negotiation between full nodes, examines a suggestion for a standardized way to prevent hardware wallets from using corrupt nonces to leak private keys, and links to an analysis of the properties necessary in a hash function for taproot to be secure. Also included are our regular sections for release announcements and notable changes to popular Bitcoin infrastructure projects. Feb 26, 2020 This week’s newsletter announces the 2020 Chaincode Residency program, describes two proposed routing improvements for LN, summarizes three interesting talks from the Stanford Blockchain Conference, links to popular questions and answers from the Bitcoin Stack Exchange, and lists several notable changes to popular Bitcoin infrastructure software. Feb 19, 2020 This week’s newsletter announces the release of C-Lightning 0.8.1, requests help testing a Bitcoin Core maintenance release, summarizes a discussion about taproot versus implementing MAST and schnorr signatures separately, describes new ideas for using PoDLEs in LN channel construction, and highlights a new implication of work on privacy-enhanced payments to unannounced LN channels. Also included are our regular sections about notable changes to popular services, client software, and infrastructure projects. Feb 12, 2020 This week’s newsletter seeks help testing a Bitcoin Core release candidate and summarizes some discussion about"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 20,
    "text": "the BIP119 OP_CHECKTEMPLATEVERIFY proposal. Also included is our regular section about notable code and documentation changes. Feb 5, 2020 This week’s newsletter announces the release of Eclair 0.3.3, requests help testing a Bitcoin Core maintenance release, links to a new tool for experimenting with taproot and tapscript, summarizes discussion about safely generating schnorr signatures with precomputed public keys, and describes a proposal for interactive construction of LN funding transactions. Also included is our regular section about notable changes to popular Bitcoin infrastructure projects. Jan 29, 2020 This week’s newsletter announces the release of LND 0.9.0-beta, asks for help testing a release candidate for a Bitcoin Core maintenance release, describes a proposal to break the linkability between UTXOs and unannounced LN channels, and summarizes a modification to the proposed SIGHASH_ANYPREVOUTANYSCRIPT signature hash that may simplify management of payments in eltoo-based payment channels. Also included are our regular sections for popular Bitcoin Stack Exchange Q&A and notable changes to popular Bitcoin infrastructure and documentation projects. Jan 22, 2020 This week’s newsletter requests help testing a pre-release of the next major version of LND, seeks review of a method for sending payments as part of a chaumian coinjoin mix, links to a work-in-progress protocol specification for discreet log contracts, and includes our regular sections about notable changes to popular services, client software, and infrastructure projects. Jan 15, 2020 This week’s newsletter requests help testing the next version of LND, summarizes a discussion about soft fork activation mechanisms, and describes a few notable changes to popular Bitcoin infrastructure software. Jan 8, 2020 This week’s newsletter summarizes the final week of the organized taproot review, describes a discussion about coinjoin mixing without either equal value inputs or outputs, and mentions a proposal to encode output script descriptors in end-user interfaces. Also included is our regular section about notable changes to popular Bitcoin infrastructure projects. Dec 28, 2019 This special edition of the Optech Newsletter summarizes notable developments in Bitcoin during all of 2019. Dec 18, 2019 This week’s newsletter announces the release of LND 0.8.2-beta, requests help testing the latest C-Lightning release candidate, discusses widespread support for basic multipath payments in LN, provides an update on bech32 error detection reliability, summarizes updates to the proposed OP_CHECKTEMPLATEVERIFY opcode, and links to a discussion about the impact of eclipse attacks on LN channels. Also included are our regular sections about notable changes to popular Bitcoin infrastructure projects, changes to services and client software, and popular Bitcoin Stack Exchange questions and answers. Dec 11, 2019 This week’s newsletter announces a new maintenance release of LND, summarizes a discussion about watchtowers for eltoo payment channels, and describes several notable changes to popular Bitcoin infrastructure projects. Dec 4, 2019 This week’s newsletter describes some recent discussion about the schnorr and taproot proposals, notes the recent update of the proposal formerly known as OP_CHECKOUTPUTSHASHVERIFY and OP_SECURETHEBAG , links to a proposal to standardize LN watchtowers, and summarizes notable changes to popular Bitcoin infrastructure projects. Nov 27, 2019 This week’s newsletter announces a new major version of Bitcoin Core, provides some updates on Bitcoin and LN developer mailing lists, and describes recent developments in the ongoing schnorr/taproot review. Also included are our regular sections with top-voted questions and answers from the Bitcoin Stack Exchange and notable changes to popular Bitcoin infrastructure projects. Nov 20, 2019 This week’s newsletter announces a new minor version release of LND, notes downtime on the development mailing lists, describes some recent updates to Bitcoin services and clients, and summarizes recent changes to popular Bitcoin infrastructure projects. Nov 13, 2019 This week’s newsletter announces a security disclosure affecting some older releases of Bitcoin Core, describes new developments related to taproot, mentions a potential privacy leak related to the LN payment data format, and describes two proposed changes to the LN specification that are under discussion. Also included is our regular section describing notable changes to popular Bitcoin infrastructure projects. Nov 6, 2019 This week’s newsletter requests help testing a Bitcoin Core release candidate, summarizes continued discussion of LN anchor outputs, and describes a proposal for allowing full nodes and lightweight clients to signal support for IP address relay. Also included is our regular section about notable changes to popular Bitcoin infrastructure projects. Oct 30, 2019 This week’s newsletter announces the latest C-Lightning release, requests help testing a Bitcoin Core release candidate, describes discussions about simplified LN commitments using CPFP carve-out, and summarizes several top-voted questions and answers from the Bitcoin Stack Exchange. Oct 23, 2019 This week’s newsletter requests testing of the C-Lightning and Bitcoin Core release candidates, invites participation in structured review of the taproot proposal, highlights updates to two Bitcoin wallets, and describes a few notable changes to popular Bitcoin infrastructure projects. Oct 16, 2019 This week’s newsletter announces the newest release of LND, requests testing"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 21,
    "text": "of release candidates for Bitcoin Core and C-Lightning, relays an update on the taproot proposal, describes a proposed increase in the default LN routing fees, and summarizes three talks from the recent Cryptoeconomic Systems Summit. Also included is our regular section on notable changes to popular Bitcoin infrastructure projects. Oct 9, 2019 This week’s newsletter requests help testing release candidates for Bitcoin Core and LND, tracks continued discussion about the proposed noinput and anyprevout sighash flags, and describes several notable changes to popular Bitcoin infrastructure projects. Oct 2, 2019 This week’s newsletter notes a proposed BIP that would allow nodes to implement the Erlay transaction relay protocol, announces full disclosure of vulnerabilities that affected earlier versions of LN implementations, links to a transcript from a recent Optech schnorr and taproot workshop, and includes a field report about some of the technology Bitcoin exchange BTSE uses to conserve block chain space while ensuring the safety of user deposits. We also describe several notable changes to popular Bitcoin infrastructure projects. Sep 25, 2019 This week’s newsletter requests feedback on not allowing P2SH-wrapped addresses for taproot, describes proposed changes to script resource limits in tapscript, and mentions a discussion about watchtower storage costs. Also included are a selection of popular questions and answers from the Bitcoin Stack Exchange and a short list of notable changes to popular Bitcoin infrastructure projects. Sep 18, 2019 This week’s newsletter summarizes several talks from the Bitcoin Edge Dev++ training sessions and Scaling Bitcoin conference held last week in Tel Aviv. Also included is our regular section on notable changes to popular Bitcoin infrastructure projects. Sep 11, 2019 This week’s newsletter describes a demo implementation of eltoo and several discussions related to it, requests comments on limiting the normal number of LN gossip updates to one per day, and provides our longest-to-date list of notable changes to popular Bitcoin infrastructure projects. Sep 4, 2019 This week’s newsletter relays a security announcement for LN implementations, describes a non-interactive coinjoin proposal, and notes a few changes in popular Bitcoin infrastructure projects. Aug 28, 2019 This week’s newsletter asks for comments on the miniscript language, publishes our final bech32 sending support section, includes popular Q&A from the Bitcoin Stack Exchange, and describes several notable changes to popular Bitcoin infrastructure projects. Aug 21, 2019 This week’s newsletter notes a change to Bitcoin Core’s consensus logic and announces a new feature on the Optech website for tracking technology adoption between different wallets and services. Also included are our regular sections about bech32 sending support and notable changes in popular Bitcoin infrastructure projects. Aug 14, 2019 This week’s newsletter briefly describes two discussions on the Bitcoin-Dev mailing list, one about Bitcoin vaults and one about reducing the size of public keys used in taproot. Also included are our regular sections about bech32 sending support and notable changes in popular Bitcoin infrastructure projects. Aug 7, 2019 This week’s newsletter announces the maintenance release of Bitcoin Core 0.18.1, summarizes a discussion about BIP174 extensions, and notes a proposal for LN trampoline payments. Our bech32 sending support section this week features a special case study contributed by BRD and our notable changes section highlights several possibly significant developments. Jul 31, 2019 This week’s newsletter describes fidelity bonds for JoinMarket-style decentralized coinjoin, mentions a PR for BIP322 signmessage support (including the ability to sign for bech32 addresses), and summarizes a discussion about bloom filters. Also included are our regular sections about bech32 sending support, popular Q&A from the Bitcoin Stack Exchange, and notable changes to popular Bitcoin infrastructure projects. Jul 24, 2019 This week’s newsletter describes progress on signet and an idea for just-in-time routing on LN. Also included are our regular sections about bech32 sending support and notable changes to popular Bitcoin infrastructure projects. Jul 17, 2019 This week’s newsletter summarizes a proposal for an update to the LN gossip protocol. Also included are our regular sections on bech32 sending support and notable changes in popular Bitcoin infrastructure projects. Jul 10, 2019 This week’s newsletter announces the release of the newest version of Eclair and describes a potential routing improvement for LN. Also included are our regular sections about bech32 sending support and notable code changes in popular Bitcoin infrastructure projects. Jul 3, 2019 This week’s newsletter announces the newest release of C-Lightning, briefly describes several proposals related to LN, and provides our usual sections about bech32 sending support and notable changes to popular Bitcoin infrastructure projects. Jun 26, 2019 This week’s newsletter announces a pending disclosure of minor vulnerabilities for older Bitcoin Core releases, suggests continued testing of RCs for LN software, and describes a proposed technique to make Bitcoin Core nodes a bit more resistant to eclipse attacks. Also included are our regular sections on bech32 sending support, popular Stack Exchange topics, and notable changes"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 22,
    "text": "to popular Bitcoin infrastructure projects. Jun 19, 2019 This week’s newsletter requests testing on RCs for both LND and C-Lightning, describes using ECDH for uncoordinated LN payments, summarizes a proposal to add information about delays to LN routing replies, and includes summaries of some interesting talks from the recent ‘Breaking Bitcoin’ conference in Amsterdam. Also included are our regular sections on bech32 sending support and notable changes in popular Bitcoin infrastructure projects. Jun 12, 2019 This week’s newsletter summarizes meetings from the CoreDev.tech event, describes a proposed amendment to BIP125 replace-by-fee, links to an Optech executive briefing video about Schnorr/Taproot, and briefly celebrates the 50th weekly issue of the Optech Newsletter. Also included are our regular sections on bech32 sending support and notable changes to popular Bitcoin infrastructure projects. Jun 5, 2019 This week’s newsletter describes the proposed Erlay protocol that could significantly reduce the overhead of relaying unconfirmed transactions between nodes, summarizes an executive briefing by Bitrefill CEO Sergej Kotliar about LN, and lists some recent changes to the COSHV proposal described last week. Also included are our regular sections on bech32 sending support and notable code changes in popular Bitcoin infrastructure projects. May 29, 2019 This week’s newsletter describes the new proposed OP_CHECKOUTPUTSHASHVERIFY opcode, covers continued discussion of Taproot, and links to a video presentation about handling increasing Bitcoin transaction fees. Also included are our regular sections on bech32 sending support, top-voted Bitcoin Stack Exchange questions and answers, and notable changes in popular Bitcoin infrastructure projects. May 21, 2019 This week’s newsletter describes the bip-anyprevout soft fork proposal, summarizes a few technical talks from the Magical Crypto Friends conference, and includes our regular sections on bech32 sending support and notable changes to popular Bitcoin infrastructure projects. May 14, 2019 This week’s newsletter includes a special section about the recent Taproot proposal, news about a small potential change to the BIP174 PSBT format, and our regular sections about bech32 sending support and notable changes in popular infrastructure projects. May 7, 2019 This week’s newsletter highlights some of the changes in the recently-released Bitcoin Core 0.18.0, briefly mentions two proposed BIPs, describes how bech32 addresses are forward compatible with expected protocol improvements, and summarizes notable changes in popular Bitcoin infrastructure projects. Apr 30, 2019 This week’s newsletter sees another slow news week but does contain our regular sections on bech32 sending support, selected questions and answers from the Bitcoin Stack Exchange, and notable changes in popular Bitcoin infrastructure projects. Apr 23, 2019 This week’s newsletter announces the release of LND 0.6-beta and the merge of BIP158 support into Bitcoin Core’s development branch. Also included are the regular sections about bech32 sending support and notable changes to popular Bitcoin infrastructure projects. Apr 16, 2019 This week’s newsletter requests testing of the latest release candidates for Bitcoin Core and LND, describes how helping people accept payments to bech32 addresses can lower fees, and lists notable code changes in popular Bitcoin projects. Apr 9, 2019 This week’s newsletter requests testing for release candidates of Bitcoin Core and LND, describes a discussion about UTXO snapshots for fast initial syncing of nodes, and provides regular coverage of bech32 sending support and notable merges in popular Bitcoin infrastructure projects. Apr 2, 2019 This week’s newsletter notes a spike in estimated transaction fees, describes LN trampoline payments, and publicizes Bitcoin Core’s intent to default its built-in wallet to bech32 receiving addresses in version 0.20 or earlier. Also included are regular sections about bech32 sending support and notable code changes in popular Bitcoin infrastructure projects. Mar 26, 2019 This week’s newsletter links to a proposal to encrypt P2P communication and describes Lightning Loop, a tool and service for withdrawing bitcoins from an LN channel to an onchain transaction. Also included are links to resources about bech32 adoption, summaries of popular questions and answers from Bitcoin Stack Exchange, and a list of notable code changes in popular Bitcoin infrastructure projects. Mar 19, 2019 This week’s newsletter gives an update on the planned removal of BIP61 reject messages from Bitcoin Core, links to further discussion about SIGHASH_NOINPUT_UNSAFE, analyzes some new features in the Esplora block chain explorer, provides information about an updated node ban list, and links to videos of talks at the recent MIT Bitcoin Club Expo. Also provided are a new weekly section about adoption of bech32 sending support and the normal list of notable changes in popular Bitcoin infrastructure projects. Mar 12, 2019 This week’s newsletter notes a vulnerability in Bitcoin Core versions that are already past end-of-life, asks for help testing release candidates of the next major version of Bitcoin Core, provides an update on the Bitcoin-Dev mailing list, describes recent discussions from the list, and links to a chapter about payment batching in Optech’s work-in-progress scaling techniques book. Also included are descriptions of several notable"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 23,
    "text": "commits in popular Bitcoin infrastructure projects. Mar 5, 2019 This week’s newsletter links to the announcement of the C-Lightning 0.7 upgrade, notes a service outage for the Bitcoin-Dev mailing list, and describes a proposed soft fork to eliminate several old problems in the Bitcoin consensus protocol. Also included are summaries of notable commits in popular Bitcoin infrastructure projects. Feb 26, 2019 This week’s newsletter describes the availability of a libsecp256k1 fork implementing BIP-Schnorr compatible signatures, lists popular questions and answers for February from the Bitcoin Stack Exchange, and describes notable merges in popular Bitcoin infrastructure projects. Feb 19, 2019 This week’s newsletter summarizes a discussion about output tagging for BIP118 SIGHASH_NOINPUT_UNSAFE, announces merges that will make it possible to pair Bitcoin Core’s built-in wallet in watching-only mode with a hardware wallet, and describes the completion of the feature freeze for the next Bitcoin Core release. Also summarized are numerous code and documentation changes in popular Bitcoin infrastructure projects. Feb 12, 2019 This week’s newsletter announces the newest version of LND, briefly describes a tool for generating bitcoin ownership proofs, and links to an Optech study about the usability of Replace-by-Fee. Also included are summaries of notable code changes to popular Bitcoin infrastructure projects. Feb 5, 2019 This week’s newsletter includes an announcement of the 2019 Chaincode Residency program, summarizes a few talks from the Stanford Blockchain Conference, and provides the usual list of notable code changes in popular Bitcoin infrastructure projects. Jan 29, 2019 This week’s newsletter summarizes a post about the privacy-improving payjoin proposal, links to top-voted questions and answers from Bitcoin Stack Exchange, and describes another busy week worth of notable commits in popular Bitcoin infrastructure projects. Jan 22, 2019 This week’s newsletter describes a proposed LN feature to allow making spontaneous payments and provides our longest-ever list of notable code changes to popular Bitcoin infrastructure projects. Jan 15, 2019 This week’s newsletter announces a security upgrade for C-Lightning, describes a paper and additional research into wallets that accidentally revealed their private keys, and lists some notable code changes in popular Bitcoin infrastructure projects. Jan 8, 2019 This week’s newsletter announces a new maintenance release of Bitcoin Core, describes continued discussion about new signature hashes, and links to a post about possible economic barriers to LN payments crossing different currencies. Descriptions of notable code changes in popular Bitcoin infrastructure projects are also provided. Dec 28, 2018 This week’s newsletter is a special year-end edition summarizing notable developments in Bitcoin during all of 2018. Despite the extended length of this newsletter, we regret that it only covers a tiny fraction of the work put into dozens of open source projects by hundreds of contributors. Without those low-level contributions, the high-concept ideas described in this newsletter would be just empty words, and so we extend our most sincere thanks to all of you who contributed to Bitcoin development this year. Dec 18, 2018 This week’s newsletter describes the new libminisketch library for bandwidth efficient set reconciliation, links to an email about Schnorr/Taproot plans, and mentions an upcoming LN protocol specification meeting. Also included are a list of notable code changes in the past week from popular Bitcoin infrastructure projects. Dec 11, 2018 This week’s newsletter suggests helping test a Bitcoin Core maintenance release candidate, provides a link to a modern block explorer whose code has been open sourced, and briefly describes a suggestion for signature hashes to optionally cover transaction size. Notable code changes made in the past week to popular infrastructure projects are also described. Dec 4, 2018 This week’s newsletter describes a proposal to tweak Bitcoin Core’s relay policy for related transactions to help simplify onchain fees for LN payments, mentions upcoming meetings about the LN protocol, and briefly describes a new LND release and work towards a Bitcoin Core maintenance release. Nov 27, 2018 This week’s newsletter provides a reminder about potential feerate increases, summarizes suggested improvements to sighash flags to accompany BIP118 SIGHASH_NOINPUT_UNSAFE , and briefly describes a proposal to simplify fee bumping for LN commitment transactions. Also included are selected recent Q&A from Bitcoin Stack Exchange and descriptions of notable code changes in popular Bitcoin infrastructure projects. Nov 20, 2018 This week’s “wumbo-sized” newsletter provides a note about Bitcoin hash rate related to forks on other coins, summarizes several accepted goals for the Lightning Network protocol version 1.1 specification, and lists several notable commits in popular Bitcoin infrastructure projects. Nov 13, 2018 This week’s newsletter summarizes a few discussions on the Lightning-Dev mailing list, suggests an opportunity to develop a new tool some users would find helpful, and provides summaries and links to some of the talks at the recent Chaincode Lightning Applications residency. Several notable code changes in popular Bitcoin infrastructure projects are also described. Nov 6, 2018 This week’s newsletter contains a security"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 24,
    "text": "notice about the C implementation of bech32 address decoding, an analysis of a temporary reduction in the number of segwit blocks, a link to an interesting discussion about future features for LN payments, and a few notable code changes in popular Bitcoin infrastructure projects. Oct 30, 2018 This week’s newsletter suggests an update for C-Lightning users, describes a discussion about BIP69 deterministic input/output ordering on the mailing list, notes public overt ASICBoost support is available for miners using Antminer S9, and provides links to resources about both Square’s open sourced Subzero HSM-based multisig cold storage solution and the recent Lightning Network Residency and Hackday in New York City. Also included are selected recent Q&A from Bitcoin Stack Exchange and descriptions of notable code changes in popular Bitcoin infrastructure projects. Oct 23, 2018 This week’s newsletter contains a warning about communicating with Bitcoin nodes using RPC over unencrypted connections, links to two new papers about creating fast multiparty ECDSA keys and signatures that could reduce transaction fees for multisig users, and lists some notable merges from popular Bitcoin infrastructure projects. Oct 16, 2018 This week’s newsletter briefly describes a proposal for splicing Lightning Network payment channels, links to videos and transcripts of talks from the Edge Dev++ training sessions, and summarizes some transcripts made during last week’s CoreDev.tech event. Oct 9, 2018 This week’s newsletter consists entirely of summaries of several notable talks presented at the Scaling Bitcoin workshop last weekend, since there was very little to report in our usual Action Items, News, and Notable Code Changes sections. We hope to return to our usual format next week. Oct 2, 2018 This week’s newsletter includes a notice of Bitcoin Core 0.17’s impending release, links to the backport releases of Bitcoin Core 0.15 and 0.14 to fix the CVE-2018-17144 duplicate inputs bug for those users unable to run more recent releases, a brief description of a chainsplit on testnet, and links to notable merges in Bitcoin infrastructure projects. Sep 25, 2018 This week’s newsletter includes action items and news related to last week’s security release of Bitcoin Core 0.16.3 and Bitcoin Core 0.17RC4, popular questions and answers from the Bitcoin Stack Exchange over the past month, and short descriptions of notable merges made to popular Bitcoin infrastructure projects. Sep 18, 2018 This week’s newsletter includes action items related to the security release of Bitcoin Core 0.16.3 and Bitcoin Core 0.17RC4, the newly-proposed BIP322, and Optech’s upcoming Paris workshop; a link to the C-Lightning 0.6.1 release, more information about BIP322, and some details about the Bustapay proposal; plus brief descriptions of notable merges in popular Bitcoin infrastructure projects. Sep 11, 2018 This week’s newsletter references a discussion about BIP151 encryption for the peer-to-peer network protocol, provides an update on compatibility between Bitcoin and the W3C Web Payments draft specification, and briefly describes some notable merges in popular Bitcoin infrastructure projects. Sep 4, 2018 This week’s newsletter includes a reminder to please help test the release candidate for Bitcoin Core’s next version, information about the development of Optech’s new public dashboard, summaries of two discussions on the Bitcoin-Dev mailing list, and notable commits from Bitcoin infrastructure projects. Aug 28, 2018 This week’s newsletter includes information about the first published release candidate for Bitcoin Core, news about BIP151 P2P protocol encryption and a potential future soft fork, top questions and answers from Bitcoin Stack Exchange, and some notable merges in popular Bitcoin infrastructure projects. Aug 21, 2018 This week’s newsletter includes a request to help test the next version of Bitcoin Core, short descriptions of projects Bitcoin Core contributors are working on, and a list of notable merges during the past week. Aug 14, 2018 This week’s newsletter includes the usual dashboard and action items, news about the importance of allowing secure and anonymous responsible disclosure of bugs, a potential new payment protocol that can improve privacy on Bitcoin without any consensus rule changes, shaving one byte off the size of every transaction signature, a new restriction on the P2P network protocol, and lowering the minimum transaction relay fee—plus a few notable commits from the Bitcoin Core, LND, and C-Lightning projects. Aug 7, 2018 This week’s newsletter includes the usual dashboard and action items, a link to discussion about generalized Bitcoin contracts over Lightning Network, a brief description of a recently-announced library for scalability-enhancing BLS signatures, and some notable commits from the Bitcoin Core, LND, and C-Lightning projects. Jul 31, 2018 This week’s newsletter includes the usual dashboard and action items, a feature article by developer Anthony Towns about the consolidation of four million UTXOs at Xapo, news about possible upgrades to Bitcoin’s script system, links to a few highly-voted questions and answers on the Bitcoin Stack Exchange, and some notable commits in the development branches of the Bitcoin Core, Lightning Network Daemon (LND),"
  },
  {
    "type": "doc",
    "url": "https://bitcoinops.org/en/newsletters/",
    "title": "Newsletters | Bitcoin Optech",
    "chunk": 25,
    "text": "and C-lightning projects. Jul 24, 2018 This week’s newsletter includes information about a new language to describe output scripts, an update on Bitcoin Core’s support for partially signed Bitcoin transactions, and news on several other notable Bitcoin Core merges. Jul 17, 2018 This week’s newsletter includes news and action items about a feature freeze for the next major version of Bitcoin Core, increasing transaction fees, a likely renaming of the proposed SIGHASH_NOINPUT flag, and several notable recent Bitcoin Core merges. Jul 10, 2018 This week’s newsletter includes news and action items about minimum fees and the upcoming Bitcoin Core release, a special feature on a Schnorr signature proposal, and a write-up of the recent Building on Bitcoin conference in Lisbon. Jul 3, 2018 Continued discussion over graftroot safety, BIP174 Partially Signed Bitcoin Transactions (PSBT) officially marked as proposed, and discussion of Dandelion transaction relay. Jun 26, 2018 Announces a pending vulnerability disclosure for older Bitcoin Core releases, links to a PR about improved coin selection, and discusses dynamic wallet loading and unloading in Bitcoin Core multiwallet mode. Jun 8, 2018 A trial run of the Optech newsletter including news about the `OP_CODESEPARATOR` opcode, BetterHash mining protocol, and BIP157/158 compact block filters."
  },
  {
    "type": "doc",
    "url": "https://developer.bitcoin.org/devguide/",
    "title": "Developer Guides",
    "chunk": 0,
    "text": "Developer Guides Find detailed information about the Bitcoin protocol and related specifications."
  },
  {
    "type": "doc",
    "url": "https://github.com/bitcoin/bips",
    "title": "bitcoin/bips: Bitcoin Improvement Proposals",
    "chunk": 0,
    "text": "You can’t perform that action at this time."
  },
  {
    "type": "doc",
    "url": "https://eips.ethereum.org/",
    "title": "Ethereum Improvement Proposals",
    "chunk": 0,
    "text": "EIPs Ethereum Improvement Proposals (EIPs) describe standards for the Ethereum platform, including core protocol specifications, client APIs, and contract standards. Network upgrades are discussed separately in the Ethereum Project Management repository. Contributing First review EIP-1 . Then clone the repository and add your EIP to it. There is a template EIP here . Then submit a Pull Request to Ethereum's EIPs repository . EIP status terms Idea - An idea that is pre-draft. This is not tracked within the EIP Repository. Draft - The first formally tracked stage of an EIP in development. An EIP is merged by an EIP Editor into the EIP repository when properly formatted. Review - An EIP Author marks an EIP as ready for and requesting Peer Review. Last Call - This is the final review window for an EIP before moving to FINAL. An EIP editor will assign Last Call status and set a review end date (`last-call-deadline`), typically 14 days later. If this period results in necessary normative changes it will revert the EIP to Review. Final - This EIP represents the final standard. A Final EIP exists in a state of finality and should only be updated to correct errata and add non-normative clarifications. Stagnant - Any EIP in Draft or Review if inactive for a period of 6 months or greater is moved to Stagnant. An EIP may be resurrected from this state by Authors or EIP Editors through moving it back to Draft. Withdrawn - The EIP Author(s) have withdrawn the proposed EIP. This state has finality and can no longer be resurrected using this EIP number. If the idea is pursued at later date it is considered a new proposal. Living - A special status for EIPs that are designed to be continually updated and not reach a state of finality. This includes most notably EIP-1. EIP Types EIPs are separated into a number of types, and each has its own list of EIPs. Standards Track (950) Describes any change that affects most or all Ethereum implementations, such as a change to the network protocol, a change in block or transaction validity rules, proposed application standards/conventions, or any change or addition that affects the interoperability of applications using Ethereum. Furthermore Standard EIPs can be broken down into the following categories. Improvements requiring a consensus fork (e.g. EIP-5 , EIP-211 ), as well as changes that are not necessarily consensus critical but may be relevant to “core dev” discussions (for example, the PoA algorithm for testnets described in EIP-225 ). Includes improvements around devp2p ( EIP-8 ) and Light Ethereum Subprotocol, as well as proposed improvements to network protocol specifications of whisper and swarm. Includes improvements around client API/RPC specifications and standards, and also certain language-level standards like method names ( EIP-6 ) and contract ABIs. The label “interface” aligns with the interfaces repo and discussion should primarily occur in that repository before an EIP is submitted to the EIPs repository. Application-level standards and conventions, including contract standards such as token standards ( EIP-20 ), name registries ( EIP-137 ), URI schemes ( EIP-681 ), library/package formats ( EIP-190 ), and account abstraction ( EIP-4337 ). Describes a process surrounding Ethereum or proposes a change to (or an event in) a process. Process EIPs are like Standards Track EIPs but apply to areas other than the Ethereum protocol itself. They may propose an implementation, but not to Ethereum's codebase; they often require community consensus; unlike Informational EIPs, they are more than recommendations, and users are typically not free to ignore them. Examples include procedures, guidelines, changes to the decision-making process, and changes to the tools or environment used in Ethereum development. Any meta-EIP is also considered a Process EIP. Describes a Ethereum design issue, or provides general guidelines or information to the Ethereum community, but does not propose a new feature. Informational EIPs do not necessarily represent Ethereum community consensus or a recommendation, so users and implementers are free to ignore Informational EIPs or follow their advice."
  },
  {
    "type": "doc",
    "url": "https://ethresear.ch/",
    "title": "Ethereum Research",
    "chunk": 0,
    "text": "Read this before posting 0 57701 August 17, 2017 LMD GHOST with ~256 validators and a fast-following finality gadget 10 631 August 11, 2025 Faster block/blob propagation in Ethereum 53 4532 August 10, 2025 HD Wallet for Lattice Cryptography 0 46 August 8, 2025 An Ethereum Prover Market Proposal 6 468 August 7, 2025 Block-level Access Lists (BALs) 12 895 August 7, 2025 Anti-Sniper Defenses: Token Launches & Beyond 0 96 August 6, 2025 Confirmation Rule for Ethereum PoS 9 5610 August 5, 2025 The language design, that on EVM v.s. that on Others 24 6365 August 5, 2025 Secure Agent-Protocol Interactions: A Specification for Canonical EVM MCP Implementations 0 80 August 4, 2025 Overclocking Blocks with Gas Refunds 0 173 August 3, 2025 Does Ethereum have a zk-verifiability problem? 6 284 August 2, 2025 A practical proposal for Multidimensional Gas Metering 20 721 August 1, 2025 The Glamsterdam equation 2 426 August 1, 2025 Generating Pasta keypairs 4 215 July 31, 2025 Ethereum Bytecode and Code Chunk Analysis 0 54 July 31, 2025 Maintaining Effective Blob Fee Markets During Network Scaling: Dual-Variable EIP-1559 0 83 July 31, 2025 Issuance Policy vs. Structural Events in Ethereum Staking — An Analysis 0 125 July 20, 2025 The Signal: Ethereum - Casper FFG Finality Proofs 0 67 July 30, 2025 eODS (Enshrined Operator Delegator Separation): a Delegation model proposal 0 166 July 28, 2025 Confidential Wrapped Ethereum 6 925 July 29, 2025 Geographical Decentralisation 51 7467 July 29, 2025 Tasklist for post-quantum ETH 13 1215 July 27, 2025 Key Insights from a Formal Framework of the Ethereum Staking Market 0 128 July 25, 2025 Incendia: On‑Chain Anonymous Voting via Proof‑of‑Burn 15 320 July 25, 2025 VRFs and Single Secret Leader Election 4 174 July 23, 2025 Fluxe: A Universal Privacy Protocol for Cross-Chain Stablecoin Payments with Programmable Compliance 19 786 July 23, 2025 Distributed Proof Generation 0 137 July 23, 2025 Resolving the Dichotomy: DeFi Compliance under Zero Knowledge 1 457 July 22, 2025 FAIR L1 Chain: On-Chain Decryption via Precompiled Contracts 8 244 July 22, 2025"
  },
  {
    "type": "doc",
    "url": "https://writings.flashbots.net/",
    "title": "Flashbots Writings",
    "chunk": 0,
    "text": "Block building must be made decentralized, uncensorable, and neutral. To accelerate decentralization in block building, Flashbots has deprecated our centralized block builders and migrated our orderflow and refunds to BuilderNet . BuilderNet will make Ethereum more resilient by introducing a neutral way for many parties to share orderflow and collaborate in building blocks. It will also create better user experiences by providing a fair and equal platform to trade, and using verifiable ordering rules and improved privacy techniques to keep users safe. As of December 5 2024, Flashbots no longer operates centralized block builders on Ethereum. A few instances will remain as backups to ensure continuity of service in extraordinary situations during the BuilderNet alpha period. We invite the community to join us in developing BuilderNet, and to accelerate decentralization and neutrality in block building."
  },
  {
    "type": "doc",
    "url": "https://www.stakingrewards.com/",
    "title": "Earn 20% More on Your Staking Returns",
    "chunk": 0,
    "text": "Staking Data API Get instant access to live, unbiased, historical staking data, spanning 90+ assets and 100k+ validators. Get Access"
  },
  {
    "type": "doc",
    "url": "https://docs.dune.com/",
    "title": "Welcome to Dune Docs",
    "chunk": 0,
    "text": "Responses are generated using AI and may contain mistakes."
  },
  {
    "type": "doc",
    "url": "https://insights.deribit.com/",
    "title": "The best Information for Crypto Derivatives trading",
    "chunk": 0,
    "text": "Overall information about our exchange and system announcements about API, Fundingrate, Fees, APIv2, and more."
  },
  {
    "type": "doc",
    "url": "https://www.glassnode.com/reports",
    "title": "Glassnode - On-chain market intelligence",
    "chunk": 0,
    "text": "Subscribe to our newsletter. Receive weekly expert market analysis, best-in-class research on Bitcoin, Ethereum, DeFi, & more."
  },
  {
    "type": "doc",
    "url": "https://www.coinmetrics.io/state-of-the-network/",
    "title": "State of the Network Archives",
    "chunk": 0,
    "text": "In this issue of Coin Metrics’ State of the Network, we release our overview report on Tether (USDT), diving into its adoption,"
  },
  {
    "type": "doc",
    "url": "https://www.oklink.com/academy",
    "title": "OKLink,欧科学院,秒懂区块链",
    "chunk": 0,
    "text": "科普：Eth2验证者如何生成和保护取款密钥 本文来自 以太坊爱好者（ID：ethfans），作者：Jim McDonald。来源：https://mp.weixin.qq.com/s/FP-hDvYT_txfI4rlm5x4jQ 以太坊 2.0 的密钥与以太坊 1.0 的密钥在生成和使用方式上大致相同，但二者并不兼容，也就是说，在以太坊 1.0 上生成的密钥不能在以太坊 2.0 上使用。 取款密钥是什么？ 取款密钥（withdrawal key）是以太坊 2.0 中的验证者用来提现以太币的密钥 1。 以太坊 2.0 的密钥与以太坊 1.0 的密钥在生成和使用方式上大致相同，但二者并不兼容，也就是说，在以太坊 1.0 上生成的密钥不能在以太坊 2.0 上使用。 以太坊 2.0 中的密钥总是以公钥与私钥的形式成对出现。取款密钥由 权益人（staker） 自己持有，因为他们是资金的提供者，当然也想保有撤资的权利。 取款密钥是用来干什么的？ 在以太坊 2.0 中，取款密钥的信息主要用于以下两种情况：在以太坊 1.0 中创建押金存入交易；在以太坊 2.0 中提现以太币。 当用户在以太坊 1.0 上存入押金时，取款公钥的作用是使该笔押金与取款私钥关联起来。这就是为什么以太坊 2.0 能通过取款公钥知道谁有权提款（与该取款公钥对应的取款私钥才有权提取该笔保证金）。取款公钥还用于将数据整合到以太坊的押金存入交易中，如下图所示： – 图一：在存款流程中使用取款公钥（预知详情，请参见这篇文章）（编者注：见文末超链接《验证者的生命流程》）- 要注意的一点是，每个押金要约（deposit agreement）中都要用到取款公钥 2。 在以太坊 2.0 上提现以太币的操作细节还未确定，但无论将来采取怎么样的方式，都需要提款私钥来签名授权。 – 图二：提现操作框架 – 在上图的示例中，权益人使用提款私钥对提现操作的细节进行签名。然后，以太坊 2.0 网络就可以比较提款操作中的签名授权与存款协议中的提款身份标识（withdrawal identification）（如图一所示）。如果两者匹配，提款操作就能进行下去。 直到可以使用提款功能之前，我们都无需用到提款私钥。提款功能可能要等存入押金后一年以上才可以使用。即使功能可用后，也不一定要使用。对于那些想要长期获得奖励的权益人来说，能否提款对他们的日常操作几乎没有影响。 这就意味着，我们应该保护好自己的提款私钥，短期内不需要使用它，即使从长期来看也只需要偶尔使用它。也就是说，在平衡密钥的安全性和可用性时，安全性应该是我们更看重的。 要保护多少个私钥？ 还有一个需要回答的问题是：我们一共需要保护多少个私钥？ 如果你只创建一个验证者身份，那么答案很简单：一个私钥。如果你要创建多个验证者身份，那么答案会变得复杂起来。我们可以为每个验证者身份创建一个不同的提款私钥，但这不是必须的。那么，每个验证者身份的提款私钥应该是唯一的吗？ 使用多个提款私钥的理由主要有两个。第一个原因是，如果不同的验证者身份共享一个密钥，这些验证者身份之间就有了联系：显然，这些账号下的押金同属一个实体。由此，也就很容易计算出使用这个私钥可以访问的资金量，以及该实体持有的以太币总量。不过，使用不同的私钥并不能防止他人从其它渠道获取这些信息，例如，发起这些存款交易的以太坊 1.0 地址同样反映了这些信息。因此，除非我们在使用以太坊 1.0 地址时足够谨慎，否则使用多个提款私钥对安全性的提升不值一提。 第二个原因是，使用同一个私钥会让这个私钥的价值过高，也就更容易遭窃。但是，如果将不同的私钥存放在同一个地方，也会导致相同的问题。也就是说，应该从物理和逻辑上将不同的私钥分开，以减少丢失所带来的影响。 总之，如果你想防止其他人发现多个验证者身份背后的同一个实体（假设你的每笔存款来自不同的以太坊 1.0 地址），并且将你的每个提款私钥分别储存在不同地方，或者采用了不同的密钥保护机制，那么使用多个私钥会带来实质性的好处。由于普通用户一般不会这样操作，本文的余下部分只介绍了单个提款私钥是如何使用的，如有需要，本文内容也将适用于持有多个私钥的情况。 步骤 我们已经了解了基本要求，现在来看看怎么创建并保护取款密钥。创建新密钥的步骤如下： 创建提款钱包； 创建提款账户； 记录取款公钥； 删除提款钱包； 确认提款钱包可以恢复。 我们来看一下图解： – 图三：创建并保护取款密钥的步骤 – 这些步骤必须在电脑上完成，要小心防止电脑被入侵。本文篇幅有限，不展开讨论如何防止电脑遭到入侵，但是用户至少要做到的一点是：在执行上述过程时，电脑不能联网。 创建提款钱包 本文截稿时，尚未开发出基于 BLS12-381 曲线的以太坊 2.0 密钥硬件钱包，也就是说，目前还没有可以储存以太坊 2.0 密钥的硬件钱包（编者注：在本译本出版时好像已经有了）。因此，密钥必须在软件中生成。本文以 ethdo 命令行工具为例，你也可以使用其它工具来实现。 ethdo 采用了 钱包 的概念。一个钱包可以包含一个或多个账户，而且可以从逻辑上将不同账户分隔开来（例如，将提款账户和验证账户分开）。一个账户包括私钥、公钥以及其它一些数据（如，一个好记的账户名），这样就不用直接使用公钥登陆了 3。如果要为提款账户创建钱包，请运行以下代码： ethdo wallet create –wallet=”Staking wallet” –type=hd –walletpassphrase=secret1 这行代码会创建一个带有 助记词 的钱包。助记词由 24 个单词组成，可用来恢复之前创建的钱包和钱包内的所有账户，应该立即保护好。上述命令会输出助记词，应该离线保存好。如果输入命令后并没有显示助记词，说明钱包无法恢复，那么我们不应使用这个钱包。 保存助记词的方法有很多，如 Blockplate 和 Cryptosteel，但你也可以把它抄下来，放在一个安全（最好防火）的地方。记住，一旦助记词丢失，你就无法提款，因此要采取恰当的措施保存好助记词。 助记词保存好后，我们就可以开始创建提款账户了。 创建提款账户 创建提款账户的命令如下： ethdo account create –account=”Staking wallet/Withdrawal account” –walletpassphrase=secret1 –passphrase=secret2 钱包口令（walletpassphrase）必须与你在上一部分提供的口令保持一致。后面一个口令则仅仅是这个账户的口令，只在删除账户前暂时使用。 记录取款公钥 顾名思义，公钥是 公开 的，无需采取特殊的安全保护措施。运行下列代码查看公钥： ethdo account info –account=”Staking wallet/Withdrawal account” 如果该代码没有输出取款公钥，可能说明账户创建过程出错。仔细查看之前运行的代码是否有错。 请注意，虽然公钥不是私密的，但我们应该采取合理措施保证恶意攻击者无法将你的公钥换成他们的公钥。 删除提款钱包 如果上述步骤均正确执行，我们就可以安全地删除这个提款钱包了。请运行下方代码： ethdo wallet delete –wallet=”Staking wallet” 随后，你可以运行下方代码来访问钱包，以确认该钱包是否已经删除： ethdo wallet info –wallet=”Staking wallet” 正常情况下，会返回一条错误消息，原因是无法找到钱包。 确认提款钱包可以恢复 在用取款密钥进行任何存款前，最好先重建提款钱包。这不仅可以保证我们熟悉操作流程，还可以在你放入资金之前最后检查一遍可能出现的错误。 首先使是用助记词重建钱包。请运行下方代码： ethdo wallet create –wallet=”Recovery wallet” –type=hd –walletpassphrase=temp1 –mnemonic=”MNEMONIC” 将上方代码中的 mnemonic 换成你的助记词。 然后运行下方代码重建提款账户： ethdo account create –account=”Recovery wallet/Withdrawal account” –walletpassphrase=temp1 –passphrase=temp2 运行下方代码获取其公钥： ethdo account info –account=”Recovery wallet/Withdrawal account” 这时，要注意确认该命令输出的公钥与之前抄下来的公钥一致。若一致，则表明你的助记词是正确的。 若要再次删除该钱包，请运行下方代码： ethdo wallet delete –wallet=”Recovery wallet” 然后运行下方代码来尝试访问钱包，确认其是否已经删除： ethdo wallet info –wallet=”Recovery wallet” 正常情况下，会返回一条错误消息，原因是无法找到钱包。 以上步骤也能用于验证者密钥吗？ 不可以。以太坊 2.0 的验证者密钥与取款密钥在使用方式上差异很大，而且与你熟悉的绝大多数密钥都不相同。我们将在下一篇文章中细述验证者密钥，及其用法和保护措施。 脚注 注 1：取款密钥和以太坊 2.0 的其它密钥并无特殊区别，我们使用这个名字只是为了直观地表达出其用途。 注 2：如图所示，实际操作中我们只需要提款身份标识。但是，我们建议也要保存好取款公钥，因为公钥可能有其他用途（如，验证签名）。 注 3：因为公钥长这样：0xa9ca9cf7fa2d0ab1d5d52d2d8f79f68c50c5296bfce81546c254df68eaac0418717b2f9fc6655cbbddb145daeb282c00，所以会很麻烦。"
  },
  {
    "type": "doc",
    "url": "https://www.galaxy.com/research/",
    "title": "Global Markets, Asset Management, and Crypto Market Insights",
    "chunk": 0,
    "text": "You are leaving Galaxy.com You are leaving the Galaxy website and being directed to an external third-party website that we think might be of interest to you. Third-party websites are not under the control of Galaxy, and Galaxy is not responsible for the accuracy or completeness of the contents or the proper operation of any linked site. Please note the security and privacy policiesÂ on third-party websites differ from Galaxy policies,Â please read third-party privacy and security policies closely. If you do not wish to continue to the third-party site, click âCancelâ. The inclusion of any linked website does not imply Galaxyâs endorsement orÂ adoption of the statements therein and is only provided for your convenience. Cancel Proceed"
  },
  {
    "type": "doc",
    "url": "https://hashrateindex.com/blog",
    "title": "The Standard For Bitcoin Mining Analytics",
    "chunk": 0,
    "text": "July 2025’s hashrate and hashprice trends, forward market participation, trading activity and contract performance."
  },
  {
    "type": "doc",
    "url": "https://www.coindesk.com/indices/",
    "title": "Regulated and Transparent Index Solutions to Power the Digital Asset Economy",
    "chunk": 0,
    "text": "The CoinDesk 20 Index captures the performance of top digital assets and is your gateway to measure, trade and invest in the ever-expanding crypto asset class. This index is built to trade, act as the benchmark for the crypto asset class, and be a foundation for investment vehicle construction. CoinDesk 20 products are available globally.Â See more"
  },
  {
    "type": "doc",
    "url": "https://www.coinshares.com/research",
    "title": "Research & Data | CoinShares",
    "chunk": 0,
    "text": "Your go-to hub for expert opinion and in-depth data on the current state of crypto. Gain exclusive insights from our team of experts with extensive experience in both traditional and crypto markets."
  }
]